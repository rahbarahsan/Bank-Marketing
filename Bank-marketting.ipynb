{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tv3CZbnS6Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MvD3L-4NM5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "url='https://raw.githubusercontent.com/scarface961/Bank-Marketting/master/bank-full.csv'\n",
        "data = pd.read_csv(url,delimiter=';')\n",
        "#data1 = pd.read_csv(\"D:/Data Mining and Concepts Learning/Project/bank/bank.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rawbYjyYNM5w",
        "colab_type": "code",
        "outputId": "c2df4c1f-cc37-45cb-8153-5a9144f199cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>management</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>2143</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>technician</td>\n",
              "      <td>single</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>29</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1506</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>unknown</td>\n",
              "      <td>single</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>technician</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>825</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>retired</td>\n",
              "      <td>divorced</td>\n",
              "      <td>primary</td>\n",
              "      <td>no</td>\n",
              "      <td>1729</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>retired</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>5715</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>success</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>668</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2971</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age           job   marital  education  ... pdays  previous poutcome    y\n",
              "0       58    management   married   tertiary  ...    -1         0  unknown   no\n",
              "1       44    technician    single  secondary  ...    -1         0  unknown   no\n",
              "2       33  entrepreneur   married  secondary  ...    -1         0  unknown   no\n",
              "3       47   blue-collar   married    unknown  ...    -1         0  unknown   no\n",
              "4       33       unknown    single    unknown  ...    -1         0  unknown   no\n",
              "...    ...           ...       ...        ...  ...   ...       ...      ...  ...\n",
              "45206   51    technician   married   tertiary  ...    -1         0  unknown  yes\n",
              "45207   71       retired  divorced    primary  ...    -1         0  unknown  yes\n",
              "45208   72       retired   married  secondary  ...   184         3  success  yes\n",
              "45209   57   blue-collar   married  secondary  ...    -1         0  unknown   no\n",
              "45210   37  entrepreneur   married  secondary  ...   188        11    other   no\n",
              "\n",
              "[45211 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8YIuEdhNM50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.get_dummies(data, columns=[\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"])\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "data['y'] = labelencoder.fit_transform(data['y'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ5b4PBZOR-2",
        "colab_type": "code",
        "outputId": "9c141750-9159-4304-c94b-0a377b9e3ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>balance</th>\n",
              "      <th>day</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>y</th>\n",
              "      <th>job_admin.</th>\n",
              "      <th>job_blue-collar</th>\n",
              "      <th>job_entrepreneur</th>\n",
              "      <th>job_housemaid</th>\n",
              "      <th>job_management</th>\n",
              "      <th>job_retired</th>\n",
              "      <th>job_self-employed</th>\n",
              "      <th>job_services</th>\n",
              "      <th>job_student</th>\n",
              "      <th>job_technician</th>\n",
              "      <th>job_unemployed</th>\n",
              "      <th>job_unknown</th>\n",
              "      <th>marital_divorced</th>\n",
              "      <th>marital_married</th>\n",
              "      <th>marital_single</th>\n",
              "      <th>education_primary</th>\n",
              "      <th>education_secondary</th>\n",
              "      <th>education_tertiary</th>\n",
              "      <th>education_unknown</th>\n",
              "      <th>default_no</th>\n",
              "      <th>default_yes</th>\n",
              "      <th>housing_no</th>\n",
              "      <th>housing_yes</th>\n",
              "      <th>loan_no</th>\n",
              "      <th>loan_yes</th>\n",
              "      <th>contact_cellular</th>\n",
              "      <th>contact_telephone</th>\n",
              "      <th>contact_unknown</th>\n",
              "      <th>month_apr</th>\n",
              "      <th>month_aug</th>\n",
              "      <th>month_dec</th>\n",
              "      <th>month_feb</th>\n",
              "      <th>month_jan</th>\n",
              "      <th>month_jul</th>\n",
              "      <th>month_jun</th>\n",
              "      <th>month_mar</th>\n",
              "      <th>month_may</th>\n",
              "      <th>month_nov</th>\n",
              "      <th>month_oct</th>\n",
              "      <th>month_sep</th>\n",
              "      <th>poutcome_failure</th>\n",
              "      <th>poutcome_other</th>\n",
              "      <th>poutcome_success</th>\n",
              "      <th>poutcome_unknown</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>2143</td>\n",
              "      <td>5</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>1506</td>\n",
              "      <td>5</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>825</td>\n",
              "      <td>17</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>1729</td>\n",
              "      <td>17</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>5715</td>\n",
              "      <td>17</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>668</td>\n",
              "      <td>17</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>2971</td>\n",
              "      <td>17</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  balance  day  ...  poutcome_other  poutcome_success  poutcome_unknown\n",
              "0       58     2143    5  ...               0                 0                 1\n",
              "1       44       29    5  ...               0                 0                 1\n",
              "2       33        2    5  ...               0                 0                 1\n",
              "3       47     1506    5  ...               0                 0                 1\n",
              "4       33        1    5  ...               0                 0                 1\n",
              "...    ...      ...  ...  ...             ...               ...               ...\n",
              "45206   51      825   17  ...               0                 0                 1\n",
              "45207   71     1729   17  ...               0                 0                 1\n",
              "45208   72     5715   17  ...               0                 1                 0\n",
              "45209   57      668   17  ...               0                 0                 1\n",
              "45210   37     2971   17  ...               1                 0                 0\n",
              "\n",
              "[45211 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGHBijTch-aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch (history):\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBPmtJk8SF8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dPCA(X_train,X_test):\n",
        "    pca = PCA(n_components= 6)\n",
        "    X_train = pca.fit_transform(X_train)\n",
        "    X_test = pca.transform(X_test)\n",
        "    return X_train,X_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_EhqT1zSIij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dSMOTE(X_train,y_train):\n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6iEGAXxTBkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_NN(percentage_split,X,Y):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "\n",
        "  model= create_NN_model()\n",
        "  history = model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs=150, batch_size=100)\n",
        "  pred = model.predict_classes(x_test)\n",
        "  tn,fp,fn,tp = confusion_matrix(y_test,pred).ravel()\n",
        "  print(\"Percentage Split\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  #epoch(history)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl_uq9SNT28D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_NN(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    model= create_NN_model()\n",
        "    history = model.fit(X[train], Y[train],validation_data=(X[test],Y[test]),epochs=150, batch_size=100)\n",
        "    pred = model.predict_classes(X[test])\n",
        "    #evaluate the model\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],pred).ravel()\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "\n",
        "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "  print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "    \n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoqaNrinSPWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_NN_PCA(percentage_split,X,Y):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "  x_train,x_test = dPCA(x_train,x_test)\n",
        "  model= create_NN_model()\n",
        "  history = model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs=150, batch_size=100)\n",
        "  pred = model.predict_classes(x_test)\n",
        "  tn,fp,fn,tp = confusion_matrix(y_test,pred).ravel()\n",
        "  print(\"Percentage Split\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  #epoch(history)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el2mOiriSbsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_NN_PCA(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    x_train = X[train]\n",
        "    x_test = X[test]\n",
        "    x_train,x_test= dPCA(x_train,x_test)\n",
        "    model= create_NN_model()\n",
        "    history = model.fit(x_train, Y[train],validation_data=(x_test,Y[test]),epochs=150, batch_size=100)\n",
        "    pred = model.predict_classes(x_test)\n",
        "    #evaluate the model\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],pred).ravel()\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "\n",
        "    scores = model.evaluate(x_test, Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "  print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "    \n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrYfoqf9T2pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_NN_SMOTE(percentage_split,X,Y):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "  x_train,y_train = dSMOTE(x_train,y_train)\n",
        "  model= create_NN_model()\n",
        "  history = model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs=150, batch_size=100)\n",
        "  pred = model.predict_classes(x_test)\n",
        "  fpr_nn,tpr_nn ,thresholds_nn  = roc_curve(y_test,pred)\n",
        "  auc_nn = auc(fpr_nn, tpr_nn)\n",
        "  tn,fp,fn,tp = confusion_matrix(y_test,pred).ravel()\n",
        "\n",
        "  print(\"Percentage Split\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  print(\"AUC: \"+repr(auc_nn))\n",
        "  return  fpr_nn,tpr_nn ,thresholds_nn,auc_nn\n",
        "  #epoch(history)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAlaWp0OT_v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_NN_SMOTE(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  fpr_nn = 0\n",
        "  tpr_nn = 0\n",
        "  thresholds_nn = 0\n",
        "  auc_nn=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    # x_train = X[train]\n",
        "    # x_test = X[test]\n",
        "    x_train,y_train= dSMOTE(X[train],Y[train])\n",
        "    model= create_NN_model()\n",
        "    history = model.fit(x_train, y_train,validation_data=(X[test],Y[test]),epochs=150, batch_size=100)\n",
        "    pred = model.predict_classes(X[test])\n",
        "    #evaluate the model\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],pred).ravel()\n",
        "    fpr_nn1,tpr_nn1 ,thresholds_nn1  = roc_curve(Y[test],pred)\n",
        "    auc_nn1 = auc(fpr_nn1, tpr_nn1)\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "    fpr_nn = fpr_nn + fpr_nn1\n",
        "    tpr_nn =  tpr_nn + tpr_nn1\n",
        "    thresholds_nn =thresholds_nn + thresholds_nn1 \n",
        "    auc_nn=auc_nn + auc_nn1\n",
        "\n",
        "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "  print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "    \n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  fpr_nn = fpr_nn/folds\n",
        "  tpr_nn = tpr_nn/folds\n",
        "  thresholds_nn = thresholds_nn/folds\n",
        "  auc_nn = auc_nn/folds \n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  print(\"AUC: \"+repr(auc_nn))\n",
        "  return  fpr_nn,tpr_nn ,thresholds_nn,auc_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQdpyIbnR_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_SVM(percentage_split,X,Y):\n",
        "\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "  clf = svm.SVC(kernel=\"linear\")\n",
        "  clf.fit(x_train, y_train)\n",
        "  predict_svm = clf.predict(x_test)\n",
        "  Confusion_matrix(y_test,predict_svm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-c5Sxtkn2jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_SVM(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    clf = svm.SVC(kernel=\"linear\")\n",
        "    clf.fit(X[train], Y[train])\n",
        "    predict_svm = clf.predict(X[test])\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],predict_svm).ravel()\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "\n",
        "\n",
        "    print(\"Acc: \"+repr(accuracy_score(Y[test],predict_svm)))\n",
        "\n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrMxb9MP8m2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_SVM_SMOTE(percentage_split,X,Y):\n",
        "\n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "    x_train,y_train = dSMOTE(x_train,y_train)\n",
        "    clf = svm.SVC(kernel=\"rbf\")\n",
        "    clf.fit(x_train, y_train)\n",
        "    predict_svm = clf.predict(x_test)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,predict_svm).ravel()\n",
        "    fpr_svm,tpr_svm ,thresholds_svm  = roc_curve(y_test,predict_svm)\n",
        "    auc_svm = auc(fpr_svm, tpr_svm)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,predict_svm).ravel()\n",
        "\n",
        "    print(\"Percentage Split\")\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC: \"+repr(auc_svm))\n",
        "    return  fpr_svm,tpr_svm ,thresholds_svm,auc_svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRuayk-h9UKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_SVM_Smote(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  fpr_svm = 0\n",
        "  tpr_svm = 0\n",
        "  thresholds_svm = 0\n",
        "  auc_svm=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    x_train = X[train]\n",
        "    y_train = Y[train]\n",
        "    x_train,y_train= dSMOTE(x_train,y_train)\n",
        "    clf = svm.SVC(kernel=\"rbf\")\n",
        "    clf.fit(x_train, y_train)\n",
        "    predict_svm = clf.predict(X[test])\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],predict_svm).ravel()\n",
        "    fpr_svm1,tpr_svm1 ,thresholds_svm1  = roc_curve(Y[test],predict_svm)\n",
        "    auc_svm1 = auc(fpr_svm1, tpr_svm1)\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "    fpr_svm = fpr_svm + fpr_svm1\n",
        "    tpr_svm =  tpr_svm + tpr_svm1\n",
        "    thresholds_svm =thresholds_svm + thresholds_svm1 \n",
        "    auc_svm=auc_svm + auc_svm1\n",
        "\n",
        "\n",
        "    print(\"Acc: \"+repr(accuracy_score(Y[test],predict_svm)))\n",
        "\n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  fpr_svm = fpr_svm/folds\n",
        "  tpr_svm = tpr_svm/folds\n",
        "  thresholds_svm = thresholds_svm/folds\n",
        "  auc_svm = auc_svm/folds \n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  print(\"AUC: \"+repr(auc_svm))\n",
        "  return  fpr_svm,tpr_svm ,thresholds_svm,auc_svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew1GchCKnwCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_LogisticsRegression_SMOTE(percentage_split,X,Y):\n",
        "    \n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "    x_train,y_train = dSMOTE(x_train,y_train)\n",
        "    clf = LogisticRegression(penalty = 'l2')\n",
        "    clf.fit(x_train,y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    fpr_lr,tpr_lr ,thresholds_lr  = roc_curve(y_test,y_pred)\n",
        "    auc_lr = auc(fpr_lr, tpr_lr)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,y_pred).ravel()\n",
        "\n",
        "    print(\"Percentage Split\")\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC: \"+repr(auc_lr))\n",
        "    return  fpr_lr,tpr_lr ,thresholds_lr,auc_lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk_j9uV2oXbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_LogisticsRegression_SMOTE(folds,X,Y):\n",
        "      kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "      cvscores = []\n",
        "      tn=0\n",
        "      fp=0\n",
        "      fn=0\n",
        "      tp=0\n",
        "      fpr_lr = 0\n",
        "      tpr_lr = 0\n",
        "      thresholds_lr = 0\n",
        "      auc_lr=0\n",
        "      for train, test in kfold.split(X, Y):\n",
        "        \n",
        "        x_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        x_train,y_train= dSMOTE(x_train,y_train)\n",
        "        clf = LogisticRegression(penalty = 'l2')\n",
        "        clf.fit(x_train,y_train)\n",
        "        y_pred = clf.predict(X[test])\n",
        "\n",
        "        tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],y_pred).ravel()\n",
        "        fpr_lr1,tpr_lr1 ,thresholds_lr1  = roc_curve(Y[test],y_pred)\n",
        "        auc_lr1 = auc(fpr_lr1, tpr_lr1)\n",
        "        tn = tn+tn1\n",
        "        fp = fp+fp1\n",
        "        fn = fn+fn1\n",
        "        tp = tp+tp1\n",
        "        fpr_lr = fpr_lr + fpr_lr1\n",
        "        tpr_lr =  tpr_lr + tpr_lr1\n",
        "        thresholds_lr =thresholds_lr + thresholds_lr1 \n",
        "        auc_lr=auc_lr + auc_lr1\n",
        "\n",
        "        print(\"Acc: \"+repr(accuracy_score(Y[test],y_pred)))\n",
        "\n",
        "      tn=tn/folds\n",
        "      fp=fp/folds\n",
        "      fn=fn/folds\n",
        "      tp=tp/folds\n",
        "      fpr_lr = fpr_lr/folds\n",
        "      tpr_lr = tpr_lr/folds\n",
        "      thresholds_lr = thresholds_lr/folds\n",
        "      auc_lr = auc_lr/folds \n",
        "      print(\"Cross-validation\")\n",
        "      Confusion_matrix(tn,fp,fn,tp)\n",
        "      print(\"AUC: \"+repr(auc_lr))\n",
        "      return  fpr_lr,tpr_lr ,thresholds_lr,auc_lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7SU1-Gmxdp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DecisionTree_percentage_SMOTE(percentage,X, y):\n",
        "\n",
        "    ##### Train & Test Split #####\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=percentage, random_state=0)\n",
        "    \n",
        "    ####### Handling Imbalanced Dataset ###########\n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    \n",
        "    clf = DecisionTreeClassifier(criterion= 'gini', max_depth=7, splitter='best', random_state=0)\n",
        "    \n",
        "    ######## WITH SMOTE ##########\n",
        "#     print(\"ONLY SMOTE\")\n",
        "#     print(\"Criterion = 'gini', max_depth=7, splitter='best'\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"\")\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    predict = clf.predict(X_test)\n",
        "    print(\"\")\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(\"   yes     no   <<-- Classified as   \")\n",
        "    print(\"  \",tp,\"  \",fn,\"        yes\")\n",
        "    print(\"  \",fp,\"  \",tn,\"        no\")\n",
        "    print(\"\")\n",
        "    Accuracy_SMOTE = accuracy_score(y_test,predict)\n",
        "    print(\"Accuracy Score:\",Accuracy_SMOTE)\n",
        "    Sensitivity_SMOTE = tp/(tp+fn)\n",
        "    print(\"Sensitivity:\", Sensitivity_SMOTE)\n",
        "    print(\"Specificity:\", tn/(tn+fp))\n",
        "    print(\"\")\n",
        "    print(\"F1 Score:\",f1_score(y_test, predict))\n",
        "    #print(\"Log Loss:\",log_loss(y_test, predict, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
        "    print(\"Precision:\",precision_score(y_test, predict))\n",
        "    #print(\"Recall:\",recall_score(y_test, predict))\n",
        "    auc= roc_auc_score(y_test, predict)\n",
        "    print(\"AUC:\",roc_auc_score(y_test, predict))\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predict)\n",
        "#     print(\"True Positive Rate:\",tpr.mean())\n",
        "#     print(\"False Positive Rate:\",fpr.mean())\n",
        "#     print(\"Threshold:\",thresholds.mean())\n",
        "#     print(\"CLASSIFICATION REPORT\")\n",
        "#     print(classification_report(y_test, predict))\n",
        "#     print(\"\")\n",
        "    return tpr, fpr, thresholds, auc\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL_W_SHWx4RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## K-Folds CV on Raw Data WITH SMOTE##########\n",
        "def DecisionTree_crossvalidation_SMOTE(folds,X, y):\n",
        "#     print(\"WITH ONLY SMOTE\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"-------------------------------------\")\n",
        "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "    tn=0\n",
        "    fp=0\n",
        "    fn=0\n",
        "    tp=0\n",
        "    fpr_dt = 0\n",
        "    tpr_dt = 0\n",
        "    thresholds_dt = 0\n",
        "    auc_dt=0\n",
        "    clf = DecisionTreeClassifier(criterion= 'gini', max_depth=7, splitter='best', random_state=0)\n",
        "    \n",
        "    for train, test in kfold.split(X, y):\n",
        "      X_train = X[train]\n",
        "      y_train = y[train]\n",
        "      X_train,y_train = dSMOTE(X_train,y_train)\n",
        "      clf.fit(X_train, y_train)\n",
        "      predict = clf.predict(X[test])\n",
        "      fpr_dt1,tpr_dt1 ,thresholds_dt1 = metrics.roc_curve(y[test],predict)\n",
        "      auc_dt1 = roc_auc_score(y[test], predict)\n",
        "      tn1, fp1, fn1, tp1 = confusion_matrix(y[test],predict).ravel()\n",
        "      tn = tn+tn1\n",
        "      fp = fp+fp1\n",
        "      fn = fn+fn1\n",
        "      tp = tp+tp1\n",
        "      fpr_dt = fpr_dt + fpr_dt1\n",
        "      tpr_dt =  tpr_dt + tpr_dt1\n",
        "      thresholds_dt = thresholds_dt + thresholds_dt1\n",
        "      auc_dt=auc_dt + auc_dt1\n",
        "      print(\"Acc: \"+repr(accuracy_score(y[test],predict)))\n",
        "    print(\"\")\n",
        "    tn=tn/folds\n",
        "    fp=fp/folds\n",
        "    fn=fn/folds\n",
        "    tp=tp/folds\n",
        "    fpr_dt = fpr_dt/folds\n",
        "    tpr_dt = tpr_dt/folds\n",
        "    thresholds_dt = thresholds_dt/folds\n",
        "    auc_dt = auc_dt/folds\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC:\",auc_dt)\n",
        "    return tpr_dt, fpr_dt, thresholds_dt, auc_dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeCq5Q7Kdy_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KNN_percentage_SMOTE(percent,X, y):\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=percent, random_state=0)\n",
        "    \n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    \n",
        "    clf = KNeighborsClassifier(algorithm=\"kd_tree\", metric=\"euclidean\", n_neighbors=7)\n",
        "    \n",
        "    ##### Training#####\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    \n",
        "    ##### Testing and accuracy #####\n",
        "    predict = clf.predict(X_test)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(\"   yes     no   <<-- Classified as   \")\n",
        "    print(\"  \",tp,\"  \",fn,\"        yes\")\n",
        "    print(\"  \",fp,\"  \",tn,\"        no\")\n",
        "    print(\"\")\n",
        "    Accuracy_SMOTE = accuracy_score(y_test,predict)\n",
        "    print(\"Accuracy Score:\",Accuracy_SMOTE)\n",
        "    Sensitivity_SMOTE = tp/(tp+fn)\n",
        "    print(\"Sensitivity:\", Sensitivity_SMOTE)\n",
        "    print(\"Specificity:\", tn/(tn+fp))\n",
        "    print(\"\")\n",
        "    print(\"F1 Score:\",f1_score(y_test, predict))\n",
        "    print(\"Precision:\",precision_score(y_test, predict))\n",
        "    #print(\"Recall:\",recall_score(y_test, predict))\n",
        "    auc = roc_auc_score(y_test, predict)\n",
        "    print(\"AUC:\",auc)\n",
        "    print(\"\")\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predict)\n",
        "#     print(\"True Positive Rate:\",tpr)\n",
        "#     print(\"False Positive Rate:\",fpr)\n",
        "#     print(\"Threshold:\",thresholds)\n",
        "    print(\"\")\n",
        "    return tpr, fpr, thresholds, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWxd-4vCd4x7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KNN_crossvalidation_SMOTE(folds,X, y):\n",
        "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "    tn=0\n",
        "    fp=0\n",
        "    fn=0\n",
        "    tp=0\n",
        "    fpr_knn = 0\n",
        "    tpr_knn = 0\n",
        "    thresholds_knn = 0\n",
        "    auc_knn=0\n",
        "    clf = KNeighborsClassifier(algorithm=\"kd_tree\", metric=\"euclidean\", n_neighbors=7)\n",
        "    \n",
        "    for train, test in kfold.split(X, y):\n",
        "      X_train = X[train]\n",
        "      y_train = y[train]\n",
        "      X_train,y_train = dSMOTE(X_train,y_train)\n",
        "      clf.fit(X_train, y_train)\n",
        "      predict = clf.predict(X[test])\n",
        "      fpr_knn1,tpr_knn1 ,thresholds_knn1 = metrics.roc_curve(y[test],predict)\n",
        "      auc_knn1 = roc_auc_score(y[test], predict)\n",
        "      tn1, fp1, fn1, tp1 = confusion_matrix(y[test],predict).ravel()\n",
        "      tn = tn+tn1\n",
        "      fp = fp+fp1\n",
        "      fn = fn+fn1\n",
        "      tp = tp+tp1\n",
        "      fpr_knn = fpr_knn + fpr_knn1\n",
        "      tpr_knn =  tpr_knn + tpr_knn1\n",
        "      thresholds_knn = thresholds_knn + thresholds_knn1\n",
        "      auc_knn=auc_knn + auc_knn1\n",
        "      print(\"Acc: \"+repr(accuracy_score(y[test],predict)))\n",
        "    print(\"\")\n",
        "    tn=tn/folds\n",
        "    fp=fp/folds\n",
        "    fn=fn/folds\n",
        "    tp=tp/folds\n",
        "    fpr_knn = fpr_knn/folds\n",
        "    tpr_knn = tpr_knn/folds\n",
        "    thresholds_knn = thresholds_knn/folds\n",
        "    auc_knn = auc_knn/folds\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC:\",auc_knn)\n",
        "    return tpr_knn, fpr_knn, thresholds_knn, auc_knn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEaL6rF-iR1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RandomForest_percentage_SMOTE(percent,X, y):\n",
        "    ##### Train & Test Split #####\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=percent, random_state=0)\n",
        "    \n",
        "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "    \n",
        "    ####### Handling Imbalanced Dataset ###########\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    \n",
        "    \n",
        "    ##### Training#####\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    \n",
        "    ####### Testing and accuracy #####\n",
        "    from sklearn.metrics import accuracy_score\n",
        "#     print(\"Feature Importances:\", clf.feature_importances_)\n",
        "#     print(\"\")\n",
        "    predict = clf.predict(X_test)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(\"   yes     no   <<-- Classified as   \")\n",
        "    print(\"  \",tp,\"  \",fn,\"        yes\")\n",
        "    print(\"  \",fp,\"  \",tn,\"        no\")\n",
        "    print(\"\")\n",
        "    Accuracy_SMOTE = accuracy_score(y_test,predict)\n",
        "    print(\"Accuracy Score:\",Accuracy_SMOTE)\n",
        "    Sensitivity_SMOTE = tp/(tp+fn)\n",
        "    print(\"Sensitivity:\", Sensitivity_SMOTE)\n",
        "    print(\"Specificity:\", tn/(tn+fp))\n",
        "    print(\"\")\n",
        "    print(\"F1 Score:\",f1_score(y_test, predict))\n",
        "    #print(\"Log Loss:\",log_loss(y_test, predict, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
        "    print(\"Precision:\",precision_score(y_test, predict))\n",
        "    #print(\"Recall:\",recall_score(y_test, predict))\n",
        "    auc = roc_auc_score(y_test, predict)\n",
        "    print(\"AUC:\",auc)\n",
        "    print(\"\")\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predict)\n",
        "#     print(\"True Positive Rate:\",tpr)\n",
        "#     print(\"False Positive Rate:\",fpr)\n",
        "#     print(\"Threshold:\",thresholds)\n",
        "    return tpr, fpr, thresholds, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ySeVXeiSSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RandomForest_crossvalidation_SMOTE(folds,X, y):\n",
        "    ########## K-Folds CV with RFECV WITH SMOTE##########\n",
        "#     print(\"WITH ONLY SMOTE\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"\")\n",
        "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "    tn=0\n",
        "    fp=0\n",
        "    fn=0\n",
        "    tp=0\n",
        "    fpr_rf = 0\n",
        "    tpr_rf = 0\n",
        "    thresholds_rf = 0\n",
        "    auc_rf=0\n",
        "    for train, test in kfold.split(X, y):\n",
        "      X_train = X[train]\n",
        "      y_train = y[train]\n",
        "      X_train,y_train = dSMOTE(X_train,y_train)\n",
        "      clf.fit(X_train, y_train)\n",
        "      #print(\"Feature Importances:\", clf.feature_importances_)\n",
        "      #print(\"\")\n",
        "      predict = clf.predict(X[test])\n",
        "      fpr_rf1,tpr_rf1 ,thresholds_rf1 = metrics.roc_curve(y[test],predict)\n",
        "      auc_rf1 = roc_auc_score(y[test], predict)\n",
        "      tn1, fp1, fn1, tp1 = confusion_matrix(y[test],predict).ravel()\n",
        "      tn = tn+tn1\n",
        "      fp = fp+fp1\n",
        "      fn = fn+fn1\n",
        "      tp = tp+tp1\n",
        "      fpr_rf = fpr_rf + fpr_rf1\n",
        "      tpr_rf =  tpr_rf + tpr_rf1\n",
        "      thresholds_rf = thresholds_rf + thresholds_rf1\n",
        "      auc_rf=auc_rf + auc_rf1\n",
        "      print(\"Accuracy: \"+repr(accuracy_score(y[test],predict)))\n",
        "      print(\"\")\n",
        "    tn=tn/folds\n",
        "    fp=fp/folds\n",
        "    fn=fn/folds\n",
        "    tp=tp/folds\n",
        "    fpr_rf = fpr_rf/folds\n",
        "    tpr_rf = tpr_rf/folds\n",
        "    thresholds_rf = thresholds_rf/folds\n",
        "    auc_rf = auc_rf/folds\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC:\",auc_rf)\n",
        "    return tpr_rf,fpr_rf,thresholds_rf,auc_rf\n",
        "\n",
        "\n",
        "\n",
        "# print(tpr_ps)\n",
        "# print(fpr_ps)\n",
        "# print(threshold_ps)\n",
        "# print(auc_ps)\n",
        "# print(\"\")\n",
        "# print(tpr_cv)\n",
        "# print(fpr_cv)\n",
        "# print(threshold_cv)\n",
        "# print(auc_cv)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui5oFfOLj5Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_Bayes(percentage_split,X,Y):\n",
        "    \n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "   \n",
        "    nb = GaussianNB()\n",
        "    nb.fit(x_train,y_train)\n",
        "    y_pred = nb.predict(x_test)\n",
        "    fpr_bayes,tpr_bayes ,thresholds_bayes  = roc_curve(y_test,y_pred)\n",
        "    auc_bayes = auc(fpr_bayes, tpr_bayes)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,y_pred).ravel()\n",
        "\n",
        "    print(\"Percentage Split\")\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC: \"+repr(auc_bayes))\n",
        "    return  fpr_bayes,tpr_bayes ,thresholds_bayes,auc_bayes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4i3fcD5j5gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_Bayes(folds,X,Y):\n",
        "      kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "      cvscores = []\n",
        "      tn=0\n",
        "      fp=0\n",
        "      fn=0\n",
        "      tp=0\n",
        "      fpr_bayes = 0\n",
        "      tpr_bayes = 0\n",
        "      thresholds_bayes = 0\n",
        "      auc_bayes=0\n",
        "      for train, test in kfold.split(X, Y):\n",
        "        nb = GaussianNB()\n",
        "        nb.fit( X[train],Y[train])\n",
        "        y_pred = nb.predict(X[test])\n",
        "\n",
        "        tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],y_pred).ravel()\n",
        "        fpr_bayes1,tpr_bayes1 ,thresholds_bayes1  = roc_curve(Y[test],y_pred)\n",
        "        auc_bayes1 = auc(fpr_bayes1, tpr_bayes1)\n",
        "        tn = tn+tn1\n",
        "        fp = fp+fp1\n",
        "        fn = fn+fn1\n",
        "        tp = tp+tp1\n",
        "        fpr_bayes = fpr_bayes + fpr_bayes1\n",
        "        tpr_bayes =  tpr_bayes + tpr_bayes1\n",
        "        thresholds_bayes =thresholds_bayes + thresholds_bayes1 \n",
        "        auc_bayes=auc_bayes + auc_bayes1\n",
        "\n",
        "\n",
        "        print(\"Acc: \"+repr(accuracy_score(Y[test],y_pred)))\n",
        "\n",
        "      tn=tn/folds\n",
        "      fp=fp/folds\n",
        "      fn=fn/folds\n",
        "      tp=tp/folds\n",
        "      fpr_bayes = fpr_bayes/folds\n",
        "      tpr_bayes = tpr_bayes/folds\n",
        "      thresholds_bayes = thresholds_bayes/folds\n",
        "      auc_bayes = auc_bayes/folds \n",
        "      print(\"Cross-validation\")\n",
        "      Confusion_matrix(tn,fp,fn,tp)\n",
        "      print(\"AUC: \"+repr(auc_bayes))\n",
        "      return  fpr_bayes,tpr_bayes ,thresholds_bayes,auc_bayes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk3RE06HSEnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Confusion_matrix(tn, fp, fn, tp):\n",
        "  #tn, fp, fn, tp = confusion_matrix(y_test,pred).ravel()\n",
        "  print(\"Confusion Matrix\")\n",
        "  print(\"   yes     no   <<-- Classified as   \")\n",
        "  # print(\"   %.2f    %.2f  yes\" %(tp,fn))\n",
        "  # print(\"   %.2f    %.2f  no\" %(fp,tn))\n",
        "  print(\"   \"+repr(int(tp))+\"    \"+repr(int(fn))+\"        yes\")\n",
        "  print(\"   \"+repr(int(fp))+\"    \"+repr(int(tn))+\"        no\")\n",
        "  accuracy = float((tp+tn)/(tp+tn+fp+fn))\n",
        "  specificity = float(tn / (tn+fp))\n",
        "  sensitivity = float(tp / (tp+fn))\n",
        "  \n",
        "  if (tp+fp)>=0:\n",
        "    precision = float(tp / (tp+fp))\n",
        "    f_score = (2*precision*sensitivity)/(precision+sensitivity) \n",
        "  else:\n",
        "    precision = 0\n",
        "    f_score = \"?\"\n",
        " \n",
        "  #print(classification_report(y_test,predictions))\n",
        "  print(\"Accuracy: \"+repr(accuracy))\n",
        "  print(\"Sensitivity: \"+repr(sensitivity))\n",
        "  print(\"Specificity: \"+repr(specificity))\n",
        "  print(\"Precision: \"+repr(precision))\n",
        "  print(\"f_score: \"+repr(f_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQPYa0HyNM54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_NN_model():\n",
        "  #Keras Multi Layer\n",
        "  model = Sequential()\n",
        "  model.add(Dense(5,input_dim=51,activation='tanh'))\n",
        "  model.add(Dense(5, activation='tanh'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  #keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  #sgd = optimizers.SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  a = optimizers.adam(lr=0.007)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=a, metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HklmHbO4NM52",
        "colab_type": "code",
        "outputId": "96bb3342-db57-480b-cc0f-d29b142ca398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Splitting the data labels and dataset\n",
        "Y = data.y.values\n",
        "X= data.drop('y',axis = 1).values\n",
        "# X = data.iloc[:,0:53]\n",
        "# Y = data.iloc[:,52:53]\n",
        "# print(X)\n",
        "# #Normalize\n",
        "# from sklearn import preprocessing\n",
        "# X = preprocessing.scale(X)\n",
        "# print(Y)\n",
        "from sklearn import preprocessing\n",
        "X = preprocessing.scale(X)\n",
        "X\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.60696496,  0.25641925, -1.29847633, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       [ 0.28852927, -0.43789469, -1.29847633, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       [-0.74738448, -0.44676247, -1.29847633, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       ...,\n",
              "       [ 2.92540065,  1.42959305,  0.14341818, ..., -0.20597248,\n",
              "         5.37784754, -2.11631591],\n",
              "       [ 1.51279098, -0.22802402,  0.14341818, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       [-0.37068857,  0.52836436,  0.14341818, ...,  4.85501757,\n",
              "        -0.185948  , -2.11631591]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7cqkSwYnNEO",
        "colab_type": "code",
        "outputId": "2971d55a-2807-4221-a012-d98a37acccd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "percentage_split_NN(0.25,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 33908 samples, validate on 11303 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "33908/33908 [==============================] - 1s 36us/step - loss: 0.2777 - acc: 0.8843 - val_loss: 0.2274 - val_acc: 0.9012\n",
            "Epoch 2/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.2240 - acc: 0.9014 - val_loss: 0.2220 - val_acc: 0.9005\n",
            "Epoch 3/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2178 - acc: 0.9060 - val_loss: 0.2171 - val_acc: 0.9028\n",
            "Epoch 4/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2151 - acc: 0.9074 - val_loss: 0.2148 - val_acc: 0.9022\n",
            "Epoch 5/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2132 - acc: 0.9063 - val_loss: 0.2175 - val_acc: 0.9013\n",
            "Epoch 6/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2117 - acc: 0.9073 - val_loss: 0.2170 - val_acc: 0.9023\n",
            "Epoch 7/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2111 - acc: 0.9075 - val_loss: 0.2170 - val_acc: 0.9022\n",
            "Epoch 8/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2098 - acc: 0.9078 - val_loss: 0.2174 - val_acc: 0.9019\n",
            "Epoch 9/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2095 - acc: 0.9073 - val_loss: 0.2150 - val_acc: 0.9053\n",
            "Epoch 10/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2086 - acc: 0.9086 - val_loss: 0.2157 - val_acc: 0.9018\n",
            "Epoch 11/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2078 - acc: 0.9076 - val_loss: 0.2173 - val_acc: 0.9024\n",
            "Epoch 12/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2081 - acc: 0.9069 - val_loss: 0.2166 - val_acc: 0.9027\n",
            "Epoch 13/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2076 - acc: 0.9074 - val_loss: 0.2131 - val_acc: 0.9022\n",
            "Epoch 14/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2072 - acc: 0.9077 - val_loss: 0.2119 - val_acc: 0.9047\n",
            "Epoch 15/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2069 - acc: 0.9080 - val_loss: 0.2133 - val_acc: 0.9024\n",
            "Epoch 16/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2068 - acc: 0.9076 - val_loss: 0.2131 - val_acc: 0.9027\n",
            "Epoch 17/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2062 - acc: 0.9077 - val_loss: 0.2117 - val_acc: 0.9022\n",
            "Epoch 18/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2055 - acc: 0.9082 - val_loss: 0.2126 - val_acc: 0.9031\n",
            "Epoch 19/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2058 - acc: 0.9088 - val_loss: 0.2125 - val_acc: 0.9037\n",
            "Epoch 20/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2051 - acc: 0.9079 - val_loss: 0.2118 - val_acc: 0.9022\n",
            "Epoch 21/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2052 - acc: 0.9072 - val_loss: 0.2127 - val_acc: 0.9014\n",
            "Epoch 22/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2042 - acc: 0.9069 - val_loss: 0.2132 - val_acc: 0.9028\n",
            "Epoch 23/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2041 - acc: 0.9079 - val_loss: 0.2133 - val_acc: 0.9038\n",
            "Epoch 24/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2041 - acc: 0.9086 - val_loss: 0.2138 - val_acc: 0.9020\n",
            "Epoch 25/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2037 - acc: 0.9077 - val_loss: 0.2153 - val_acc: 0.9044\n",
            "Epoch 26/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2047 - acc: 0.9065 - val_loss: 0.2138 - val_acc: 0.9037\n",
            "Epoch 27/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.2037 - acc: 0.9076 - val_loss: 0.2129 - val_acc: 0.9030\n",
            "Epoch 28/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9082 - val_loss: 0.2138 - val_acc: 0.9032\n",
            "Epoch 29/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9078 - val_loss: 0.2129 - val_acc: 0.9035\n",
            "Epoch 30/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9074 - val_loss: 0.2138 - val_acc: 0.9038\n",
            "Epoch 31/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9081 - val_loss: 0.2133 - val_acc: 0.9032\n",
            "Epoch 32/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2035 - acc: 0.9078 - val_loss: 0.2139 - val_acc: 0.9014\n",
            "Epoch 33/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2030 - acc: 0.9083 - val_loss: 0.2125 - val_acc: 0.9037\n",
            "Epoch 34/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2029 - acc: 0.9080 - val_loss: 0.2123 - val_acc: 0.9038\n",
            "Epoch 35/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2025 - acc: 0.9082 - val_loss: 0.2140 - val_acc: 0.9035\n",
            "Epoch 36/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2027 - acc: 0.9073 - val_loss: 0.2128 - val_acc: 0.9045\n",
            "Epoch 37/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2027 - acc: 0.9083 - val_loss: 0.2115 - val_acc: 0.9049\n",
            "Epoch 38/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2027 - acc: 0.9084 - val_loss: 0.2126 - val_acc: 0.9045\n",
            "Epoch 39/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2024 - acc: 0.9077 - val_loss: 0.2135 - val_acc: 0.9043\n",
            "Epoch 40/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2024 - acc: 0.9078 - val_loss: 0.2121 - val_acc: 0.9054\n",
            "Epoch 41/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2019 - acc: 0.9081 - val_loss: 0.2117 - val_acc: 0.9039\n",
            "Epoch 42/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2023 - acc: 0.9077 - val_loss: 0.2106 - val_acc: 0.9054\n",
            "Epoch 43/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2019 - acc: 0.9082 - val_loss: 0.2118 - val_acc: 0.9042\n",
            "Epoch 44/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2018 - acc: 0.9075 - val_loss: 0.2122 - val_acc: 0.9049\n",
            "Epoch 45/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2013 - acc: 0.9087 - val_loss: 0.2144 - val_acc: 0.9033\n",
            "Epoch 46/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2010 - acc: 0.9080 - val_loss: 0.2128 - val_acc: 0.9037\n",
            "Epoch 47/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2015 - acc: 0.9082 - val_loss: 0.2117 - val_acc: 0.9026\n",
            "Epoch 48/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2016 - acc: 0.9078 - val_loss: 0.2126 - val_acc: 0.9015\n",
            "Epoch 49/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.2009 - acc: 0.9080 - val_loss: 0.2121 - val_acc: 0.9042\n",
            "Epoch 50/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2007 - acc: 0.9078 - val_loss: 0.2123 - val_acc: 0.9020\n",
            "Epoch 51/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2010 - acc: 0.9076 - val_loss: 0.2118 - val_acc: 0.9021\n",
            "Epoch 52/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2008 - acc: 0.9088 - val_loss: 0.2125 - val_acc: 0.9045\n",
            "Epoch 53/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2010 - acc: 0.9076 - val_loss: 0.2114 - val_acc: 0.9061\n",
            "Epoch 54/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2006 - acc: 0.9080 - val_loss: 0.2114 - val_acc: 0.9045\n",
            "Epoch 55/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2004 - acc: 0.9081 - val_loss: 0.2118 - val_acc: 0.9024\n",
            "Epoch 56/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2008 - acc: 0.9086 - val_loss: 0.2123 - val_acc: 0.9031\n",
            "Epoch 57/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.2001 - acc: 0.9085 - val_loss: 0.2118 - val_acc: 0.9026\n",
            "Epoch 58/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2007 - acc: 0.9082 - val_loss: 0.2125 - val_acc: 0.9020\n",
            "Epoch 59/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2009 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.9022\n",
            "Epoch 60/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2006 - acc: 0.9080 - val_loss: 0.2121 - val_acc: 0.9037\n",
            "Epoch 61/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1999 - acc: 0.9083 - val_loss: 0.2116 - val_acc: 0.9050\n",
            "Epoch 62/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1998 - acc: 0.9082 - val_loss: 0.2127 - val_acc: 0.9052\n",
            "Epoch 63/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2002 - acc: 0.9073 - val_loss: 0.2118 - val_acc: 0.9033\n",
            "Epoch 64/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1998 - acc: 0.9076 - val_loss: 0.2137 - val_acc: 0.9017\n",
            "Epoch 65/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.1999 - acc: 0.9079 - val_loss: 0.2110 - val_acc: 0.9037\n",
            "Epoch 66/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1994 - acc: 0.9092 - val_loss: 0.2113 - val_acc: 0.9025\n",
            "Epoch 67/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1997 - acc: 0.9074 - val_loss: 0.2130 - val_acc: 0.9041\n",
            "Epoch 68/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1995 - acc: 0.9081 - val_loss: 0.2119 - val_acc: 0.9048\n",
            "Epoch 69/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1992 - acc: 0.9075 - val_loss: 0.2100 - val_acc: 0.9045\n",
            "Epoch 70/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1992 - acc: 0.9080 - val_loss: 0.2110 - val_acc: 0.9031\n",
            "Epoch 71/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1995 - acc: 0.9076 - val_loss: 0.2104 - val_acc: 0.9026\n",
            "Epoch 72/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9080 - val_loss: 0.2097 - val_acc: 0.9043\n",
            "Epoch 73/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1989 - acc: 0.9080 - val_loss: 0.2125 - val_acc: 0.9012\n",
            "Epoch 74/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1993 - acc: 0.9095 - val_loss: 0.2109 - val_acc: 0.9025\n",
            "Epoch 75/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1995 - acc: 0.9080 - val_loss: 0.2107 - val_acc: 0.9039\n",
            "Epoch 76/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9079 - val_loss: 0.2118 - val_acc: 0.9016\n",
            "Epoch 77/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9078 - val_loss: 0.2127 - val_acc: 0.9014\n",
            "Epoch 78/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1991 - acc: 0.9098 - val_loss: 0.2110 - val_acc: 0.9057\n",
            "Epoch 79/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1986 - acc: 0.9083 - val_loss: 0.2125 - val_acc: 0.9034\n",
            "Epoch 80/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1988 - acc: 0.9087 - val_loss: 0.2104 - val_acc: 0.9040\n",
            "Epoch 81/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.2112 - val_acc: 0.9050\n",
            "Epoch 82/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.2111 - val_acc: 0.9050\n",
            "Epoch 83/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1989 - acc: 0.9084 - val_loss: 0.2111 - val_acc: 0.9036\n",
            "Epoch 84/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1988 - acc: 0.9071 - val_loss: 0.2120 - val_acc: 0.9014\n",
            "Epoch 85/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1986 - acc: 0.9076 - val_loss: 0.2114 - val_acc: 0.9035\n",
            "Epoch 86/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1989 - acc: 0.9079 - val_loss: 0.2117 - val_acc: 0.9028\n",
            "Epoch 87/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1983 - acc: 0.9077 - val_loss: 0.2114 - val_acc: 0.9034\n",
            "Epoch 88/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1988 - acc: 0.9082 - val_loss: 0.2137 - val_acc: 0.9027\n",
            "Epoch 89/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1981 - acc: 0.9086 - val_loss: 0.2123 - val_acc: 0.9037\n",
            "Epoch 90/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1983 - acc: 0.9086 - val_loss: 0.2104 - val_acc: 0.9051\n",
            "Epoch 91/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1985 - acc: 0.9084 - val_loss: 0.2130 - val_acc: 0.9014\n",
            "Epoch 92/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9072 - val_loss: 0.2115 - val_acc: 0.9034\n",
            "Epoch 93/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1984 - acc: 0.9078 - val_loss: 0.2113 - val_acc: 0.9026\n",
            "Epoch 94/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9086 - val_loss: 0.2121 - val_acc: 0.9020\n",
            "Epoch 95/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1985 - acc: 0.9074 - val_loss: 0.2118 - val_acc: 0.9039\n",
            "Epoch 96/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9081 - val_loss: 0.2103 - val_acc: 0.9058\n",
            "Epoch 97/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1981 - acc: 0.9085 - val_loss: 0.2117 - val_acc: 0.9023\n",
            "Epoch 98/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9078 - val_loss: 0.2136 - val_acc: 0.9006\n",
            "Epoch 99/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9080 - val_loss: 0.2110 - val_acc: 0.9037\n",
            "Epoch 100/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9090 - val_loss: 0.2120 - val_acc: 0.9013\n",
            "Epoch 101/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9085 - val_loss: 0.2115 - val_acc: 0.9024\n",
            "Epoch 102/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2120 - val_acc: 0.9033\n",
            "Epoch 103/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9085 - val_loss: 0.2121 - val_acc: 0.9037\n",
            "Epoch 104/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9078 - val_loss: 0.2124 - val_acc: 0.9031\n",
            "Epoch 105/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9082 - val_loss: 0.2127 - val_acc: 0.9033\n",
            "Epoch 106/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9081 - val_loss: 0.2106 - val_acc: 0.9036\n",
            "Epoch 107/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9079 - val_loss: 0.2112 - val_acc: 0.9032\n",
            "Epoch 108/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1984 - acc: 0.9073 - val_loss: 0.2131 - val_acc: 0.9044\n",
            "Epoch 109/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1980 - acc: 0.9082 - val_loss: 0.2149 - val_acc: 0.9006\n",
            "Epoch 110/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9085 - val_loss: 0.2118 - val_acc: 0.9037\n",
            "Epoch 111/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9087 - val_loss: 0.2122 - val_acc: 0.9044\n",
            "Epoch 112/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1981 - acc: 0.9083 - val_loss: 0.2123 - val_acc: 0.9040\n",
            "Epoch 113/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9085 - val_loss: 0.2120 - val_acc: 0.9009\n",
            "Epoch 114/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9092 - val_loss: 0.2118 - val_acc: 0.9021\n",
            "Epoch 115/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9087 - val_loss: 0.2176 - val_acc: 0.8994\n",
            "Epoch 116/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2137 - val_acc: 0.9002\n",
            "Epoch 117/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9072 - val_loss: 0.2135 - val_acc: 0.9025\n",
            "Epoch 118/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2136 - val_acc: 0.9024\n",
            "Epoch 119/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9079 - val_loss: 0.2114 - val_acc: 0.9029\n",
            "Epoch 120/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9092 - val_loss: 0.2112 - val_acc: 0.9047\n",
            "Epoch 121/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9079 - val_loss: 0.2138 - val_acc: 0.9006\n",
            "Epoch 122/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9073 - val_loss: 0.2132 - val_acc: 0.9035\n",
            "Epoch 123/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9083 - val_loss: 0.2142 - val_acc: 0.9023\n",
            "Epoch 124/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2125 - val_acc: 0.9021\n",
            "Epoch 125/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9082 - val_loss: 0.2111 - val_acc: 0.9034\n",
            "Epoch 126/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9077 - val_loss: 0.2134 - val_acc: 0.9018\n",
            "Epoch 127/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9087 - val_loss: 0.2131 - val_acc: 0.9006\n",
            "Epoch 128/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1977 - acc: 0.9083 - val_loss: 0.2124 - val_acc: 0.9025\n",
            "Epoch 129/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9079 - val_loss: 0.2117 - val_acc: 0.9014\n",
            "Epoch 130/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9083 - val_loss: 0.2135 - val_acc: 0.9010\n",
            "Epoch 131/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.2134 - val_acc: 0.9026\n",
            "Epoch 132/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9088 - val_loss: 0.2119 - val_acc: 0.9021\n",
            "Epoch 133/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9087 - val_loss: 0.2132 - val_acc: 0.9034\n",
            "Epoch 134/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9074 - val_loss: 0.2124 - val_acc: 0.9039\n",
            "Epoch 135/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9075 - val_loss: 0.2142 - val_acc: 0.9023\n",
            "Epoch 136/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9086 - val_loss: 0.2117 - val_acc: 0.9021\n",
            "Epoch 137/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.2122 - val_acc: 0.9039\n",
            "Epoch 138/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9089 - val_loss: 0.2125 - val_acc: 0.9021\n",
            "Epoch 139/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9082 - val_loss: 0.2142 - val_acc: 0.9016\n",
            "Epoch 140/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1971 - acc: 0.9081 - val_loss: 0.2137 - val_acc: 0.9009\n",
            "Epoch 141/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9075 - val_loss: 0.2145 - val_acc: 0.9014\n",
            "Epoch 142/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1970 - acc: 0.9089 - val_loss: 0.2124 - val_acc: 0.9028\n",
            "Epoch 143/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1972 - acc: 0.9075 - val_loss: 0.2122 - val_acc: 0.9037\n",
            "Epoch 144/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9079 - val_loss: 0.2138 - val_acc: 0.9005\n",
            "Epoch 145/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1971 - acc: 0.9085 - val_loss: 0.2127 - val_acc: 0.9022\n",
            "Epoch 146/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9082 - val_loss: 0.2125 - val_acc: 0.9018\n",
            "Epoch 147/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9085 - val_loss: 0.2123 - val_acc: 0.9013\n",
            "Epoch 148/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2147 - val_acc: 0.9035\n",
            "Epoch 149/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1971 - acc: 0.9086 - val_loss: 0.2132 - val_acc: 0.9021\n",
            "Epoch 150/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1972 - acc: 0.9093 - val_loss: 0.2124 - val_acc: 0.9037\n",
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   638    684        yes\n",
            "   405    9576        no\n",
            "Accuracy: 0.9036538971954349\n",
            "Sensitivity: 0.4826021180030257\n",
            "Specificity: 0.9594229035166817\n",
            "Precision: 0.6116970278044104\n",
            "f_score: 0.5395348837209303\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuAPwy_7m9NN",
        "colab_type": "code",
        "outputId": "d4eac071-3e08-41a3-85e8-d1ba7be7c17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "crossvalidate_NN(10,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40689 samples, validate on 4522 samples\n",
            "Epoch 1/150\n",
            "40689/40689 [==============================] - 6s 141us/step - loss: 0.2573 - acc: 0.8895 - val_loss: 0.2255 - val_acc: 0.9027\n",
            "Epoch 2/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2171 - acc: 0.9012 - val_loss: 0.2155 - val_acc: 0.9045\n",
            "Epoch 3/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2118 - acc: 0.9027 - val_loss: 0.2112 - val_acc: 0.9034\n",
            "Epoch 4/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2097 - acc: 0.9035 - val_loss: 0.2111 - val_acc: 0.9067\n",
            "Epoch 5/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2078 - acc: 0.9039 - val_loss: 0.2120 - val_acc: 0.9054\n",
            "Epoch 6/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2067 - acc: 0.9057 - val_loss: 0.2126 - val_acc: 0.9076\n",
            "Epoch 7/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2065 - acc: 0.9049 - val_loss: 0.2096 - val_acc: 0.9056\n",
            "Epoch 8/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2056 - acc: 0.9048 - val_loss: 0.2079 - val_acc: 0.9093\n",
            "Epoch 9/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2045 - acc: 0.9049 - val_loss: 0.2068 - val_acc: 0.9096\n",
            "Epoch 10/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2040 - acc: 0.9062 - val_loss: 0.2061 - val_acc: 0.9109\n",
            "Epoch 11/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2035 - acc: 0.9061 - val_loss: 0.2073 - val_acc: 0.9104\n",
            "Epoch 12/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2034 - acc: 0.9070 - val_loss: 0.2069 - val_acc: 0.9115\n",
            "Epoch 13/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2025 - acc: 0.9074 - val_loss: 0.2055 - val_acc: 0.9104\n",
            "Epoch 14/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2020 - acc: 0.9071 - val_loss: 0.2068 - val_acc: 0.9093\n",
            "Epoch 15/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2020 - acc: 0.9071 - val_loss: 0.2051 - val_acc: 0.9133\n",
            "Epoch 16/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2022 - acc: 0.9076 - val_loss: 0.2049 - val_acc: 0.9120\n",
            "Epoch 17/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2012 - acc: 0.9072 - val_loss: 0.2074 - val_acc: 0.9109\n",
            "Epoch 18/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2008 - acc: 0.9076 - val_loss: 0.2097 - val_acc: 0.9131\n",
            "Epoch 19/150\n",
            "40689/40689 [==============================] - 1s 23us/step - loss: 0.2008 - acc: 0.9071 - val_loss: 0.2047 - val_acc: 0.9111\n",
            "Epoch 20/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2009 - acc: 0.9085 - val_loss: 0.2074 - val_acc: 0.9111\n",
            "Epoch 21/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2004 - acc: 0.9080 - val_loss: 0.2052 - val_acc: 0.9129\n",
            "Epoch 22/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2005 - acc: 0.9079 - val_loss: 0.2092 - val_acc: 0.9102\n",
            "Epoch 23/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2002 - acc: 0.9083 - val_loss: 0.2057 - val_acc: 0.9113\n",
            "Epoch 24/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2001 - acc: 0.9084 - val_loss: 0.2053 - val_acc: 0.9100\n",
            "Epoch 25/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2000 - acc: 0.9078 - val_loss: 0.2044 - val_acc: 0.9122\n",
            "Epoch 26/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1991 - acc: 0.9083 - val_loss: 0.2092 - val_acc: 0.9102\n",
            "Epoch 27/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1994 - acc: 0.9086 - val_loss: 0.2052 - val_acc: 0.9089\n",
            "Epoch 28/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1991 - acc: 0.9088 - val_loss: 0.2046 - val_acc: 0.9098\n",
            "Epoch 29/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1993 - acc: 0.9093 - val_loss: 0.2066 - val_acc: 0.9118\n",
            "Epoch 30/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1987 - acc: 0.9092 - val_loss: 0.2042 - val_acc: 0.9102\n",
            "Epoch 31/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1995 - acc: 0.9086 - val_loss: 0.2044 - val_acc: 0.9115\n",
            "Epoch 32/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9089 - val_loss: 0.2078 - val_acc: 0.9089\n",
            "Epoch 33/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1989 - acc: 0.9084 - val_loss: 0.2045 - val_acc: 0.9113\n",
            "Epoch 34/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1983 - acc: 0.9092 - val_loss: 0.2050 - val_acc: 0.9073\n",
            "Epoch 35/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1983 - acc: 0.9085 - val_loss: 0.2047 - val_acc: 0.9115\n",
            "Epoch 36/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1980 - acc: 0.9092 - val_loss: 0.2060 - val_acc: 0.9071\n",
            "Epoch 37/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1981 - acc: 0.9097 - val_loss: 0.2040 - val_acc: 0.9111\n",
            "Epoch 38/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1974 - acc: 0.9093 - val_loss: 0.2045 - val_acc: 0.9089\n",
            "Epoch 39/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1978 - acc: 0.9096 - val_loss: 0.2043 - val_acc: 0.9111\n",
            "Epoch 40/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1983 - acc: 0.9092 - val_loss: 0.2052 - val_acc: 0.9122\n",
            "Epoch 41/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1976 - acc: 0.9103 - val_loss: 0.2039 - val_acc: 0.9091\n",
            "Epoch 42/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1975 - acc: 0.9089 - val_loss: 0.2034 - val_acc: 0.9087\n",
            "Epoch 43/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1978 - acc: 0.9087 - val_loss: 0.2045 - val_acc: 0.9100\n",
            "Epoch 44/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1973 - acc: 0.9093 - val_loss: 0.2040 - val_acc: 0.9100\n",
            "Epoch 45/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1968 - acc: 0.9097 - val_loss: 0.2053 - val_acc: 0.9102\n",
            "Epoch 46/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.2024 - val_acc: 0.9107\n",
            "Epoch 47/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9094 - val_loss: 0.2031 - val_acc: 0.9111\n",
            "Epoch 48/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1970 - acc: 0.9094 - val_loss: 0.2024 - val_acc: 0.9102\n",
            "Epoch 49/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1970 - acc: 0.9099 - val_loss: 0.2037 - val_acc: 0.9089\n",
            "Epoch 50/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9098 - val_loss: 0.2037 - val_acc: 0.9084\n",
            "Epoch 51/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1970 - acc: 0.9095 - val_loss: 0.2032 - val_acc: 0.9118\n",
            "Epoch 52/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1970 - acc: 0.9099 - val_loss: 0.2040 - val_acc: 0.9065\n",
            "Epoch 53/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1966 - acc: 0.9097 - val_loss: 0.2030 - val_acc: 0.9091\n",
            "Epoch 54/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1968 - acc: 0.9101 - val_loss: 0.2042 - val_acc: 0.9113\n",
            "Epoch 55/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2028 - val_acc: 0.9109\n",
            "Epoch 56/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9095 - val_loss: 0.2031 - val_acc: 0.9109\n",
            "Epoch 57/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9103 - val_loss: 0.2042 - val_acc: 0.9120\n",
            "Epoch 58/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2031 - val_acc: 0.9109\n",
            "Epoch 59/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1967 - acc: 0.9095 - val_loss: 0.2035 - val_acc: 0.9098\n",
            "Epoch 60/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9092 - val_loss: 0.2037 - val_acc: 0.9109\n",
            "Epoch 61/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9107 - val_loss: 0.2038 - val_acc: 0.9111\n",
            "Epoch 62/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1965 - acc: 0.9100 - val_loss: 0.2035 - val_acc: 0.9107\n",
            "Epoch 63/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9096 - val_loss: 0.2028 - val_acc: 0.9111\n",
            "Epoch 64/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1958 - acc: 0.9101 - val_loss: 0.2041 - val_acc: 0.9076\n",
            "Epoch 65/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1965 - acc: 0.9109 - val_loss: 0.2046 - val_acc: 0.9102\n",
            "Epoch 66/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9101 - val_loss: 0.2033 - val_acc: 0.9107\n",
            "Epoch 67/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9099 - val_loss: 0.2027 - val_acc: 0.9082\n",
            "Epoch 68/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9102 - val_loss: 0.2050 - val_acc: 0.9124\n",
            "Epoch 69/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9103 - val_loss: 0.2030 - val_acc: 0.9118\n",
            "Epoch 70/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1965 - acc: 0.9097 - val_loss: 0.2030 - val_acc: 0.9118\n",
            "Epoch 71/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1959 - acc: 0.9101 - val_loss: 0.2034 - val_acc: 0.9115\n",
            "Epoch 72/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9094 - val_loss: 0.2038 - val_acc: 0.9151\n",
            "Epoch 73/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9094 - val_loss: 0.2032 - val_acc: 0.9113\n",
            "Epoch 74/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9097 - val_loss: 0.2060 - val_acc: 0.9111\n",
            "Epoch 75/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9100 - val_loss: 0.2021 - val_acc: 0.9100\n",
            "Epoch 76/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9100 - val_loss: 0.2042 - val_acc: 0.9091\n",
            "Epoch 77/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1963 - acc: 0.9099 - val_loss: 0.2039 - val_acc: 0.9109\n",
            "Epoch 78/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9111 - val_loss: 0.2028 - val_acc: 0.9100\n",
            "Epoch 79/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9107 - val_loss: 0.2035 - val_acc: 0.9102\n",
            "Epoch 80/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9100 - val_loss: 0.2048 - val_acc: 0.9102\n",
            "Epoch 81/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1964 - acc: 0.9099 - val_loss: 0.2035 - val_acc: 0.9091\n",
            "Epoch 82/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9101 - val_loss: 0.2035 - val_acc: 0.9102\n",
            "Epoch 83/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9102 - val_loss: 0.2042 - val_acc: 0.9109\n",
            "Epoch 84/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9100 - val_loss: 0.2021 - val_acc: 0.9102\n",
            "Epoch 85/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1964 - acc: 0.9092 - val_loss: 0.2033 - val_acc: 0.9093\n",
            "Epoch 86/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9103 - val_loss: 0.2031 - val_acc: 0.9122\n",
            "Epoch 87/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1958 - acc: 0.9109 - val_loss: 0.2036 - val_acc: 0.9087\n",
            "Epoch 88/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9100 - val_loss: 0.2048 - val_acc: 0.9138\n",
            "Epoch 89/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1958 - acc: 0.9101 - val_loss: 0.2036 - val_acc: 0.9107\n",
            "Epoch 90/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9103 - val_loss: 0.2027 - val_acc: 0.9113\n",
            "Epoch 91/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9101 - val_loss: 0.2043 - val_acc: 0.9109\n",
            "Epoch 92/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9104 - val_loss: 0.2077 - val_acc: 0.9109\n",
            "Epoch 93/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9103 - val_loss: 0.2036 - val_acc: 0.9109\n",
            "Epoch 94/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9108 - val_loss: 0.2043 - val_acc: 0.9093\n",
            "Epoch 95/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9104 - val_loss: 0.2035 - val_acc: 0.9131\n",
            "Epoch 96/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9107 - val_loss: 0.2018 - val_acc: 0.9087\n",
            "Epoch 97/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9102 - val_loss: 0.2064 - val_acc: 0.9104\n",
            "Epoch 98/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9099 - val_loss: 0.2042 - val_acc: 0.9096\n",
            "Epoch 99/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9097 - val_loss: 0.2050 - val_acc: 0.9113\n",
            "Epoch 100/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9107 - val_loss: 0.2033 - val_acc: 0.9109\n",
            "Epoch 101/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9093 - val_loss: 0.2055 - val_acc: 0.9124\n",
            "Epoch 102/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9103 - val_loss: 0.2018 - val_acc: 0.9107\n",
            "Epoch 103/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9115 - val_loss: 0.2033 - val_acc: 0.9140\n",
            "Epoch 104/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9104 - val_loss: 0.2028 - val_acc: 0.9122\n",
            "Epoch 105/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9104 - val_loss: 0.2032 - val_acc: 0.9100\n",
            "Epoch 106/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9096 - val_loss: 0.2039 - val_acc: 0.9093\n",
            "Epoch 107/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9099 - val_loss: 0.2032 - val_acc: 0.9118\n",
            "Epoch 108/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9105 - val_loss: 0.2032 - val_acc: 0.9118\n",
            "Epoch 109/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9110 - val_loss: 0.2025 - val_acc: 0.9118\n",
            "Epoch 110/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9104 - val_loss: 0.2021 - val_acc: 0.9100\n",
            "Epoch 111/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9107 - val_loss: 0.2043 - val_acc: 0.9115\n",
            "Epoch 112/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1953 - acc: 0.9103 - val_loss: 0.2028 - val_acc: 0.9126\n",
            "Epoch 113/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9103 - val_loss: 0.2041 - val_acc: 0.9133\n",
            "Epoch 114/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9106 - val_loss: 0.2030 - val_acc: 0.9100\n",
            "Epoch 115/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9097 - val_loss: 0.2029 - val_acc: 0.9096\n",
            "Epoch 116/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9105 - val_loss: 0.2031 - val_acc: 0.9120\n",
            "Epoch 117/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9109 - val_loss: 0.2022 - val_acc: 0.9115\n",
            "Epoch 118/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9103 - val_loss: 0.2020 - val_acc: 0.9113\n",
            "Epoch 119/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9105 - val_loss: 0.2036 - val_acc: 0.9122\n",
            "Epoch 120/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9104 - val_loss: 0.2041 - val_acc: 0.9131\n",
            "Epoch 121/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9103 - val_loss: 0.2032 - val_acc: 0.9098\n",
            "Epoch 122/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9102 - val_loss: 0.2031 - val_acc: 0.9113\n",
            "Epoch 123/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9107 - val_loss: 0.2051 - val_acc: 0.9098\n",
            "Epoch 124/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9113 - val_loss: 0.2037 - val_acc: 0.9133\n",
            "Epoch 125/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1953 - acc: 0.9103 - val_loss: 0.2020 - val_acc: 0.9118\n",
            "Epoch 126/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1952 - acc: 0.9104 - val_loss: 0.2033 - val_acc: 0.9120\n",
            "Epoch 127/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9106 - val_loss: 0.2049 - val_acc: 0.9113\n",
            "Epoch 128/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9108 - val_loss: 0.2032 - val_acc: 0.9120\n",
            "Epoch 129/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9111 - val_loss: 0.2035 - val_acc: 0.9113\n",
            "Epoch 130/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1950 - acc: 0.9106 - val_loss: 0.2027 - val_acc: 0.9120\n",
            "Epoch 131/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9104 - val_loss: 0.2014 - val_acc: 0.9089\n",
            "Epoch 132/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9109 - val_loss: 0.2043 - val_acc: 0.9129\n",
            "Epoch 133/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9099 - val_loss: 0.2037 - val_acc: 0.9109\n",
            "Epoch 134/150\n",
            "40689/40689 [==============================] - 1s 23us/step - loss: 0.1951 - acc: 0.9102 - val_loss: 0.2049 - val_acc: 0.9084\n",
            "Epoch 135/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1950 - acc: 0.9110 - val_loss: 0.2055 - val_acc: 0.9129\n",
            "Epoch 136/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1951 - acc: 0.9103 - val_loss: 0.2033 - val_acc: 0.9087\n",
            "Epoch 137/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1952 - acc: 0.9111 - val_loss: 0.2041 - val_acc: 0.9109\n",
            "Epoch 138/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9097 - val_loss: 0.2034 - val_acc: 0.9111\n",
            "Epoch 139/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9105 - val_loss: 0.2030 - val_acc: 0.9113\n",
            "Epoch 140/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9101 - val_loss: 0.2040 - val_acc: 0.9118\n",
            "Epoch 141/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1951 - acc: 0.9108 - val_loss: 0.2049 - val_acc: 0.9113\n",
            "Epoch 142/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9103 - val_loss: 0.2035 - val_acc: 0.9120\n",
            "Epoch 143/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9108 - val_loss: 0.2038 - val_acc: 0.9124\n",
            "Epoch 144/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1953 - acc: 0.9106 - val_loss: 0.2035 - val_acc: 0.9089\n",
            "Epoch 145/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9105 - val_loss: 0.2072 - val_acc: 0.9118\n",
            "Epoch 146/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9110 - val_loss: 0.2039 - val_acc: 0.9124\n",
            "Epoch 147/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1952 - acc: 0.9104 - val_loss: 0.2042 - val_acc: 0.9102\n",
            "Epoch 148/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9109 - val_loss: 0.2046 - val_acc: 0.9129\n",
            "Epoch 149/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1948 - acc: 0.9108 - val_loss: 0.2031 - val_acc: 0.9109\n",
            "Epoch 150/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1953 - acc: 0.9103 - val_loss: 0.2061 - val_acc: 0.9096\n",
            "acc: 90.96%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 141us/step - loss: 0.2628 - acc: 0.8849 - val_loss: 0.2353 - val_acc: 0.8967\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2178 - acc: 0.9011 - val_loss: 0.2269 - val_acc: 0.8994\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2126 - acc: 0.9030 - val_loss: 0.2269 - val_acc: 0.8958\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2098 - acc: 0.9035 - val_loss: 0.2244 - val_acc: 0.8974\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2078 - acc: 0.9045 - val_loss: 0.2200 - val_acc: 0.8987\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2061 - acc: 0.9049 - val_loss: 0.2262 - val_acc: 0.9002\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2061 - acc: 0.9055 - val_loss: 0.2168 - val_acc: 0.8994\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2049 - acc: 0.9050 - val_loss: 0.2209 - val_acc: 0.9011\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2041 - acc: 0.9057 - val_loss: 0.2193 - val_acc: 0.9007\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2031 - acc: 0.9063 - val_loss: 0.2221 - val_acc: 0.8983\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2027 - acc: 0.9067 - val_loss: 0.2174 - val_acc: 0.9002\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2021 - acc: 0.9075 - val_loss: 0.2182 - val_acc: 0.8980\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2019 - acc: 0.9077 - val_loss: 0.2184 - val_acc: 0.8985\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2015 - acc: 0.9074 - val_loss: 0.2189 - val_acc: 0.8994\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2011 - acc: 0.9078 - val_loss: 0.2214 - val_acc: 0.8978\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2006 - acc: 0.9085 - val_loss: 0.2234 - val_acc: 0.8967\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2005 - acc: 0.9081 - val_loss: 0.2191 - val_acc: 0.9018\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2001 - acc: 0.9080 - val_loss: 0.2192 - val_acc: 0.8998\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2006 - acc: 0.9080 - val_loss: 0.2180 - val_acc: 0.8996\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2001 - acc: 0.9088 - val_loss: 0.2168 - val_acc: 0.8991\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1995 - acc: 0.9078 - val_loss: 0.2151 - val_acc: 0.9011\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2000 - acc: 0.9075 - val_loss: 0.2150 - val_acc: 0.9013\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2000 - acc: 0.9085 - val_loss: 0.2144 - val_acc: 0.8983\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1996 - acc: 0.9087 - val_loss: 0.2162 - val_acc: 0.9000\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9080 - val_loss: 0.2177 - val_acc: 0.8991\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1996 - acc: 0.9079 - val_loss: 0.2158 - val_acc: 0.8996\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1995 - acc: 0.9081 - val_loss: 0.2170 - val_acc: 0.9005\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1990 - acc: 0.9078 - val_loss: 0.2177 - val_acc: 0.8998\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1994 - acc: 0.9075 - val_loss: 0.2181 - val_acc: 0.8983\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1987 - acc: 0.9084 - val_loss: 0.2159 - val_acc: 0.8998\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9078 - val_loss: 0.2160 - val_acc: 0.8989\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9077 - val_loss: 0.2206 - val_acc: 0.9013\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9084 - val_loss: 0.2154 - val_acc: 0.9011\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1988 - acc: 0.9071 - val_loss: 0.2136 - val_acc: 0.9020\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.2178 - val_acc: 0.8996\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9083 - val_loss: 0.2174 - val_acc: 0.9005\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.2160 - val_acc: 0.9033\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1984 - acc: 0.9078 - val_loss: 0.2173 - val_acc: 0.9007\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1985 - acc: 0.9080 - val_loss: 0.2152 - val_acc: 0.9027\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1984 - acc: 0.9087 - val_loss: 0.2194 - val_acc: 0.8980\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1985 - acc: 0.9075 - val_loss: 0.2180 - val_acc: 0.9007\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1983 - acc: 0.9079 - val_loss: 0.2183 - val_acc: 0.8983\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.2187 - val_acc: 0.9002\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1982 - acc: 0.9086 - val_loss: 0.2182 - val_acc: 0.9011\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1981 - acc: 0.9075 - val_loss: 0.2158 - val_acc: 0.9011\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1977 - acc: 0.9073 - val_loss: 0.2152 - val_acc: 0.9020\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.2144 - val_acc: 0.9038\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9081 - val_loss: 0.2204 - val_acc: 0.9029\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9085 - val_loss: 0.2169 - val_acc: 0.9009\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1978 - acc: 0.9081 - val_loss: 0.2165 - val_acc: 0.8998\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9081 - val_loss: 0.2165 - val_acc: 0.9007\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1978 - acc: 0.9083 - val_loss: 0.2183 - val_acc: 0.9000\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9085 - val_loss: 0.2178 - val_acc: 0.9018\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9085 - val_loss: 0.2161 - val_acc: 0.9007\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9089 - val_loss: 0.2177 - val_acc: 0.9002\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9083 - val_loss: 0.2150 - val_acc: 0.9011\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9078 - val_loss: 0.2196 - val_acc: 0.8965\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1977 - acc: 0.9085 - val_loss: 0.2149 - val_acc: 0.9018\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1975 - acc: 0.9084 - val_loss: 0.2163 - val_acc: 0.9013\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1977 - acc: 0.9080 - val_loss: 0.2166 - val_acc: 0.9020\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2187 - val_acc: 0.9044\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9091 - val_loss: 0.2146 - val_acc: 0.9044\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1973 - acc: 0.9082 - val_loss: 0.2139 - val_acc: 0.9031\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1973 - acc: 0.9087 - val_loss: 0.2161 - val_acc: 0.8985\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9083 - val_loss: 0.2177 - val_acc: 0.9000\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9090 - val_loss: 0.2144 - val_acc: 0.9027\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1972 - acc: 0.9089 - val_loss: 0.2136 - val_acc: 0.9022\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9089 - val_loss: 0.2167 - val_acc: 0.9000\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9092 - val_loss: 0.2144 - val_acc: 0.9013\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9088 - val_loss: 0.2139 - val_acc: 0.9020\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9083 - val_loss: 0.2134 - val_acc: 0.9007\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9082 - val_loss: 0.2146 - val_acc: 0.8998\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9084 - val_loss: 0.2146 - val_acc: 0.8994\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1973 - acc: 0.9090 - val_loss: 0.2150 - val_acc: 0.8987\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9084 - val_loss: 0.2184 - val_acc: 0.9000\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9090 - val_loss: 0.2201 - val_acc: 0.8998\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1968 - acc: 0.9083 - val_loss: 0.2161 - val_acc: 0.9022\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9086 - val_loss: 0.2168 - val_acc: 0.9033\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9086 - val_loss: 0.2153 - val_acc: 0.8980\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9092 - val_loss: 0.2176 - val_acc: 0.9029\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9090 - val_loss: 0.2145 - val_acc: 0.9033\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9087 - val_loss: 0.2188 - val_acc: 0.9013\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9088 - val_loss: 0.2148 - val_acc: 0.9025\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9094 - val_loss: 0.2160 - val_acc: 0.8998\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9084 - val_loss: 0.2139 - val_acc: 0.9022\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1967 - acc: 0.9079 - val_loss: 0.2173 - val_acc: 0.9005\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9090 - val_loss: 0.2160 - val_acc: 0.9007\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9087 - val_loss: 0.2145 - val_acc: 0.8987\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9094 - val_loss: 0.2173 - val_acc: 0.9016\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9086 - val_loss: 0.2162 - val_acc: 0.9031\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1963 - acc: 0.9090 - val_loss: 0.2140 - val_acc: 0.9009\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9097 - val_loss: 0.2146 - val_acc: 0.9036\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9094 - val_loss: 0.2169 - val_acc: 0.9027\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9092 - val_loss: 0.2148 - val_acc: 0.9005\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9086 - val_loss: 0.2151 - val_acc: 0.9025\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9086 - val_loss: 0.2143 - val_acc: 0.9022\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9084 - val_loss: 0.2155 - val_acc: 0.9031\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9088 - val_loss: 0.2129 - val_acc: 0.9018\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9085 - val_loss: 0.2177 - val_acc: 0.9009\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9090 - val_loss: 0.2152 - val_acc: 0.8989\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9095 - val_loss: 0.2187 - val_acc: 0.9038\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9080 - val_loss: 0.2159 - val_acc: 0.9016\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9088 - val_loss: 0.2169 - val_acc: 0.9000\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9094 - val_loss: 0.2157 - val_acc: 0.9018\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9089 - val_loss: 0.2160 - val_acc: 0.9025\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9087 - val_loss: 0.2164 - val_acc: 0.9036\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9093 - val_loss: 0.2155 - val_acc: 0.9007\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9089 - val_loss: 0.2139 - val_acc: 0.9009\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9087 - val_loss: 0.2158 - val_acc: 0.9025\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9096 - val_loss: 0.2164 - val_acc: 0.9007\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9083 - val_loss: 0.2150 - val_acc: 0.9031\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9089 - val_loss: 0.2144 - val_acc: 0.9020\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9086 - val_loss: 0.2152 - val_acc: 0.8978\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9091 - val_loss: 0.2138 - val_acc: 0.9002\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9089 - val_loss: 0.2168 - val_acc: 0.8985\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9100 - val_loss: 0.2168 - val_acc: 0.9025\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9084 - val_loss: 0.2181 - val_acc: 0.9009\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9092 - val_loss: 0.2152 - val_acc: 0.9036\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9097 - val_loss: 0.2177 - val_acc: 0.8998\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9090 - val_loss: 0.2151 - val_acc: 0.9020\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9093 - val_loss: 0.2165 - val_acc: 0.9016\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9083 - val_loss: 0.2173 - val_acc: 0.8989\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9093 - val_loss: 0.2143 - val_acc: 0.8998\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9093 - val_loss: 0.2156 - val_acc: 0.9020\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9098 - val_loss: 0.2175 - val_acc: 0.9002\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9088 - val_loss: 0.2153 - val_acc: 0.8976\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9091 - val_loss: 0.2168 - val_acc: 0.9020\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9089 - val_loss: 0.2197 - val_acc: 0.8996\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9093 - val_loss: 0.2148 - val_acc: 0.9018\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9090 - val_loss: 0.2140 - val_acc: 0.9022\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9092 - val_loss: 0.2149 - val_acc: 0.9018\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9092 - val_loss: 0.2169 - val_acc: 0.8985\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9086 - val_loss: 0.2176 - val_acc: 0.9020\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9094 - val_loss: 0.2147 - val_acc: 0.9013\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9094 - val_loss: 0.2178 - val_acc: 0.9029\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9088 - val_loss: 0.2133 - val_acc: 0.9007\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9087 - val_loss: 0.2150 - val_acc: 0.9020\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9082 - val_loss: 0.2168 - val_acc: 0.9036\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1958 - acc: 0.9090 - val_loss: 0.2153 - val_acc: 0.9009\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9088 - val_loss: 0.2173 - val_acc: 0.9016\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9095 - val_loss: 0.2151 - val_acc: 0.8998\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9098 - val_loss: 0.2145 - val_acc: 0.8991\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9085 - val_loss: 0.2126 - val_acc: 0.9007\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9088 - val_loss: 0.2177 - val_acc: 0.8983\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9095 - val_loss: 0.2149 - val_acc: 0.9016\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9096 - val_loss: 0.2168 - val_acc: 0.9029\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9085 - val_loss: 0.2139 - val_acc: 0.9005\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9090 - val_loss: 0.2153 - val_acc: 0.9025\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9093 - val_loss: 0.2175 - val_acc: 0.9011\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9093 - val_loss: 0.2179 - val_acc: 0.9018\n",
            "acc: 90.18%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 143us/step - loss: 0.2483 - acc: 0.8971 - val_loss: 0.2098 - val_acc: 0.9020\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2182 - acc: 0.9032 - val_loss: 0.2092 - val_acc: 0.9044\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2135 - acc: 0.9049 - val_loss: 0.2068 - val_acc: 0.9031\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2107 - acc: 0.9055 - val_loss: 0.2040 - val_acc: 0.9040\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2086 - acc: 0.9056 - val_loss: 0.2051 - val_acc: 0.9033\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2068 - acc: 0.9069 - val_loss: 0.2044 - val_acc: 0.9067\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2060 - acc: 0.9060 - val_loss: 0.2037 - val_acc: 0.9060\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2053 - acc: 0.9058 - val_loss: 0.2053 - val_acc: 0.9064\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2053 - acc: 0.9068 - val_loss: 0.2002 - val_acc: 0.9078\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2039 - acc: 0.9072 - val_loss: 0.2017 - val_acc: 0.9016\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2040 - acc: 0.9075 - val_loss: 0.1982 - val_acc: 0.9047\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2030 - acc: 0.9075 - val_loss: 0.1949 - val_acc: 0.9067\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2025 - acc: 0.9079 - val_loss: 0.1983 - val_acc: 0.9058\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2018 - acc: 0.9091 - val_loss: 0.1973 - val_acc: 0.9086\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2013 - acc: 0.9092 - val_loss: 0.1953 - val_acc: 0.9093\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2008 - acc: 0.9088 - val_loss: 0.1967 - val_acc: 0.9100\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2003 - acc: 0.9091 - val_loss: 0.1957 - val_acc: 0.9089\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2005 - acc: 0.9098 - val_loss: 0.1955 - val_acc: 0.9102\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9098 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 2s 37us/step - loss: 0.2000 - acc: 0.9099 - val_loss: 0.1946 - val_acc: 0.9104\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 32us/step - loss: 0.1997 - acc: 0.9094 - val_loss: 0.1952 - val_acc: 0.9084\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1998 - acc: 0.9099 - val_loss: 0.1942 - val_acc: 0.9082\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9100 - val_loss: 0.1976 - val_acc: 0.9084\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9101 - val_loss: 0.1955 - val_acc: 0.9071\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1991 - acc: 0.9096 - val_loss: 0.1951 - val_acc: 0.9080\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9094 - val_loss: 0.1974 - val_acc: 0.9093\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9100 - val_loss: 0.1968 - val_acc: 0.9080\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9100 - val_loss: 0.1933 - val_acc: 0.9100\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9102 - val_loss: 0.1965 - val_acc: 0.9078\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9094 - val_loss: 0.1964 - val_acc: 0.9089\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9100 - val_loss: 0.1939 - val_acc: 0.9106\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9102 - val_loss: 0.1952 - val_acc: 0.9111\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9099 - val_loss: 0.1952 - val_acc: 0.9086\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9106 - val_loss: 0.1966 - val_acc: 0.9073\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9106 - val_loss: 0.1953 - val_acc: 0.9089\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9101 - val_loss: 0.1945 - val_acc: 0.9093\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9097 - val_loss: 0.1952 - val_acc: 0.9093\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1986 - acc: 0.9090 - val_loss: 0.1955 - val_acc: 0.9117\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9095 - val_loss: 0.1939 - val_acc: 0.9098\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9102 - val_loss: 0.1944 - val_acc: 0.9089\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9097 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9101 - val_loss: 0.1962 - val_acc: 0.9089\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9098 - val_loss: 0.1930 - val_acc: 0.9100\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9102 - val_loss: 0.1936 - val_acc: 0.9078\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9098 - val_loss: 0.1942 - val_acc: 0.9080\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9099 - val_loss: 0.1963 - val_acc: 0.9086\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9095 - val_loss: 0.1958 - val_acc: 0.9044\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9110 - val_loss: 0.1947 - val_acc: 0.9098\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1978 - acc: 0.9100 - val_loss: 0.1939 - val_acc: 0.9095\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9110 - val_loss: 0.1965 - val_acc: 0.9089\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9104 - val_loss: 0.1957 - val_acc: 0.9080\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9100 - val_loss: 0.1924 - val_acc: 0.9100\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9114 - val_loss: 0.1945 - val_acc: 0.9117\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9104 - val_loss: 0.1950 - val_acc: 0.9109\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9102 - val_loss: 0.1930 - val_acc: 0.9126\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9109 - val_loss: 0.1938 - val_acc: 0.9093\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9101 - val_loss: 0.1946 - val_acc: 0.9100\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9103 - val_loss: 0.1958 - val_acc: 0.9111\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9095 - val_loss: 0.1942 - val_acc: 0.9093\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9103 - val_loss: 0.1951 - val_acc: 0.9069\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9105 - val_loss: 0.1946 - val_acc: 0.9091\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9101 - val_loss: 0.1941 - val_acc: 0.9111\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9107 - val_loss: 0.1954 - val_acc: 0.9115\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9103 - val_loss: 0.1935 - val_acc: 0.9095\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9095 - val_loss: 0.1939 - val_acc: 0.9104\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9111 - val_loss: 0.1932 - val_acc: 0.9095\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9101 - val_loss: 0.1966 - val_acc: 0.9098\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9105 - val_loss: 0.1941 - val_acc: 0.9069\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9112 - val_loss: 0.1976 - val_acc: 0.9060\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9105 - val_loss: 0.1941 - val_acc: 0.9111\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9105 - val_loss: 0.1940 - val_acc: 0.9091\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9112 - val_loss: 0.1946 - val_acc: 0.9086\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9101 - val_loss: 0.1943 - val_acc: 0.9082\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9108 - val_loss: 0.1937 - val_acc: 0.9113\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9101 - val_loss: 0.1935 - val_acc: 0.9102\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9107 - val_loss: 0.1937 - val_acc: 0.9093\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9107 - val_loss: 0.1951 - val_acc: 0.9106\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9104 - val_loss: 0.1950 - val_acc: 0.9089\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9098 - val_loss: 0.1955 - val_acc: 0.9115\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9109 - val_loss: 0.1946 - val_acc: 0.9102\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9113 - val_loss: 0.1946 - val_acc: 0.9109\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.1944 - val_acc: 0.9067\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9101 - val_loss: 0.1954 - val_acc: 0.9075\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9100 - val_loss: 0.1961 - val_acc: 0.9124\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9102 - val_loss: 0.1950 - val_acc: 0.9093\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9106 - val_loss: 0.1945 - val_acc: 0.9102\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9103 - val_loss: 0.1945 - val_acc: 0.9084\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9108 - val_loss: 0.1932 - val_acc: 0.9098\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9102 - val_loss: 0.1937 - val_acc: 0.9117\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9106 - val_loss: 0.1943 - val_acc: 0.9086\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9108 - val_loss: 0.1941 - val_acc: 0.9082\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9113 - val_loss: 0.1949 - val_acc: 0.9084\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9106 - val_loss: 0.1940 - val_acc: 0.9089\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9101 - val_loss: 0.1960 - val_acc: 0.9091\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9119 - val_loss: 0.1928 - val_acc: 0.9095\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9110 - val_loss: 0.1937 - val_acc: 0.9091\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9105 - val_loss: 0.1946 - val_acc: 0.9084\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9108 - val_loss: 0.1943 - val_acc: 0.9098\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9105 - val_loss: 0.1952 - val_acc: 0.9111\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9107 - val_loss: 0.1945 - val_acc: 0.9086\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9109 - val_loss: 0.1946 - val_acc: 0.9117\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9111 - val_loss: 0.1942 - val_acc: 0.9104\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9102 - val_loss: 0.1937 - val_acc: 0.9120\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9104 - val_loss: 0.1941 - val_acc: 0.9091\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9108 - val_loss: 0.1944 - val_acc: 0.9093\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9108 - val_loss: 0.1944 - val_acc: 0.9102\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9106 - val_loss: 0.1939 - val_acc: 0.9111\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9104 - val_loss: 0.1932 - val_acc: 0.9093\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9107 - val_loss: 0.1944 - val_acc: 0.9115\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9110 - val_loss: 0.1945 - val_acc: 0.9078\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9104 - val_loss: 0.1945 - val_acc: 0.9089\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9106 - val_loss: 0.1930 - val_acc: 0.9102\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9112 - val_loss: 0.1969 - val_acc: 0.9111\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9110 - val_loss: 0.1941 - val_acc: 0.9095\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1958 - acc: 0.9107 - val_loss: 0.1932 - val_acc: 0.9084\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9111 - val_loss: 0.1942 - val_acc: 0.9100\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1955 - val_acc: 0.9104\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9100 - val_loss: 0.1961 - val_acc: 0.9104\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9105 - val_loss: 0.1964 - val_acc: 0.9102\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9107 - val_loss: 0.1929 - val_acc: 0.9098\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9106 - val_loss: 0.1949 - val_acc: 0.9104\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9107 - val_loss: 0.1939 - val_acc: 0.9106\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9108 - val_loss: 0.1942 - val_acc: 0.9106\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9112 - val_loss: 0.1931 - val_acc: 0.9104\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9114 - val_loss: 0.1981 - val_acc: 0.9120\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9115 - val_loss: 0.1962 - val_acc: 0.9091\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1959 - acc: 0.9109 - val_loss: 0.1938 - val_acc: 0.9084\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9114 - val_loss: 0.1961 - val_acc: 0.9078\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1960 - acc: 0.9111 - val_loss: 0.1957 - val_acc: 0.9106\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9110 - val_loss: 0.1938 - val_acc: 0.9084\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9111 - val_loss: 0.1956 - val_acc: 0.9082\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9102 - val_loss: 0.1943 - val_acc: 0.9089\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9106 - val_loss: 0.1934 - val_acc: 0.9091\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1962 - acc: 0.9108 - val_loss: 0.1948 - val_acc: 0.9111\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9110 - val_loss: 0.1939 - val_acc: 0.9082\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9110 - val_loss: 0.1924 - val_acc: 0.9080\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9112 - val_loss: 0.1955 - val_acc: 0.9100\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1958 - acc: 0.9107 - val_loss: 0.1938 - val_acc: 0.9069\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9105 - val_loss: 0.1919 - val_acc: 0.9106\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9112 - val_loss: 0.1953 - val_acc: 0.9073\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1941 - val_acc: 0.9086\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1947 - val_acc: 0.9109\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9109 - val_loss: 0.1953 - val_acc: 0.9098\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1960 - acc: 0.9111 - val_loss: 0.1935 - val_acc: 0.9064\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9111 - val_loss: 0.1925 - val_acc: 0.9093\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9112 - val_loss: 0.1929 - val_acc: 0.9089\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9107 - val_loss: 0.1930 - val_acc: 0.9084\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9109 - val_loss: 0.1933 - val_acc: 0.9069\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1950 - val_acc: 0.9078\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9111 - val_loss: 0.1946 - val_acc: 0.9098\n",
            "acc: 90.98%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 149us/step - loss: 0.2972 - acc: 0.8691 - val_loss: 0.2143 - val_acc: 0.9073\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2205 - acc: 0.9016 - val_loss: 0.2110 - val_acc: 0.9058\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2153 - acc: 0.9040 - val_loss: 0.2056 - val_acc: 0.9124\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2124 - acc: 0.9040 - val_loss: 0.2082 - val_acc: 0.9084\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2107 - acc: 0.9048 - val_loss: 0.2039 - val_acc: 0.9100\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2090 - acc: 0.9069 - val_loss: 0.2037 - val_acc: 0.9137\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2084 - acc: 0.9063 - val_loss: 0.2020 - val_acc: 0.9122\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2082 - acc: 0.9067 - val_loss: 0.2012 - val_acc: 0.9115\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2073 - acc: 0.9068 - val_loss: 0.2001 - val_acc: 0.9109\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2066 - acc: 0.9072 - val_loss: 0.2016 - val_acc: 0.9086\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2064 - acc: 0.9068 - val_loss: 0.1981 - val_acc: 0.9120\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2059 - acc: 0.9071 - val_loss: 0.2020 - val_acc: 0.9086\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2059 - acc: 0.9066 - val_loss: 0.2007 - val_acc: 0.9111\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2052 - acc: 0.9070 - val_loss: 0.1981 - val_acc: 0.9120\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2054 - acc: 0.9072 - val_loss: 0.1971 - val_acc: 0.9122\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2048 - acc: 0.9067 - val_loss: 0.1963 - val_acc: 0.9117\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2044 - acc: 0.9066 - val_loss: 0.1992 - val_acc: 0.9117\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2039 - acc: 0.9069 - val_loss: 0.1980 - val_acc: 0.9117\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2037 - acc: 0.9080 - val_loss: 0.1944 - val_acc: 0.9104\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2036 - acc: 0.9068 - val_loss: 0.1980 - val_acc: 0.9098\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2032 - acc: 0.9072 - val_loss: 0.1973 - val_acc: 0.9131\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2030 - acc: 0.9072 - val_loss: 0.1969 - val_acc: 0.9106\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9074 - val_loss: 0.1942 - val_acc: 0.9106\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2029 - acc: 0.9068 - val_loss: 0.1976 - val_acc: 0.9111\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2020 - acc: 0.9079 - val_loss: 0.1961 - val_acc: 0.9120\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2025 - acc: 0.9073 - val_loss: 0.1997 - val_acc: 0.9111\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2023 - acc: 0.9069 - val_loss: 0.1951 - val_acc: 0.9102\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2016 - acc: 0.9072 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2016 - acc: 0.9077 - val_loss: 0.1966 - val_acc: 0.9084\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2011 - acc: 0.9081 - val_loss: 0.1991 - val_acc: 0.9098\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2013 - acc: 0.9069 - val_loss: 0.1958 - val_acc: 0.9115\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9070 - val_loss: 0.1973 - val_acc: 0.9078\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9071 - val_loss: 0.1967 - val_acc: 0.9102\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2009 - acc: 0.9071 - val_loss: 0.1955 - val_acc: 0.9115\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2005 - acc: 0.9084 - val_loss: 0.1961 - val_acc: 0.9111\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2005 - acc: 0.9081 - val_loss: 0.1958 - val_acc: 0.9102\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2006 - acc: 0.9084 - val_loss: 0.1966 - val_acc: 0.9089\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9074 - val_loss: 0.1945 - val_acc: 0.9098\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9081 - val_loss: 0.1974 - val_acc: 0.9082\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9072 - val_loss: 0.1951 - val_acc: 0.9095\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9079 - val_loss: 0.1960 - val_acc: 0.9093\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2002 - acc: 0.9075 - val_loss: 0.1959 - val_acc: 0.9113\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9083 - val_loss: 0.1962 - val_acc: 0.9111\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9079 - val_loss: 0.1969 - val_acc: 0.9091\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2004 - acc: 0.9072 - val_loss: 0.1976 - val_acc: 0.9109\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9079 - val_loss: 0.1978 - val_acc: 0.9078\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1998 - acc: 0.9084 - val_loss: 0.1963 - val_acc: 0.9075\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1999 - acc: 0.9077 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1997 - acc: 0.9080 - val_loss: 0.1970 - val_acc: 0.9086\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9078 - val_loss: 0.1976 - val_acc: 0.9073\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9077 - val_loss: 0.1988 - val_acc: 0.9058\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1998 - acc: 0.9078 - val_loss: 0.1979 - val_acc: 0.9089\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1996 - acc: 0.9080 - val_loss: 0.1959 - val_acc: 0.9086\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9089 - val_loss: 0.1965 - val_acc: 0.9093\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9078 - val_loss: 0.1989 - val_acc: 0.9078\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1999 - acc: 0.9083 - val_loss: 0.2021 - val_acc: 0.9091\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9080 - val_loss: 0.1994 - val_acc: 0.9036\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9082 - val_loss: 0.1972 - val_acc: 0.9078\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9073 - val_loss: 0.1985 - val_acc: 0.9084\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1996 - acc: 0.9087 - val_loss: 0.1982 - val_acc: 0.9098\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9074 - val_loss: 0.1967 - val_acc: 0.9095\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9088 - val_loss: 0.1973 - val_acc: 0.9109\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9085 - val_loss: 0.1954 - val_acc: 0.9098\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9085 - val_loss: 0.1958 - val_acc: 0.9069\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9082 - val_loss: 0.1979 - val_acc: 0.9071\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9095 - val_loss: 0.1986 - val_acc: 0.9053\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9085 - val_loss: 0.1974 - val_acc: 0.9122\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9095\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9083 - val_loss: 0.1977 - val_acc: 0.9069\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9080 - val_loss: 0.1959 - val_acc: 0.9080\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9085 - val_loss: 0.1975 - val_acc: 0.9106\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9088 - val_loss: 0.1967 - val_acc: 0.9086\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9077 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9077 - val_loss: 0.1985 - val_acc: 0.9098\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9081 - val_loss: 0.1964 - val_acc: 0.9100\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9082 - val_loss: 0.1966 - val_acc: 0.9115\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9086 - val_loss: 0.1960 - val_acc: 0.9106\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9089 - val_loss: 0.1949 - val_acc: 0.9086\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1996 - acc: 0.9075 - val_loss: 0.1972 - val_acc: 0.9102\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9091 - val_loss: 0.1957 - val_acc: 0.9124\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9086 - val_loss: 0.1969 - val_acc: 0.9115\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9102\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9087 - val_loss: 0.1974 - val_acc: 0.9084\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.1963 - val_acc: 0.9104\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9084 - val_loss: 0.1957 - val_acc: 0.9095\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1973 - val_acc: 0.9106\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9088 - val_loss: 0.1971 - val_acc: 0.9082\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9079 - val_loss: 0.1985 - val_acc: 0.9106\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9081 - val_loss: 0.1998 - val_acc: 0.9111\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9084 - val_loss: 0.1960 - val_acc: 0.9073\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9088 - val_loss: 0.1966 - val_acc: 0.9102\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9084 - val_loss: 0.1977 - val_acc: 0.9078\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9084 - val_loss: 0.1962 - val_acc: 0.9075\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.1969 - val_acc: 0.9104\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9082 - val_loss: 0.1960 - val_acc: 0.9086\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9087 - val_loss: 0.1957 - val_acc: 0.9102\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9080 - val_loss: 0.1978 - val_acc: 0.9075\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1988 - val_acc: 0.9126\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9082 - val_loss: 0.1957 - val_acc: 0.9084\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9088 - val_loss: 0.1958 - val_acc: 0.9095\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9088 - val_loss: 0.1958 - val_acc: 0.9102\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.1966 - val_acc: 0.9100\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9086 - val_loss: 0.1961 - val_acc: 0.9111\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1969 - val_acc: 0.9111\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9087 - val_loss: 0.1974 - val_acc: 0.9091\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9087 - val_loss: 0.1942 - val_acc: 0.9100\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9090 - val_loss: 0.1955 - val_acc: 0.9086\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9080 - val_loss: 0.1969 - val_acc: 0.9069\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9092 - val_loss: 0.1951 - val_acc: 0.9069\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9087 - val_loss: 0.1957 - val_acc: 0.9080\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9090 - val_loss: 0.1974 - val_acc: 0.9106\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9089 - val_loss: 0.1968 - val_acc: 0.9093\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9087 - val_loss: 0.1961 - val_acc: 0.9093\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9086 - val_loss: 0.1970 - val_acc: 0.9091\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9088 - val_loss: 0.1962 - val_acc: 0.9129\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9091 - val_loss: 0.1962 - val_acc: 0.9124\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9089 - val_loss: 0.1956 - val_acc: 0.9120\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.1943 - val_acc: 0.9095\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1949 - val_acc: 0.9080\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9089 - val_loss: 0.1965 - val_acc: 0.9122\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9079 - val_loss: 0.2027 - val_acc: 0.9080\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.1971 - val_acc: 0.9120\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9089 - val_loss: 0.1967 - val_acc: 0.9098\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9088 - val_loss: 0.1961 - val_acc: 0.9073\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9086 - val_loss: 0.1957 - val_acc: 0.9122\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9086 - val_loss: 0.1950 - val_acc: 0.9084\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9082 - val_loss: 0.1946 - val_acc: 0.9091\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9089 - val_loss: 0.1954 - val_acc: 0.9098\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9095 - val_loss: 0.1965 - val_acc: 0.9102\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9086 - val_loss: 0.1953 - val_acc: 0.9098\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9089 - val_loss: 0.1947 - val_acc: 0.9104\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.1954 - val_acc: 0.9102\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.1967 - val_acc: 0.9102\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9088 - val_loss: 0.1948 - val_acc: 0.9106\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9090 - val_loss: 0.1974 - val_acc: 0.9100\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9089 - val_loss: 0.1962 - val_acc: 0.9102\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9087 - val_loss: 0.1962 - val_acc: 0.9106\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9088 - val_loss: 0.1968 - val_acc: 0.9113\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9085 - val_loss: 0.1966 - val_acc: 0.9093\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9088 - val_loss: 0.1962 - val_acc: 0.9106\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9085 - val_loss: 0.1959 - val_acc: 0.9115\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9093 - val_loss: 0.1969 - val_acc: 0.9120\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9085 - val_loss: 0.1978 - val_acc: 0.9093\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9091 - val_loss: 0.1960 - val_acc: 0.9113\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9088 - val_loss: 0.1943 - val_acc: 0.9104\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9082 - val_loss: 0.1953 - val_acc: 0.9111\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9085 - val_loss: 0.1962 - val_acc: 0.9102\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9095 - val_loss: 0.1973 - val_acc: 0.9089\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9083 - val_loss: 0.1970 - val_acc: 0.9089\n",
            "acc: 90.89%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 150us/step - loss: 0.2520 - acc: 0.8925 - val_loss: 0.2233 - val_acc: 0.9027\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2176 - acc: 0.9017 - val_loss: 0.2174 - val_acc: 0.9058\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2140 - acc: 0.9033 - val_loss: 0.2175 - val_acc: 0.9047\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2116 - acc: 0.9048 - val_loss: 0.2162 - val_acc: 0.9049\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2100 - acc: 0.9060 - val_loss: 0.2148 - val_acc: 0.9086\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2080 - acc: 0.9039 - val_loss: 0.2130 - val_acc: 0.9084\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2069 - acc: 0.9049 - val_loss: 0.2120 - val_acc: 0.9067\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2059 - acc: 0.9053 - val_loss: 0.2111 - val_acc: 0.9084\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2055 - acc: 0.9058 - val_loss: 0.2115 - val_acc: 0.9078\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9065 - val_loss: 0.2126 - val_acc: 0.9067\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2035 - acc: 0.9067 - val_loss: 0.2104 - val_acc: 0.9064\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2031 - acc: 0.9066 - val_loss: 0.2096 - val_acc: 0.9067\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9069 - val_loss: 0.2094 - val_acc: 0.9084\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2025 - acc: 0.9068 - val_loss: 0.2094 - val_acc: 0.9056\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2024 - acc: 0.9070 - val_loss: 0.2088 - val_acc: 0.9060\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9071 - val_loss: 0.2086 - val_acc: 0.9075\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2018 - acc: 0.9074 - val_loss: 0.2104 - val_acc: 0.9089\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2011 - acc: 0.9083 - val_loss: 0.2093 - val_acc: 0.9086\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9087 - val_loss: 0.2079 - val_acc: 0.9104\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9083 - val_loss: 0.2079 - val_acc: 0.9073\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9072 - val_loss: 0.2102 - val_acc: 0.9049\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9082 - val_loss: 0.2095 - val_acc: 0.9064\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1999 - acc: 0.9084 - val_loss: 0.2075 - val_acc: 0.9086\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2005 - acc: 0.9078 - val_loss: 0.2071 - val_acc: 0.9078\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9078 - val_loss: 0.2074 - val_acc: 0.9080\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1997 - acc: 0.9087 - val_loss: 0.2067 - val_acc: 0.9124\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9093 - val_loss: 0.2071 - val_acc: 0.9058\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9083 - val_loss: 0.2087 - val_acc: 0.9106\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9086 - val_loss: 0.2062 - val_acc: 0.9104\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2092 - val_acc: 0.9073\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9097 - val_loss: 0.2101 - val_acc: 0.9102\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9087 - val_loss: 0.2072 - val_acc: 0.9078\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.2071 - val_acc: 0.9080\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1989 - acc: 0.9089 - val_loss: 0.2088 - val_acc: 0.9091\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9092 - val_loss: 0.2073 - val_acc: 0.9093\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9092 - val_loss: 0.2085 - val_acc: 0.9080\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9084 - val_loss: 0.2048 - val_acc: 0.9080\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9090 - val_loss: 0.2102 - val_acc: 0.9086\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9087 - val_loss: 0.2081 - val_acc: 0.9078\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9092 - val_loss: 0.2074 - val_acc: 0.9075\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9088 - val_loss: 0.2062 - val_acc: 0.9080\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9092 - val_loss: 0.2065 - val_acc: 0.9089\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9088 - val_loss: 0.2055 - val_acc: 0.9080\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9087 - val_loss: 0.2064 - val_acc: 0.9093\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9095 - val_loss: 0.2066 - val_acc: 0.9100\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9089 - val_loss: 0.2044 - val_acc: 0.9084\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9085 - val_loss: 0.2059 - val_acc: 0.9073\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9088 - val_loss: 0.2061 - val_acc: 0.9073\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9091 - val_loss: 0.2058 - val_acc: 0.9080\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9094 - val_loss: 0.2081 - val_acc: 0.9049\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9089 - val_loss: 0.2071 - val_acc: 0.9095\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9096 - val_loss: 0.2069 - val_acc: 0.9080\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9094 - val_loss: 0.2080 - val_acc: 0.9062\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9094 - val_loss: 0.2077 - val_acc: 0.9117\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9096 - val_loss: 0.2064 - val_acc: 0.9086\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9091 - val_loss: 0.2078 - val_acc: 0.9084\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9086 - val_loss: 0.2062 - val_acc: 0.9084\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9093 - val_loss: 0.2051 - val_acc: 0.9102\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9097 - val_loss: 0.2062 - val_acc: 0.9100\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9092 - val_loss: 0.2058 - val_acc: 0.9080\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9092 - val_loss: 0.2071 - val_acc: 0.9093\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9089 - val_loss: 0.2085 - val_acc: 0.9084\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9092 - val_loss: 0.2069 - val_acc: 0.9078\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9101 - val_loss: 0.2074 - val_acc: 0.9089\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9102 - val_loss: 0.2086 - val_acc: 0.9095\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9092 - val_loss: 0.2063 - val_acc: 0.9095\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9095 - val_loss: 0.2080 - val_acc: 0.9086\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9092 - val_loss: 0.2072 - val_acc: 0.9086\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9098 - val_loss: 0.2094 - val_acc: 0.9095\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9093 - val_loss: 0.2068 - val_acc: 0.9086\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9097 - val_loss: 0.2073 - val_acc: 0.9091\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9101 - val_loss: 0.2075 - val_acc: 0.9095\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9099 - val_loss: 0.2066 - val_acc: 0.9106\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9100 - val_loss: 0.2067 - val_acc: 0.9095\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9095 - val_loss: 0.2082 - val_acc: 0.9082\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9096 - val_loss: 0.2085 - val_acc: 0.9095\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.2058 - val_acc: 0.9098\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9094 - val_loss: 0.2068 - val_acc: 0.9080\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9097 - val_loss: 0.2065 - val_acc: 0.9080\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9097 - val_loss: 0.2086 - val_acc: 0.9071\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9096 - val_loss: 0.2088 - val_acc: 0.9100\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9102 - val_loss: 0.2081 - val_acc: 0.9071\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9096 - val_loss: 0.2080 - val_acc: 0.9075\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9099 - val_loss: 0.2060 - val_acc: 0.9078\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9097 - val_loss: 0.2071 - val_acc: 0.9093\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9100 - val_loss: 0.2084 - val_acc: 0.9089\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2084 - val_acc: 0.9067\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9102 - val_loss: 0.2082 - val_acc: 0.9109\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9097 - val_loss: 0.2061 - val_acc: 0.9069\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9098 - val_loss: 0.2064 - val_acc: 0.9080\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9090 - val_loss: 0.2067 - val_acc: 0.9080\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9097 - val_loss: 0.2075 - val_acc: 0.9086\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9103 - val_loss: 0.2068 - val_acc: 0.9091\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9102 - val_loss: 0.2059 - val_acc: 0.9067\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9095 - val_loss: 0.2054 - val_acc: 0.9078\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9095 - val_loss: 0.2054 - val_acc: 0.9106\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9104 - val_loss: 0.2058 - val_acc: 0.9093\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9093 - val_loss: 0.2060 - val_acc: 0.9080\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.2061 - val_acc: 0.9080\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9101 - val_loss: 0.2060 - val_acc: 0.9089\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9098 - val_loss: 0.2076 - val_acc: 0.9086\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9098 - val_loss: 0.2058 - val_acc: 0.9082\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9097 - val_loss: 0.2073 - val_acc: 0.9109\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9097 - val_loss: 0.2078 - val_acc: 0.9089\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9093 - val_loss: 0.2082 - val_acc: 0.9082\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9093 - val_loss: 0.2059 - val_acc: 0.9067\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9100 - val_loss: 0.2065 - val_acc: 0.9086\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9093 - val_loss: 0.2067 - val_acc: 0.9091\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9096 - val_loss: 0.2062 - val_acc: 0.9073\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9099 - val_loss: 0.2059 - val_acc: 0.9095\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9099 - val_loss: 0.2066 - val_acc: 0.9064\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9094 - val_loss: 0.2069 - val_acc: 0.9080\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9091 - val_loss: 0.2060 - val_acc: 0.9082\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9095 - val_loss: 0.2062 - val_acc: 0.9091\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9091 - val_loss: 0.2078 - val_acc: 0.9084\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9097 - val_loss: 0.2073 - val_acc: 0.9075\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9092 - val_loss: 0.2058 - val_acc: 0.9082\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9092 - val_loss: 0.2089 - val_acc: 0.9064\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9094 - val_loss: 0.2073 - val_acc: 0.9073\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9089 - val_loss: 0.2064 - val_acc: 0.9080\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9095 - val_loss: 0.2069 - val_acc: 0.9082\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9098 - val_loss: 0.2077 - val_acc: 0.9082\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9090 - val_loss: 0.2074 - val_acc: 0.9067\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9096 - val_loss: 0.2059 - val_acc: 0.9093\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2067 - val_acc: 0.9082\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9087 - val_loss: 0.2076 - val_acc: 0.9062\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9094 - val_loss: 0.2086 - val_acc: 0.9073\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9094 - val_loss: 0.2074 - val_acc: 0.9098\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9089 - val_loss: 0.2084 - val_acc: 0.9058\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9091 - val_loss: 0.2105 - val_acc: 0.9089\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9093 - val_loss: 0.2085 - val_acc: 0.9089\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9098 - val_loss: 0.2089 - val_acc: 0.9071\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9099 - val_loss: 0.2083 - val_acc: 0.9058\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9085 - val_loss: 0.2076 - val_acc: 0.9078\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9091 - val_loss: 0.2080 - val_acc: 0.9073\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2066 - val_acc: 0.9069\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9099 - val_loss: 0.2062 - val_acc: 0.9084\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9093 - val_loss: 0.2078 - val_acc: 0.9062\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9096 - val_loss: 0.2075 - val_acc: 0.9080\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9089 - val_loss: 0.2071 - val_acc: 0.9091\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9101 - val_loss: 0.2067 - val_acc: 0.9075\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9094 - val_loss: 0.2078 - val_acc: 0.9071\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9095 - val_loss: 0.2075 - val_acc: 0.9071\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9092 - val_loss: 0.2092 - val_acc: 0.9069\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9095 - val_loss: 0.2062 - val_acc: 0.9071\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9095 - val_loss: 0.2075 - val_acc: 0.9056\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9096 - val_loss: 0.2085 - val_acc: 0.9080\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9100 - val_loss: 0.2074 - val_acc: 0.9075\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9100 - val_loss: 0.2072 - val_acc: 0.9095\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9089 - val_loss: 0.2054 - val_acc: 0.9106\n",
            "acc: 91.06%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 151us/step - loss: 0.2520 - acc: 0.8880 - val_loss: 0.2243 - val_acc: 0.8969\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2181 - acc: 0.9026 - val_loss: 0.2165 - val_acc: 0.9058\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2134 - acc: 0.9045 - val_loss: 0.2134 - val_acc: 0.9022\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2114 - acc: 0.9046 - val_loss: 0.2108 - val_acc: 0.9067\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2098 - acc: 0.9059 - val_loss: 0.2111 - val_acc: 0.9038\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2089 - acc: 0.9053 - val_loss: 0.2098 - val_acc: 0.9071\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2074 - acc: 0.9083 - val_loss: 0.2096 - val_acc: 0.9051\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2067 - acc: 0.9071 - val_loss: 0.2059 - val_acc: 0.9042\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2058 - acc: 0.9075 - val_loss: 0.2078 - val_acc: 0.9080\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2054 - acc: 0.9082 - val_loss: 0.2076 - val_acc: 0.9100\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2048 - acc: 0.9076 - val_loss: 0.2086 - val_acc: 0.9064\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2039 - acc: 0.9095 - val_loss: 0.2059 - val_acc: 0.9089\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9089 - val_loss: 0.2069 - val_acc: 0.9062\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9090 - val_loss: 0.2060 - val_acc: 0.9095\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2041 - acc: 0.9093 - val_loss: 0.2088 - val_acc: 0.9071\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2038 - acc: 0.9084 - val_loss: 0.2031 - val_acc: 0.9086\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2033 - acc: 0.9097 - val_loss: 0.2062 - val_acc: 0.9078\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2030 - acc: 0.9095 - val_loss: 0.2085 - val_acc: 0.9082\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2024 - acc: 0.9101 - val_loss: 0.2046 - val_acc: 0.9086\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9098 - val_loss: 0.2070 - val_acc: 0.9080\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2025 - acc: 0.9096 - val_loss: 0.2057 - val_acc: 0.9047\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9100 - val_loss: 0.2078 - val_acc: 0.9060\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2025 - acc: 0.9091 - val_loss: 0.2072 - val_acc: 0.9080\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2020 - acc: 0.9102 - val_loss: 0.2045 - val_acc: 0.9071\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9098 - val_loss: 0.2089 - val_acc: 0.9067\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9091 - val_loss: 0.2062 - val_acc: 0.9075\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2020 - acc: 0.9094 - val_loss: 0.2063 - val_acc: 0.9075\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2022 - acc: 0.9093 - val_loss: 0.2053 - val_acc: 0.9053\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2019 - acc: 0.9092 - val_loss: 0.2063 - val_acc: 0.9047\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2019 - acc: 0.9100 - val_loss: 0.2066 - val_acc: 0.9053\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9104 - val_loss: 0.2069 - val_acc: 0.9058\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9092 - val_loss: 0.2090 - val_acc: 0.9060\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2016 - acc: 0.9090 - val_loss: 0.2069 - val_acc: 0.9049\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2016 - acc: 0.9097 - val_loss: 0.2112 - val_acc: 0.9060\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2012 - acc: 0.9095 - val_loss: 0.2047 - val_acc: 0.9067\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9095 - val_loss: 0.2078 - val_acc: 0.9058\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9095 - val_loss: 0.2061 - val_acc: 0.9082\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9097 - val_loss: 0.2057 - val_acc: 0.9053\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9103 - val_loss: 0.2062 - val_acc: 0.9089\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9094 - val_loss: 0.2060 - val_acc: 0.9069\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9093 - val_loss: 0.2058 - val_acc: 0.9078\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9099 - val_loss: 0.2058 - val_acc: 0.9080\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2013 - acc: 0.9094 - val_loss: 0.2045 - val_acc: 0.9104\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2009 - acc: 0.9094 - val_loss: 0.2101 - val_acc: 0.9044\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2011 - acc: 0.9098 - val_loss: 0.2058 - val_acc: 0.9075\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2009 - acc: 0.9098 - val_loss: 0.2069 - val_acc: 0.9086\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9094 - val_loss: 0.2056 - val_acc: 0.9069\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9094 - val_loss: 0.2087 - val_acc: 0.9049\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2010 - acc: 0.9102 - val_loss: 0.2064 - val_acc: 0.9047\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9103 - val_loss: 0.2061 - val_acc: 0.9060\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9094 - val_loss: 0.2068 - val_acc: 0.9047\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9094 - val_loss: 0.2096 - val_acc: 0.9047\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9093 - val_loss: 0.2069 - val_acc: 0.9067\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9096 - val_loss: 0.2057 - val_acc: 0.9071\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9101 - val_loss: 0.2064 - val_acc: 0.9060\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9099 - val_loss: 0.2069 - val_acc: 0.9071\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9097 - val_loss: 0.2076 - val_acc: 0.9049\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9095 - val_loss: 0.2056 - val_acc: 0.9071\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9104 - val_loss: 0.2058 - val_acc: 0.9058\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9098 - val_loss: 0.2075 - val_acc: 0.9062\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9099 - val_loss: 0.2048 - val_acc: 0.9082\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9094 - val_loss: 0.2049 - val_acc: 0.9093\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9093 - val_loss: 0.2065 - val_acc: 0.9075\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9093 - val_loss: 0.2063 - val_acc: 0.9071\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9097 - val_loss: 0.2078 - val_acc: 0.9062\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2008 - acc: 0.9100 - val_loss: 0.2047 - val_acc: 0.9062\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9100 - val_loss: 0.2059 - val_acc: 0.9073\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2002 - acc: 0.9092 - val_loss: 0.2054 - val_acc: 0.9080\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9098 - val_loss: 0.2052 - val_acc: 0.9095\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9108 - val_loss: 0.2085 - val_acc: 0.9075\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9095 - val_loss: 0.2094 - val_acc: 0.9056\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9099 - val_loss: 0.2056 - val_acc: 0.9069\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9101 - val_loss: 0.2052 - val_acc: 0.9075\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9102 - val_loss: 0.2059 - val_acc: 0.9069\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2001 - acc: 0.9097 - val_loss: 0.2065 - val_acc: 0.9080\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2006 - acc: 0.9094 - val_loss: 0.2072 - val_acc: 0.9064\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9104 - val_loss: 0.2086 - val_acc: 0.9086\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9099 - val_loss: 0.2062 - val_acc: 0.9056\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9100 - val_loss: 0.2067 - val_acc: 0.9078\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1998 - acc: 0.9103 - val_loss: 0.2065 - val_acc: 0.9067\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2004 - acc: 0.9102 - val_loss: 0.2052 - val_acc: 0.9082\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9106 - val_loss: 0.2089 - val_acc: 0.9060\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9105 - val_loss: 0.2060 - val_acc: 0.9060\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2003 - acc: 0.9100 - val_loss: 0.2054 - val_acc: 0.9064\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9100 - val_loss: 0.2055 - val_acc: 0.9071\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9100 - val_loss: 0.2077 - val_acc: 0.9071\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9100 - val_loss: 0.2102 - val_acc: 0.9075\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9100 - val_loss: 0.2059 - val_acc: 0.9075\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9105 - val_loss: 0.2049 - val_acc: 0.9071\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9102 - val_loss: 0.2077 - val_acc: 0.9067\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9105 - val_loss: 0.2055 - val_acc: 0.9093\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9099 - val_loss: 0.2038 - val_acc: 0.9086\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9095 - val_loss: 0.2055 - val_acc: 0.9075\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9104 - val_loss: 0.2040 - val_acc: 0.9093\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9101 - val_loss: 0.2055 - val_acc: 0.9060\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9108 - val_loss: 0.2039 - val_acc: 0.9078\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9104 - val_loss: 0.2044 - val_acc: 0.9056\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9103 - val_loss: 0.2063 - val_acc: 0.9067\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9099 - val_loss: 0.2057 - val_acc: 0.9060\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9105 - val_loss: 0.2082 - val_acc: 0.9064\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9094 - val_loss: 0.2066 - val_acc: 0.9051\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9097 - val_loss: 0.2116 - val_acc: 0.9060\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9103 - val_loss: 0.2042 - val_acc: 0.9067\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1996 - acc: 0.9106 - val_loss: 0.2057 - val_acc: 0.9064\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 29us/step - loss: 0.2001 - acc: 0.9100 - val_loss: 0.2058 - val_acc: 0.9060\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 29us/step - loss: 0.2002 - acc: 0.9104 - val_loss: 0.2092 - val_acc: 0.9040\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 28us/step - loss: 0.1996 - acc: 0.9104 - val_loss: 0.2065 - val_acc: 0.9056\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1996 - acc: 0.9105 - val_loss: 0.2076 - val_acc: 0.9064\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1997 - acc: 0.9106 - val_loss: 0.2120 - val_acc: 0.9040\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 32us/step - loss: 0.1997 - acc: 0.9103 - val_loss: 0.2079 - val_acc: 0.9073\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2000 - acc: 0.9102 - val_loss: 0.2067 - val_acc: 0.9056\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9104 - val_loss: 0.2137 - val_acc: 0.9049\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9104 - val_loss: 0.2085 - val_acc: 0.9069\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9108 - val_loss: 0.2076 - val_acc: 0.9049\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9104 - val_loss: 0.2063 - val_acc: 0.9042\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9109 - val_loss: 0.2064 - val_acc: 0.9060\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9102 - val_loss: 0.2053 - val_acc: 0.9069\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9101 - val_loss: 0.2069 - val_acc: 0.9062\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9108 - val_loss: 0.2062 - val_acc: 0.9078\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9105 - val_loss: 0.2054 - val_acc: 0.9064\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9110 - val_loss: 0.2065 - val_acc: 0.9056\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9093 - val_loss: 0.2073 - val_acc: 0.9067\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9109 - val_loss: 0.2059 - val_acc: 0.9051\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9101 - val_loss: 0.2057 - val_acc: 0.9084\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9108 - val_loss: 0.2062 - val_acc: 0.9071\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9103 - val_loss: 0.2073 - val_acc: 0.9064\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9109 - val_loss: 0.2087 - val_acc: 0.9069\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9108 - val_loss: 0.2060 - val_acc: 0.9058\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9101 - val_loss: 0.2058 - val_acc: 0.9078\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9106 - val_loss: 0.2054 - val_acc: 0.9053\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9103 - val_loss: 0.2057 - val_acc: 0.9053\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9101 - val_loss: 0.2049 - val_acc: 0.9056\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9100 - val_loss: 0.2056 - val_acc: 0.9051\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9111 - val_loss: 0.2078 - val_acc: 0.9058\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9103 - val_loss: 0.2077 - val_acc: 0.9053\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9103 - val_loss: 0.2040 - val_acc: 0.9073\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9106 - val_loss: 0.2054 - val_acc: 0.9071\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9099 - val_loss: 0.2108 - val_acc: 0.9060\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9106 - val_loss: 0.2043 - val_acc: 0.9051\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9100 - val_loss: 0.2047 - val_acc: 0.9078\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9103 - val_loss: 0.2081 - val_acc: 0.9051\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9104 - val_loss: 0.2051 - val_acc: 0.9056\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9098 - val_loss: 0.2082 - val_acc: 0.9056\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9109 - val_loss: 0.2043 - val_acc: 0.9067\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9104 - val_loss: 0.2057 - val_acc: 0.9067\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9105 - val_loss: 0.2053 - val_acc: 0.9056\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9107 - val_loss: 0.2076 - val_acc: 0.9049\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9098 - val_loss: 0.2052 - val_acc: 0.9078\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9099 - val_loss: 0.2061 - val_acc: 0.9093\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9103 - val_loss: 0.2037 - val_acc: 0.9064\n",
            "acc: 90.64%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 161us/step - loss: 0.2583 - acc: 0.8902 - val_loss: 0.2309 - val_acc: 0.8971\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2171 - acc: 0.9036 - val_loss: 0.2280 - val_acc: 0.8938\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2136 - acc: 0.9034 - val_loss: 0.2244 - val_acc: 0.8956\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2105 - acc: 0.9053 - val_loss: 0.2189 - val_acc: 0.8987\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2095 - acc: 0.9064 - val_loss: 0.2186 - val_acc: 0.8987\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2079 - acc: 0.9055 - val_loss: 0.2196 - val_acc: 0.8967\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2074 - acc: 0.9059 - val_loss: 0.2170 - val_acc: 0.8974\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2063 - acc: 0.9073 - val_loss: 0.2165 - val_acc: 0.8983\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2061 - acc: 0.9072 - val_loss: 0.2149 - val_acc: 0.8996\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2053 - acc: 0.9071 - val_loss: 0.2172 - val_acc: 0.8965\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2047 - acc: 0.9070 - val_loss: 0.2177 - val_acc: 0.8969\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2039 - acc: 0.9075 - val_loss: 0.2141 - val_acc: 0.8983\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2039 - acc: 0.9082 - val_loss: 0.2150 - val_acc: 0.8983\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2036 - acc: 0.9076 - val_loss: 0.2132 - val_acc: 0.8994\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9078 - val_loss: 0.2154 - val_acc: 0.8985\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2031 - acc: 0.9073 - val_loss: 0.2144 - val_acc: 0.8960\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2021 - acc: 0.9072 - val_loss: 0.2140 - val_acc: 0.8978\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9074 - val_loss: 0.2127 - val_acc: 0.8978\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9078 - val_loss: 0.2128 - val_acc: 0.8954\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2015 - acc: 0.9078 - val_loss: 0.2146 - val_acc: 0.8983\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2018 - acc: 0.9068 - val_loss: 0.2128 - val_acc: 0.8996\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2013 - acc: 0.9073 - val_loss: 0.2130 - val_acc: 0.8985\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2010 - acc: 0.9070 - val_loss: 0.2141 - val_acc: 0.8978\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9079 - val_loss: 0.2114 - val_acc: 0.8983\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9081 - val_loss: 0.2126 - val_acc: 0.8956\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9070 - val_loss: 0.2114 - val_acc: 0.8974\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9080 - val_loss: 0.2130 - val_acc: 0.8954\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9086 - val_loss: 0.2122 - val_acc: 0.8987\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9078 - val_loss: 0.2124 - val_acc: 0.8987\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9082 - val_loss: 0.2106 - val_acc: 0.8991\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9072 - val_loss: 0.2136 - val_acc: 0.8978\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.8952\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9079 - val_loss: 0.2130 - val_acc: 0.8987\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9080 - val_loss: 0.2135 - val_acc: 0.8989\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9076 - val_loss: 0.2130 - val_acc: 0.8965\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9079 - val_loss: 0.2123 - val_acc: 0.8980\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9084 - val_loss: 0.2121 - val_acc: 0.8954\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9070 - val_loss: 0.2140 - val_acc: 0.8991\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9079 - val_loss: 0.2125 - val_acc: 0.8985\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9079 - val_loss: 0.2134 - val_acc: 0.8958\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9077 - val_loss: 0.2112 - val_acc: 0.8987\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9073 - val_loss: 0.2137 - val_acc: 0.8952\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9084 - val_loss: 0.2129 - val_acc: 0.8971\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9081 - val_loss: 0.2135 - val_acc: 0.8978\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9081 - val_loss: 0.2142 - val_acc: 0.8960\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9087 - val_loss: 0.2123 - val_acc: 0.8958\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9075 - val_loss: 0.2143 - val_acc: 0.8963\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9085 - val_loss: 0.2132 - val_acc: 0.8989\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.2136 - val_acc: 0.8971\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9079 - val_loss: 0.2125 - val_acc: 0.8980\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9094 - val_loss: 0.2138 - val_acc: 0.8994\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9083 - val_loss: 0.2127 - val_acc: 0.8963\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9078 - val_loss: 0.2129 - val_acc: 0.9009\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9085 - val_loss: 0.2115 - val_acc: 0.8983\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9082 - val_loss: 0.2105 - val_acc: 0.8971\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9074 - val_loss: 0.2112 - val_acc: 0.8989\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9078 - val_loss: 0.2117 - val_acc: 0.8983\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9080 - val_loss: 0.2128 - val_acc: 0.8987\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9081 - val_loss: 0.2123 - val_acc: 0.8980\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2136 - val_acc: 0.8963\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9084 - val_loss: 0.2112 - val_acc: 0.8967\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9088 - val_loss: 0.2124 - val_acc: 0.8987\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9087 - val_loss: 0.2129 - val_acc: 0.8967\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.2150 - val_acc: 0.8989\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9083 - val_loss: 0.2124 - val_acc: 0.8974\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9090 - val_loss: 0.2124 - val_acc: 0.8965\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9089 - val_loss: 0.2139 - val_acc: 0.8967\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9095 - val_loss: 0.2122 - val_acc: 0.8994\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9083 - val_loss: 0.2108 - val_acc: 0.8971\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9087 - val_loss: 0.2125 - val_acc: 0.9000\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9080 - val_loss: 0.2126 - val_acc: 0.8960\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9081 - val_loss: 0.2113 - val_acc: 0.8978\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9079 - val_loss: 0.2113 - val_acc: 0.8987\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9083 - val_loss: 0.2116 - val_acc: 0.8963\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9088 - val_loss: 0.2105 - val_acc: 0.8998\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.2127 - val_acc: 0.8956\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9082 - val_loss: 0.2115 - val_acc: 0.8971\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9081 - val_loss: 0.2114 - val_acc: 0.8994\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9079 - val_loss: 0.2131 - val_acc: 0.8963\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9073 - val_loss: 0.2111 - val_acc: 0.8987\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9090 - val_loss: 0.2112 - val_acc: 0.8996\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9091 - val_loss: 0.2111 - val_acc: 0.8996\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9096 - val_loss: 0.2106 - val_acc: 0.9005\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9075 - val_loss: 0.2112 - val_acc: 0.8971\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9074 - val_loss: 0.2138 - val_acc: 0.9000\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9081 - val_loss: 0.2130 - val_acc: 0.8980\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9077 - val_loss: 0.2100 - val_acc: 0.8996\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9085 - val_loss: 0.2111 - val_acc: 0.8976\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9087 - val_loss: 0.2114 - val_acc: 0.8958\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9090 - val_loss: 0.2120 - val_acc: 0.8989\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9081 - val_loss: 0.2122 - val_acc: 0.8965\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9081 - val_loss: 0.2132 - val_acc: 0.8956\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.8991\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9089 - val_loss: 0.2103 - val_acc: 0.8991\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9087 - val_loss: 0.2103 - val_acc: 0.8983\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9088 - val_loss: 0.2122 - val_acc: 0.9002\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9090 - val_loss: 0.2118 - val_acc: 0.8969\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9080 - val_loss: 0.2119 - val_acc: 0.9013\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.2101 - val_acc: 0.8994\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.2111 - val_acc: 0.9002\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2126 - val_acc: 0.8980\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.2117 - val_acc: 0.8985\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9084 - val_loss: 0.2105 - val_acc: 0.8996\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9087 - val_loss: 0.2133 - val_acc: 0.8969\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9090 - val_loss: 0.2098 - val_acc: 0.9016\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9084 - val_loss: 0.2107 - val_acc: 0.8983\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9088 - val_loss: 0.2115 - val_acc: 0.8983\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9082 - val_loss: 0.2114 - val_acc: 0.8974\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.2108 - val_acc: 0.8976\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2119 - val_acc: 0.8965\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9083 - val_loss: 0.2097 - val_acc: 0.9020\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2120 - val_acc: 0.8978\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9082 - val_loss: 0.2116 - val_acc: 0.9007\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9080 - val_loss: 0.2111 - val_acc: 0.8989\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.2142 - val_acc: 0.8987\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1973 - acc: 0.9091 - val_loss: 0.2090 - val_acc: 0.9007\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9084 - val_loss: 0.2108 - val_acc: 0.8960\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9087 - val_loss: 0.2105 - val_acc: 0.8987\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.2113 - val_acc: 0.8996\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9084 - val_loss: 0.2107 - val_acc: 0.8987\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9088 - val_loss: 0.2114 - val_acc: 0.8978\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9085 - val_loss: 0.2113 - val_acc: 0.8991\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9092 - val_loss: 0.2115 - val_acc: 0.8987\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9092 - val_loss: 0.2104 - val_acc: 0.8996\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2115 - val_acc: 0.8996\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9082 - val_loss: 0.2157 - val_acc: 0.8985\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9089 - val_loss: 0.2088 - val_acc: 0.9002\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9091 - val_loss: 0.2100 - val_acc: 0.8983\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9089 - val_loss: 0.2097 - val_acc: 0.8971\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9091 - val_loss: 0.2121 - val_acc: 0.8971\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9088 - val_loss: 0.2131 - val_acc: 0.8985\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9078 - val_loss: 0.2125 - val_acc: 0.8987\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9083 - val_loss: 0.2097 - val_acc: 0.8978\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9088 - val_loss: 0.2129 - val_acc: 0.8974\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9088 - val_loss: 0.2111 - val_acc: 0.8987\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9090 - val_loss: 0.2115 - val_acc: 0.8971\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9080 - val_loss: 0.2108 - val_acc: 0.8980\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9094 - val_loss: 0.2098 - val_acc: 0.8976\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9078 - val_loss: 0.2097 - val_acc: 0.8998\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9097 - val_loss: 0.2097 - val_acc: 0.8998\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9081 - val_loss: 0.2114 - val_acc: 0.8969\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1966 - acc: 0.9084 - val_loss: 0.2143 - val_acc: 0.8983\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9094 - val_loss: 0.2125 - val_acc: 0.8985\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9085 - val_loss: 0.2134 - val_acc: 0.8998\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9084 - val_loss: 0.2106 - val_acc: 0.9002\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9092 - val_loss: 0.2112 - val_acc: 0.8998\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9082 - val_loss: 0.2143 - val_acc: 0.8989\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9085 - val_loss: 0.2116 - val_acc: 0.8994\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9086 - val_loss: 0.2106 - val_acc: 0.8994\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9086 - val_loss: 0.2103 - val_acc: 0.8983\n",
            "acc: 89.83%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 165us/step - loss: 0.2568 - acc: 0.8916 - val_loss: 0.2073 - val_acc: 0.9064\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2155 - acc: 0.9030 - val_loss: 0.2047 - val_acc: 0.9091\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2116 - acc: 0.9039 - val_loss: 0.2069 - val_acc: 0.9047\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2103 - acc: 0.9045 - val_loss: 0.2044 - val_acc: 0.9060\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2085 - acc: 0.9051 - val_loss: 0.2036 - val_acc: 0.9086\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2076 - acc: 0.9050 - val_loss: 0.2029 - val_acc: 0.9062\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2067 - acc: 0.9059 - val_loss: 0.2031 - val_acc: 0.9058\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2065 - acc: 0.9055 - val_loss: 0.2019 - val_acc: 0.9082\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2062 - acc: 0.9062 - val_loss: 0.2028 - val_acc: 0.9089\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2055 - acc: 0.9062 - val_loss: 0.2012 - val_acc: 0.9049\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2053 - acc: 0.9058 - val_loss: 0.2017 - val_acc: 0.9056\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2047 - acc: 0.9067 - val_loss: 0.2011 - val_acc: 0.9064\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2045 - acc: 0.9056 - val_loss: 0.1994 - val_acc: 0.9098\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2043 - acc: 0.9063 - val_loss: 0.1989 - val_acc: 0.9089\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2038 - acc: 0.9067 - val_loss: 0.2021 - val_acc: 0.9064\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9063 - val_loss: 0.2008 - val_acc: 0.9084\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2031 - acc: 0.9065 - val_loss: 0.1994 - val_acc: 0.9095\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9067 - val_loss: 0.1984 - val_acc: 0.9086\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2029 - acc: 0.9063 - val_loss: 0.1994 - val_acc: 0.9082\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2029 - acc: 0.9072 - val_loss: 0.1974 - val_acc: 0.9086\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9076 - val_loss: 0.1963 - val_acc: 0.9069\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2019 - acc: 0.9057 - val_loss: 0.1973 - val_acc: 0.9084\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2019 - acc: 0.9076 - val_loss: 0.1976 - val_acc: 0.9062\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2018 - acc: 0.9063 - val_loss: 0.1966 - val_acc: 0.9084\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2010 - acc: 0.9073 - val_loss: 0.1986 - val_acc: 0.9084\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2011 - acc: 0.9066 - val_loss: 0.1983 - val_acc: 0.9098\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2012 - acc: 0.9065 - val_loss: 0.1985 - val_acc: 0.9095\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2005 - acc: 0.9067 - val_loss: 0.1967 - val_acc: 0.9102\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9062 - val_loss: 0.1980 - val_acc: 0.9080\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9073 - val_loss: 0.1968 - val_acc: 0.9093\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9070 - val_loss: 0.1970 - val_acc: 0.9053\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9068 - val_loss: 0.1973 - val_acc: 0.9082\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9071 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2000 - acc: 0.9077 - val_loss: 0.1975 - val_acc: 0.9089\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9075 - val_loss: 0.1982 - val_acc: 0.9089\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9075 - val_loss: 0.1973 - val_acc: 0.9109\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9070 - val_loss: 0.1966 - val_acc: 0.9082\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9074 - val_loss: 0.1972 - val_acc: 0.9082\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9077 - val_loss: 0.1982 - val_acc: 0.9069\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9071 - val_loss: 0.1973 - val_acc: 0.9100\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9075 - val_loss: 0.1960 - val_acc: 0.9084\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9075 - val_loss: 0.1969 - val_acc: 0.9106\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9067 - val_loss: 0.1974 - val_acc: 0.9095\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9069 - val_loss: 0.1963 - val_acc: 0.9093\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9068 - val_loss: 0.1963 - val_acc: 0.9100\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9070 - val_loss: 0.1955 - val_acc: 0.9098\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9068 - val_loss: 0.1965 - val_acc: 0.9084\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9077 - val_loss: 0.1958 - val_acc: 0.9111\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9074 - val_loss: 0.1951 - val_acc: 0.9086\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9067 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9068 - val_loss: 0.1972 - val_acc: 0.9089\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9070 - val_loss: 0.1969 - val_acc: 0.9084\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9060 - val_loss: 0.1959 - val_acc: 0.9084\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9074 - val_loss: 0.1966 - val_acc: 0.9095\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9073 - val_loss: 0.1975 - val_acc: 0.9064\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9076 - val_loss: 0.1970 - val_acc: 0.9062\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9072 - val_loss: 0.1968 - val_acc: 0.9078\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9066 - val_loss: 0.1969 - val_acc: 0.9093\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9075 - val_loss: 0.1956 - val_acc: 0.9082\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9072 - val_loss: 0.1946 - val_acc: 0.9084\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9071 - val_loss: 0.1974 - val_acc: 0.9082\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9074 - val_loss: 0.1980 - val_acc: 0.9086\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9081 - val_loss: 0.1952 - val_acc: 0.9086\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9086 - val_loss: 0.1956 - val_acc: 0.9084\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9073 - val_loss: 0.1964 - val_acc: 0.9098\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9082 - val_loss: 0.1976 - val_acc: 0.9067\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9087 - val_loss: 0.1989 - val_acc: 0.9053\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9081 - val_loss: 0.1967 - val_acc: 0.9080\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9074 - val_loss: 0.1963 - val_acc: 0.9091\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9077 - val_loss: 0.1954 - val_acc: 0.9086\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1983 - acc: 0.9087 - val_loss: 0.1963 - val_acc: 0.9073\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1981 - acc: 0.9082 - val_loss: 0.1949 - val_acc: 0.9089\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1986 - acc: 0.9077 - val_loss: 0.1947 - val_acc: 0.9095\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1983 - acc: 0.9075 - val_loss: 0.1986 - val_acc: 0.9062\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1980 - acc: 0.9083 - val_loss: 0.1967 - val_acc: 0.9091\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1983 - acc: 0.9079 - val_loss: 0.1955 - val_acc: 0.9100\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.1975 - val_acc: 0.9082\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1979 - acc: 0.9077 - val_loss: 0.1955 - val_acc: 0.9080\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9075 - val_loss: 0.1959 - val_acc: 0.9089\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1981 - acc: 0.9082 - val_loss: 0.1963 - val_acc: 0.9078\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9077 - val_loss: 0.1974 - val_acc: 0.9073\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9078 - val_loss: 0.1962 - val_acc: 0.9073\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1980 - acc: 0.9076 - val_loss: 0.1983 - val_acc: 0.9084\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1982 - acc: 0.9077 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.1959 - val_acc: 0.9075\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9078 - val_loss: 0.1942 - val_acc: 0.9086\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9082 - val_loss: 0.1953 - val_acc: 0.9080\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9076 - val_loss: 0.1976 - val_acc: 0.9080\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.1973 - val_acc: 0.9064\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9080 - val_loss: 0.1971 - val_acc: 0.9078\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9076 - val_loss: 0.1972 - val_acc: 0.9091\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9078 - val_loss: 0.1948 - val_acc: 0.9082\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9084 - val_loss: 0.1972 - val_acc: 0.9058\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9086 - val_loss: 0.1957 - val_acc: 0.9082\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9081 - val_loss: 0.1970 - val_acc: 0.9086\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9081 - val_loss: 0.1981 - val_acc: 0.9080\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1979 - acc: 0.9079 - val_loss: 0.1951 - val_acc: 0.9073\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1978 - acc: 0.9081 - val_loss: 0.1975 - val_acc: 0.9078\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9077 - val_loss: 0.1963 - val_acc: 0.9093\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9077 - val_loss: 0.1970 - val_acc: 0.9071\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.1959 - val_acc: 0.9067\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1978 - acc: 0.9080 - val_loss: 0.1975 - val_acc: 0.9075\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9082 - val_loss: 0.1973 - val_acc: 0.9080\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.1959 - val_acc: 0.9075\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1964 - val_acc: 0.9069\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9080 - val_loss: 0.1965 - val_acc: 0.9073\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9084 - val_loss: 0.1996 - val_acc: 0.9080\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9084 - val_loss: 0.1956 - val_acc: 0.9071\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9081 - val_loss: 0.1956 - val_acc: 0.9091\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9087 - val_loss: 0.1979 - val_acc: 0.9086\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9080 - val_loss: 0.1963 - val_acc: 0.9064\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9080 - val_loss: 0.1971 - val_acc: 0.9082\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.1959 - val_acc: 0.9069\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1978 - acc: 0.9079 - val_loss: 0.1981 - val_acc: 0.9075\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9083 - val_loss: 0.1952 - val_acc: 0.9086\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9080 - val_loss: 0.1961 - val_acc: 0.9060\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1970 - acc: 0.9082 - val_loss: 0.2005 - val_acc: 0.9073\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9082 - val_loss: 0.1952 - val_acc: 0.9069\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1973 - acc: 0.9079 - val_loss: 0.1972 - val_acc: 0.9069\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.1968 - val_acc: 0.9073\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9077 - val_loss: 0.1976 - val_acc: 0.9084\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1976 - val_acc: 0.9064\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9053\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1972 - acc: 0.9083 - val_loss: 0.1962 - val_acc: 0.9067\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9083 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9085 - val_loss: 0.1970 - val_acc: 0.9080\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9086 - val_loss: 0.1967 - val_acc: 0.9069\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9089 - val_loss: 0.1968 - val_acc: 0.9073\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9084 - val_loss: 0.1967 - val_acc: 0.9064\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9091 - val_loss: 0.1964 - val_acc: 0.9062\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9081 - val_loss: 0.1984 - val_acc: 0.9069\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1973 - acc: 0.9080 - val_loss: 0.1949 - val_acc: 0.9071\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9086 - val_loss: 0.1975 - val_acc: 0.9075\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9087 - val_loss: 0.1964 - val_acc: 0.9064\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9079 - val_loss: 0.1966 - val_acc: 0.9053\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.1968 - val_acc: 0.9062\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9085 - val_loss: 0.1957 - val_acc: 0.9056\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9081 - val_loss: 0.1991 - val_acc: 0.9056\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9085 - val_loss: 0.1955 - val_acc: 0.9056\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9087 - val_loss: 0.1964 - val_acc: 0.9062\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.1947 - val_acc: 0.9080\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9085 - val_loss: 0.1956 - val_acc: 0.9084\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9095\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9075 - val_loss: 0.1972 - val_acc: 0.9049\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9080 - val_loss: 0.1960 - val_acc: 0.9069\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1957 - val_acc: 0.9067\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9090 - val_loss: 0.1969 - val_acc: 0.9044\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9078 - val_loss: 0.1951 - val_acc: 0.9060\n",
            "acc: 90.60%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 167us/step - loss: 0.2662 - acc: 0.8900 - val_loss: 0.2219 - val_acc: 0.8971\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2173 - acc: 0.9038 - val_loss: 0.2152 - val_acc: 0.8991\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2137 - acc: 0.9044 - val_loss: 0.2121 - val_acc: 0.8998\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2109 - acc: 0.9048 - val_loss: 0.2134 - val_acc: 0.8998\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2091 - acc: 0.9054 - val_loss: 0.2105 - val_acc: 0.8989\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2081 - acc: 0.9059 - val_loss: 0.2113 - val_acc: 0.9018\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2068 - acc: 0.9069 - val_loss: 0.2131 - val_acc: 0.9005\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2065 - acc: 0.9064 - val_loss: 0.2107 - val_acc: 0.8996\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2050 - acc: 0.9073 - val_loss: 0.2082 - val_acc: 0.9031\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2043 - acc: 0.9079 - val_loss: 0.2114 - val_acc: 0.9020\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2040 - acc: 0.9073 - val_loss: 0.2094 - val_acc: 0.9020\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2033 - acc: 0.9073 - val_loss: 0.2096 - val_acc: 0.9029\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2025 - acc: 0.9078 - val_loss: 0.2090 - val_acc: 0.9009\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2019 - acc: 0.9074 - val_loss: 0.2085 - val_acc: 0.9005\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2021 - acc: 0.9073 - val_loss: 0.2078 - val_acc: 0.9033\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2019 - acc: 0.9074 - val_loss: 0.2094 - val_acc: 0.9016\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9070 - val_loss: 0.2088 - val_acc: 0.8994\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9078 - val_loss: 0.2105 - val_acc: 0.9002\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2012 - acc: 0.9070 - val_loss: 0.2093 - val_acc: 0.9013\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2012 - acc: 0.9084 - val_loss: 0.2084 - val_acc: 0.9040\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.8989\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2008 - acc: 0.9079 - val_loss: 0.2106 - val_acc: 0.9013\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9082 - val_loss: 0.2070 - val_acc: 0.9002\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9088 - val_loss: 0.2080 - val_acc: 0.8994\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9086 - val_loss: 0.2096 - val_acc: 0.9013\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9091 - val_loss: 0.2085 - val_acc: 0.9007\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9085 - val_loss: 0.2089 - val_acc: 0.9002\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1997 - acc: 0.9084 - val_loss: 0.2124 - val_acc: 0.9029\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9096 - val_loss: 0.2073 - val_acc: 0.9009\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1994 - acc: 0.9087 - val_loss: 0.2071 - val_acc: 0.9029\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9092 - val_loss: 0.2078 - val_acc: 0.9051\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1991 - acc: 0.9077 - val_loss: 0.2116 - val_acc: 0.9022\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9090 - val_loss: 0.2105 - val_acc: 0.9009\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9095 - val_loss: 0.2096 - val_acc: 0.8996\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9094 - val_loss: 0.2092 - val_acc: 0.9025\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9086 - val_loss: 0.2078 - val_acc: 0.9033\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9091 - val_loss: 0.2092 - val_acc: 0.9011\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9087 - val_loss: 0.2064 - val_acc: 0.9007\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9086 - val_loss: 0.2077 - val_acc: 0.8989\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9085 - val_loss: 0.2075 - val_acc: 0.9018\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9090 - val_loss: 0.2084 - val_acc: 0.9016\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9075 - val_loss: 0.2065 - val_acc: 0.9020\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9092 - val_loss: 0.2075 - val_acc: 0.9011\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9088 - val_loss: 0.2099 - val_acc: 0.8983\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9086 - val_loss: 0.2079 - val_acc: 0.9002\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9089 - val_loss: 0.2098 - val_acc: 0.9016\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9087 - val_loss: 0.2075 - val_acc: 0.8998\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9088 - val_loss: 0.2083 - val_acc: 0.9027\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9087 - val_loss: 0.2093 - val_acc: 0.9029\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9094 - val_loss: 0.2070 - val_acc: 0.9027\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9087 - val_loss: 0.2086 - val_acc: 0.9002\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9092 - val_loss: 0.2104 - val_acc: 0.9011\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9101 - val_loss: 0.2087 - val_acc: 0.9033\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9094 - val_loss: 0.2058 - val_acc: 0.9013\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9106 - val_loss: 0.2073 - val_acc: 0.9007\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1965 - acc: 0.9094 - val_loss: 0.2144 - val_acc: 0.8991\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9103 - val_loss: 0.2085 - val_acc: 0.8994\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9098 - val_loss: 0.2064 - val_acc: 0.8998\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9092 - val_loss: 0.2068 - val_acc: 0.8998\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9101 - val_loss: 0.2062 - val_acc: 0.8989\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9108 - val_loss: 0.2066 - val_acc: 0.9027\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1962 - acc: 0.9104 - val_loss: 0.2049 - val_acc: 0.9025\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9102 - val_loss: 0.2063 - val_acc: 0.8987\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9096 - val_loss: 0.2081 - val_acc: 0.9002\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.2051 - val_acc: 0.9016\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1963 - acc: 0.9106 - val_loss: 0.2067 - val_acc: 0.8987\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1962 - acc: 0.9091 - val_loss: 0.2055 - val_acc: 0.8996\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1961 - acc: 0.9107 - val_loss: 0.2075 - val_acc: 0.9027\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1958 - acc: 0.9100 - val_loss: 0.2067 - val_acc: 0.8980\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1960 - acc: 0.9098 - val_loss: 0.2053 - val_acc: 0.9022\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9103 - val_loss: 0.2067 - val_acc: 0.9013\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1956 - acc: 0.9100 - val_loss: 0.2084 - val_acc: 0.8969\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1958 - acc: 0.9099 - val_loss: 0.2063 - val_acc: 0.9002\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9102 - val_loss: 0.2082 - val_acc: 0.9007\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9111 - val_loss: 0.2063 - val_acc: 0.9025\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1957 - acc: 0.9094 - val_loss: 0.2095 - val_acc: 0.8991\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1957 - acc: 0.9090 - val_loss: 0.2060 - val_acc: 0.9018\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1957 - acc: 0.9097 - val_loss: 0.2051 - val_acc: 0.9013\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9108 - val_loss: 0.2052 - val_acc: 0.9002\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1952 - acc: 0.9105 - val_loss: 0.2054 - val_acc: 0.9040\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1954 - acc: 0.9098 - val_loss: 0.2059 - val_acc: 0.8989\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1953 - acc: 0.9101 - val_loss: 0.2072 - val_acc: 0.9005\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1953 - acc: 0.9098 - val_loss: 0.2071 - val_acc: 0.8998\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1954 - acc: 0.9089 - val_loss: 0.2058 - val_acc: 0.9016\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9098 - val_loss: 0.2061 - val_acc: 0.9000\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1950 - acc: 0.9094 - val_loss: 0.2076 - val_acc: 0.8987\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9107 - val_loss: 0.2047 - val_acc: 0.9016\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1951 - acc: 0.9096 - val_loss: 0.2049 - val_acc: 0.9005\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1954 - acc: 0.9106 - val_loss: 0.2046 - val_acc: 0.9040\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1952 - acc: 0.9092 - val_loss: 0.2051 - val_acc: 0.9056\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1951 - acc: 0.9097 - val_loss: 0.2055 - val_acc: 0.9044\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9097 - val_loss: 0.2055 - val_acc: 0.9002\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9102 - val_loss: 0.2059 - val_acc: 0.9033\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1951 - acc: 0.9090 - val_loss: 0.2066 - val_acc: 0.9040\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1953 - acc: 0.9096 - val_loss: 0.2091 - val_acc: 0.9009\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9099 - val_loss: 0.2073 - val_acc: 0.9009\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1952 - acc: 0.9105 - val_loss: 0.2074 - val_acc: 0.8994\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9105 - val_loss: 0.2059 - val_acc: 0.9005\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1944 - acc: 0.9102 - val_loss: 0.2083 - val_acc: 0.9033\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9098 - val_loss: 0.2057 - val_acc: 0.8991\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9106 - val_loss: 0.2056 - val_acc: 0.8980\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9103 - val_loss: 0.2081 - val_acc: 0.8985\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1947 - acc: 0.9111 - val_loss: 0.2048 - val_acc: 0.9018\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9107 - val_loss: 0.2056 - val_acc: 0.9025\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9102 - val_loss: 0.2067 - val_acc: 0.9002\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9097 - val_loss: 0.2069 - val_acc: 0.8989\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9097 - val_loss: 0.2062 - val_acc: 0.9027\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9105 - val_loss: 0.2114 - val_acc: 0.8940\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9105 - val_loss: 0.2054 - val_acc: 0.9040\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1949 - acc: 0.9096 - val_loss: 0.2079 - val_acc: 0.9002\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1944 - acc: 0.9098 - val_loss: 0.2064 - val_acc: 0.9000\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9097 - val_loss: 0.2055 - val_acc: 0.9016\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1949 - acc: 0.9094 - val_loss: 0.2067 - val_acc: 0.8998\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1942 - acc: 0.9096 - val_loss: 0.2060 - val_acc: 0.8994\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9092 - val_loss: 0.2083 - val_acc: 0.8987\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9097 - val_loss: 0.2132 - val_acc: 0.8971\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9100 - val_loss: 0.2054 - val_acc: 0.9025\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9112 - val_loss: 0.2073 - val_acc: 0.9011\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9101 - val_loss: 0.2069 - val_acc: 0.8994\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1948 - acc: 0.9100 - val_loss: 0.2073 - val_acc: 0.8980\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9093 - val_loss: 0.2066 - val_acc: 0.9007\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9101 - val_loss: 0.2071 - val_acc: 0.9002\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9093 - val_loss: 0.2054 - val_acc: 0.9007\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9108 - val_loss: 0.2045 - val_acc: 0.9029\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9103 - val_loss: 0.2063 - val_acc: 0.9016\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1942 - acc: 0.9106 - val_loss: 0.2075 - val_acc: 0.8991\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9104 - val_loss: 0.2063 - val_acc: 0.9044\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9096 - val_loss: 0.2065 - val_acc: 0.9020\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1951 - acc: 0.9098 - val_loss: 0.2050 - val_acc: 0.9027\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9103 - val_loss: 0.2061 - val_acc: 0.8978\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9103 - val_loss: 0.2047 - val_acc: 0.9025\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9090 - val_loss: 0.2046 - val_acc: 0.9025\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9094 - val_loss: 0.2078 - val_acc: 0.8989\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9102 - val_loss: 0.2069 - val_acc: 0.9000\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1942 - acc: 0.9096 - val_loss: 0.2071 - val_acc: 0.8991\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9107 - val_loss: 0.2070 - val_acc: 0.9018\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9097 - val_loss: 0.2074 - val_acc: 0.9036\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9115 - val_loss: 0.2057 - val_acc: 0.9002\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9099 - val_loss: 0.2058 - val_acc: 0.9018\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9105 - val_loss: 0.2067 - val_acc: 0.9009\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9109 - val_loss: 0.2068 - val_acc: 0.8998\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1946 - acc: 0.9097 - val_loss: 0.2056 - val_acc: 0.8991\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9101 - val_loss: 0.2081 - val_acc: 0.9009\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9111 - val_loss: 0.2081 - val_acc: 0.9002\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9092 - val_loss: 0.2067 - val_acc: 0.9022\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9110 - val_loss: 0.2066 - val_acc: 0.9025\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9102 - val_loss: 0.2067 - val_acc: 0.8989\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9104 - val_loss: 0.2068 - val_acc: 0.9025\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1944 - acc: 0.9099 - val_loss: 0.2068 - val_acc: 0.8989\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1946 - acc: 0.9090 - val_loss: 0.2080 - val_acc: 0.8971\n",
            "acc: 89.71%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 166us/step - loss: 0.2636 - acc: 0.8872 - val_loss: 0.2256 - val_acc: 0.9018\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2171 - acc: 0.9014 - val_loss: 0.2206 - val_acc: 0.9020\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2126 - acc: 0.9031 - val_loss: 0.2162 - val_acc: 0.9007\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2101 - acc: 0.9033 - val_loss: 0.2146 - val_acc: 0.9044\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2079 - acc: 0.9053 - val_loss: 0.2112 - val_acc: 0.9049\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2070 - acc: 0.9065 - val_loss: 0.2146 - val_acc: 0.9044\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2056 - acc: 0.9057 - val_loss: 0.2164 - val_acc: 0.9020\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2053 - acc: 0.9060 - val_loss: 0.2144 - val_acc: 0.9042\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2051 - acc: 0.9061 - val_loss: 0.2109 - val_acc: 0.9058\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2046 - acc: 0.9070 - val_loss: 0.2120 - val_acc: 0.9047\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2050 - acc: 0.9064 - val_loss: 0.2103 - val_acc: 0.9056\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2045 - acc: 0.9069 - val_loss: 0.2112 - val_acc: 0.9044\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2042 - acc: 0.9068 - val_loss: 0.2118 - val_acc: 0.9033\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2039 - acc: 0.9070 - val_loss: 0.2130 - val_acc: 0.9042\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2039 - acc: 0.9078 - val_loss: 0.2102 - val_acc: 0.9027\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2034 - acc: 0.9078 - val_loss: 0.2136 - val_acc: 0.9047\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2035 - acc: 0.9079 - val_loss: 0.2110 - val_acc: 0.9069\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2032 - acc: 0.9070 - val_loss: 0.2115 - val_acc: 0.9049\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2028 - acc: 0.9082 - val_loss: 0.2120 - val_acc: 0.9071\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2031 - acc: 0.9077 - val_loss: 0.2099 - val_acc: 0.9047\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2025 - acc: 0.9078 - val_loss: 0.2111 - val_acc: 0.9022\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2026 - acc: 0.9077 - val_loss: 0.2091 - val_acc: 0.9060\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2021 - acc: 0.9081 - val_loss: 0.2125 - val_acc: 0.9044\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2023 - acc: 0.9068 - val_loss: 0.2115 - val_acc: 0.9056\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2020 - acc: 0.9074 - val_loss: 0.2098 - val_acc: 0.9044\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2024 - acc: 0.9078 - val_loss: 0.2108 - val_acc: 0.9029\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9081 - val_loss: 0.2116 - val_acc: 0.9007\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2017 - acc: 0.9067 - val_loss: 0.2109 - val_acc: 0.9042\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9078 - val_loss: 0.2104 - val_acc: 0.9020\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2017 - acc: 0.9072 - val_loss: 0.2101 - val_acc: 0.9053\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2016 - acc: 0.9087 - val_loss: 0.2117 - val_acc: 0.9060\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2017 - acc: 0.9079 - val_loss: 0.2126 - val_acc: 0.9049\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2016 - acc: 0.9078 - val_loss: 0.2095 - val_acc: 0.9051\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9079 - val_loss: 0.2099 - val_acc: 0.9049\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2014 - acc: 0.9079 - val_loss: 0.2121 - val_acc: 0.9033\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2014 - acc: 0.9084 - val_loss: 0.2098 - val_acc: 0.9056\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2014 - acc: 0.9075 - val_loss: 0.2104 - val_acc: 0.9056\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9083 - val_loss: 0.2102 - val_acc: 0.9062\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9083 - val_loss: 0.2085 - val_acc: 0.9062\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2012 - acc: 0.9082 - val_loss: 0.2096 - val_acc: 0.9038\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2008 - acc: 0.9084 - val_loss: 0.2116 - val_acc: 0.9038\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9078 - val_loss: 0.2100 - val_acc: 0.9049\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2011 - acc: 0.9080 - val_loss: 0.2099 - val_acc: 0.9060\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9076 - val_loss: 0.2097 - val_acc: 0.9040\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9080 - val_loss: 0.2081 - val_acc: 0.9069\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9082 - val_loss: 0.2094 - val_acc: 0.9053\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9079 - val_loss: 0.2098 - val_acc: 0.9042\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2008 - acc: 0.9088 - val_loss: 0.2114 - val_acc: 0.9044\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2005 - acc: 0.9085 - val_loss: 0.2114 - val_acc: 0.9060\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2009 - acc: 0.9083 - val_loss: 0.2098 - val_acc: 0.9051\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9080 - val_loss: 0.2085 - val_acc: 0.9025\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9083 - val_loss: 0.2121 - val_acc: 0.9009\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9094 - val_loss: 0.2100 - val_acc: 0.9071\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9085 - val_loss: 0.2110 - val_acc: 0.9022\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2006 - acc: 0.9085 - val_loss: 0.2095 - val_acc: 0.9018\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9077 - val_loss: 0.2112 - val_acc: 0.9042\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2005 - acc: 0.9091 - val_loss: 0.2106 - val_acc: 0.9016\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9082 - val_loss: 0.2101 - val_acc: 0.9060\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9088 - val_loss: 0.2102 - val_acc: 0.9047\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9085 - val_loss: 0.2089 - val_acc: 0.9047\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2004 - acc: 0.9088 - val_loss: 0.2083 - val_acc: 0.9047\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9087 - val_loss: 0.2095 - val_acc: 0.9038\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9081 - val_loss: 0.2102 - val_acc: 0.9031\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2002 - acc: 0.9084 - val_loss: 0.2097 - val_acc: 0.9047\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2003 - acc: 0.9087 - val_loss: 0.2114 - val_acc: 0.9049\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2000 - acc: 0.9091 - val_loss: 0.2102 - val_acc: 0.9042\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.2000 - acc: 0.9085 - val_loss: 0.2115 - val_acc: 0.9027\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2006 - acc: 0.9081 - val_loss: 0.2106 - val_acc: 0.9044\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2001 - acc: 0.9093 - val_loss: 0.2119 - val_acc: 0.9027\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1999 - acc: 0.9093 - val_loss: 0.2092 - val_acc: 0.9053\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1999 - acc: 0.9091 - val_loss: 0.2106 - val_acc: 0.9049\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1997 - acc: 0.9089 - val_loss: 0.2113 - val_acc: 0.9040\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1996 - acc: 0.9098 - val_loss: 0.2092 - val_acc: 0.9022\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9091 - val_loss: 0.2108 - val_acc: 0.9027\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9085 - val_loss: 0.2098 - val_acc: 0.9038\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9087 - val_loss: 0.2105 - val_acc: 0.9058\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9090 - val_loss: 0.2098 - val_acc: 0.9051\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9094 - val_loss: 0.2097 - val_acc: 0.9073\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9099 - val_loss: 0.2127 - val_acc: 0.9025\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1998 - acc: 0.9087 - val_loss: 0.2101 - val_acc: 0.9029\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9089 - val_loss: 0.2136 - val_acc: 0.9047\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9085 - val_loss: 0.2124 - val_acc: 0.9056\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9092 - val_loss: 0.2115 - val_acc: 0.9049\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9092 - val_loss: 0.2122 - val_acc: 0.9060\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2110 - val_acc: 0.9051\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9094 - val_loss: 0.2109 - val_acc: 0.8998\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9097 - val_loss: 0.2104 - val_acc: 0.9056\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9097 - val_loss: 0.2090 - val_acc: 0.9051\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9096 - val_loss: 0.2089 - val_acc: 0.9047\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9092 - val_loss: 0.2105 - val_acc: 0.9067\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9090 - val_loss: 0.2124 - val_acc: 0.9013\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9094 - val_loss: 0.2124 - val_acc: 0.9002\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1990 - acc: 0.9101 - val_loss: 0.2128 - val_acc: 0.9038\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9093 - val_loss: 0.2141 - val_acc: 0.9031\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9093 - val_loss: 0.2094 - val_acc: 0.9056\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9091 - val_loss: 0.2108 - val_acc: 0.9022\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9094 - val_loss: 0.2090 - val_acc: 0.9042\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9092 - val_loss: 0.2099 - val_acc: 0.9031\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9094 - val_loss: 0.2105 - val_acc: 0.9036\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2107 - val_acc: 0.9025\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9096 - val_loss: 0.2112 - val_acc: 0.9029\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9090 - val_loss: 0.2109 - val_acc: 0.9020\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9096 - val_loss: 0.2104 - val_acc: 0.9025\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9089 - val_loss: 0.2097 - val_acc: 0.9025\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9087 - val_loss: 0.2104 - val_acc: 0.9056\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2108 - val_acc: 0.9058\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9099 - val_loss: 0.2117 - val_acc: 0.9033\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9091 - val_loss: 0.2118 - val_acc: 0.9027\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9096 - val_loss: 0.2152 - val_acc: 0.9067\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9085 - val_loss: 0.2102 - val_acc: 0.9038\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9090 - val_loss: 0.2116 - val_acc: 0.9018\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9100 - val_loss: 0.2107 - val_acc: 0.9047\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2112 - val_acc: 0.9080\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2142 - val_acc: 0.9011\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9102 - val_loss: 0.2122 - val_acc: 0.9005\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9102 - val_loss: 0.2114 - val_acc: 0.9027\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9101 - val_loss: 0.2158 - val_acc: 0.9000\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2128 - val_acc: 0.9033\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9094 - val_loss: 0.2094 - val_acc: 0.9038\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9087 - val_loss: 0.2112 - val_acc: 0.9044\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9097 - val_loss: 0.2108 - val_acc: 0.9040\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9095 - val_loss: 0.2099 - val_acc: 0.9040\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9099 - val_loss: 0.2121 - val_acc: 0.9064\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9094 - val_loss: 0.2121 - val_acc: 0.9051\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9094 - val_loss: 0.2112 - val_acc: 0.9067\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9098 - val_loss: 0.2093 - val_acc: 0.9029\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9100 - val_loss: 0.2109 - val_acc: 0.9044\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9097 - val_loss: 0.2097 - val_acc: 0.9051\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2110 - val_acc: 0.9053\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9091 - val_loss: 0.2114 - val_acc: 0.9058\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9098 - val_loss: 0.2108 - val_acc: 0.9049\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9094 - val_loss: 0.2122 - val_acc: 0.9036\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9096 - val_loss: 0.2110 - val_acc: 0.9058\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9105 - val_loss: 0.2119 - val_acc: 0.9036\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9108 - val_loss: 0.2100 - val_acc: 0.9049\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9097 - val_loss: 0.2112 - val_acc: 0.9036\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9092 - val_loss: 0.2155 - val_acc: 0.9002\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9098 - val_loss: 0.2100 - val_acc: 0.9042\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9096 - val_loss: 0.2114 - val_acc: 0.9058\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9101 - val_loss: 0.2122 - val_acc: 0.9042\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9100 - val_loss: 0.2133 - val_acc: 0.9022\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9093 - val_loss: 0.2163 - val_acc: 0.9062\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9099 - val_loss: 0.2145 - val_acc: 0.9051\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9105 - val_loss: 0.2121 - val_acc: 0.9029\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9101 - val_loss: 0.2100 - val_acc: 0.9036\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9099 - val_loss: 0.2106 - val_acc: 0.9056\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9099 - val_loss: 0.2099 - val_acc: 0.9022\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9098 - val_loss: 0.2106 - val_acc: 0.9073\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9105 - val_loss: 0.2137 - val_acc: 0.9062\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9099 - val_loss: 0.2117 - val_acc: 0.9064\n",
            "acc: 90.64%\n",
            "90.55% (+/- 0.46%)\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   264    264        yes\n",
            "   163    3829        no\n",
            "Accuracy: 0.9054876025745947\n",
            "Sensitivity: 0.5004726791453961\n",
            "Specificity: 0.9591453334001303\n",
            "Precision: 0.6187470780738663\n",
            "f_score: 0.5533605100867566\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lqxC5G2TI9X",
        "colab_type": "code",
        "outputId": "f8eaa770-2ee4-4376-f5e4-2d5b35181596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "percentage_split_NN_PCA(0.25,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 33908 samples, validate on 11303 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "33908/33908 [==============================] - 1s 23us/step - loss: 0.3686 - acc: 0.8585 - val_loss: 0.3165 - val_acc: 0.8859\n",
            "Epoch 2/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3146 - acc: 0.8872 - val_loss: 0.3148 - val_acc: 0.8841\n",
            "Epoch 3/150\n",
            "33908/33908 [==============================] - 0s 14us/step - loss: 0.3115 - acc: 0.8865 - val_loss: 0.3121 - val_acc: 0.8846\n",
            "Epoch 4/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3098 - acc: 0.8867 - val_loss: 0.3116 - val_acc: 0.8860\n",
            "Epoch 5/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3091 - acc: 0.8858 - val_loss: 0.3111 - val_acc: 0.8827\n",
            "Epoch 6/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3089 - acc: 0.8860 - val_loss: 0.3115 - val_acc: 0.8826\n",
            "Epoch 7/150\n",
            "33908/33908 [==============================] - 0s 15us/step - loss: 0.3083 - acc: 0.8870 - val_loss: 0.3090 - val_acc: 0.8860\n",
            "Epoch 8/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3084 - acc: 0.8863 - val_loss: 0.3089 - val_acc: 0.8856\n",
            "Epoch 9/150\n",
            "33908/33908 [==============================] - 0s 15us/step - loss: 0.3080 - acc: 0.8870 - val_loss: 0.3081 - val_acc: 0.8868\n",
            "Epoch 10/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3075 - acc: 0.8868 - val_loss: 0.3085 - val_acc: 0.8854\n",
            "Epoch 11/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3076 - acc: 0.8871 - val_loss: 0.3090 - val_acc: 0.8848\n",
            "Epoch 12/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8867 - val_loss: 0.3078 - val_acc: 0.8842\n",
            "Epoch 13/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3071 - acc: 0.8868 - val_loss: 0.3069 - val_acc: 0.8853\n",
            "Epoch 14/150\n",
            "33908/33908 [==============================] - 0s 14us/step - loss: 0.3069 - acc: 0.8863 - val_loss: 0.3105 - val_acc: 0.8821\n",
            "Epoch 15/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8866 - val_loss: 0.3090 - val_acc: 0.8845\n",
            "Epoch 16/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8869 - val_loss: 0.3071 - val_acc: 0.8846\n",
            "Epoch 17/150\n",
            "33908/33908 [==============================] - 0s 14us/step - loss: 0.3062 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8860\n",
            "Epoch 18/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8873 - val_loss: 0.3069 - val_acc: 0.8856\n",
            "Epoch 19/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8883 - val_loss: 0.3071 - val_acc: 0.8850\n",
            "Epoch 20/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8865 - val_loss: 0.3075 - val_acc: 0.8838\n",
            "Epoch 21/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8872 - val_loss: 0.3080 - val_acc: 0.8848\n",
            "Epoch 22/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8875 - val_loss: 0.3077 - val_acc: 0.8854\n",
            "Epoch 23/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3062 - acc: 0.8873 - val_loss: 0.3063 - val_acc: 0.8847\n",
            "Epoch 24/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8872 - val_loss: 0.3069 - val_acc: 0.8847\n",
            "Epoch 25/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8870 - val_loss: 0.3074 - val_acc: 0.8839\n",
            "Epoch 26/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8873 - val_loss: 0.3062 - val_acc: 0.8859\n",
            "Epoch 27/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3062 - acc: 0.8873 - val_loss: 0.3070 - val_acc: 0.8837\n",
            "Epoch 28/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8875 - val_loss: 0.3076 - val_acc: 0.8835\n",
            "Epoch 29/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8881 - val_loss: 0.3094 - val_acc: 0.8840\n",
            "Epoch 30/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8870 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 31/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8876 - val_loss: 0.3072 - val_acc: 0.8847\n",
            "Epoch 32/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8876 - val_loss: 0.3083 - val_acc: 0.8830\n",
            "Epoch 33/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8857\n",
            "Epoch 34/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8883 - val_loss: 0.3066 - val_acc: 0.8859\n",
            "Epoch 35/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8874 - val_loss: 0.3070 - val_acc: 0.8866\n",
            "Epoch 36/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8873 - val_loss: 0.3088 - val_acc: 0.8845\n",
            "Epoch 37/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8875 - val_loss: 0.3066 - val_acc: 0.8855\n",
            "Epoch 38/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3060 - acc: 0.8870 - val_loss: 0.3075 - val_acc: 0.8844\n",
            "Epoch 39/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3057 - acc: 0.8878 - val_loss: 0.3073 - val_acc: 0.8852\n",
            "Epoch 40/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3060 - acc: 0.8863 - val_loss: 0.3063 - val_acc: 0.8848\n",
            "Epoch 41/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8868 - val_loss: 0.3066 - val_acc: 0.8857\n",
            "Epoch 42/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8877 - val_loss: 0.3069 - val_acc: 0.8843\n",
            "Epoch 43/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8870 - val_loss: 0.3062 - val_acc: 0.8853\n",
            "Epoch 44/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8869 - val_loss: 0.3067 - val_acc: 0.8853\n",
            "Epoch 45/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8873 - val_loss: 0.3066 - val_acc: 0.8851\n",
            "Epoch 46/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3057 - acc: 0.8876 - val_loss: 0.3067 - val_acc: 0.8839\n",
            "Epoch 47/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8876 - val_loss: 0.3084 - val_acc: 0.8833\n",
            "Epoch 48/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8870 - val_loss: 0.3065 - val_acc: 0.8840\n",
            "Epoch 49/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8866 - val_loss: 0.3090 - val_acc: 0.8853\n",
            "Epoch 50/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3060 - acc: 0.8872 - val_loss: 0.3059 - val_acc: 0.8844\n",
            "Epoch 51/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8874 - val_loss: 0.3068 - val_acc: 0.8845\n",
            "Epoch 52/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8872 - val_loss: 0.3071 - val_acc: 0.8841\n",
            "Epoch 53/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8878 - val_loss: 0.3072 - val_acc: 0.8830\n",
            "Epoch 54/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8868 - val_loss: 0.3069 - val_acc: 0.8849\n",
            "Epoch 55/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3058 - acc: 0.8865 - val_loss: 0.3072 - val_acc: 0.8839\n",
            "Epoch 56/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8875 - val_loss: 0.3062 - val_acc: 0.8856\n",
            "Epoch 57/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8871 - val_loss: 0.3065 - val_acc: 0.8848\n",
            "Epoch 58/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8862 - val_loss: 0.3073 - val_acc: 0.8854\n",
            "Epoch 59/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8868 - val_loss: 0.3071 - val_acc: 0.8846\n",
            "Epoch 60/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8869 - val_loss: 0.3070 - val_acc: 0.8852\n",
            "Epoch 61/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8863 - val_loss: 0.3064 - val_acc: 0.8850\n",
            "Epoch 62/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8872 - val_loss: 0.3071 - val_acc: 0.8842\n",
            "Epoch 63/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8840\n",
            "Epoch 64/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3054 - acc: 0.8875 - val_loss: 0.3078 - val_acc: 0.8850\n",
            "Epoch 65/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3051 - acc: 0.8875 - val_loss: 0.3083 - val_acc: 0.8816\n",
            "Epoch 66/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3073 - val_acc: 0.8834\n",
            "Epoch 67/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8875 - val_loss: 0.3069 - val_acc: 0.8850\n",
            "Epoch 68/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8874 - val_loss: 0.3075 - val_acc: 0.8846\n",
            "Epoch 69/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8871 - val_loss: 0.3069 - val_acc: 0.8837\n",
            "Epoch 70/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8869 - val_loss: 0.3072 - val_acc: 0.8855\n",
            "Epoch 71/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8875 - val_loss: 0.3087 - val_acc: 0.8829\n",
            "Epoch 72/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8870 - val_loss: 0.3066 - val_acc: 0.8856\n",
            "Epoch 73/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3075 - val_acc: 0.8860\n",
            "Epoch 74/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8875 - val_loss: 0.3072 - val_acc: 0.8845\n",
            "Epoch 75/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8874 - val_loss: 0.3071 - val_acc: 0.8852\n",
            "Epoch 76/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8875 - val_loss: 0.3071 - val_acc: 0.8828\n",
            "Epoch 77/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8871 - val_loss: 0.3073 - val_acc: 0.8855\n",
            "Epoch 78/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3071 - val_acc: 0.8849\n",
            "Epoch 79/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8872 - val_loss: 0.3062 - val_acc: 0.8837\n",
            "Epoch 80/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3054 - acc: 0.8869 - val_loss: 0.3067 - val_acc: 0.8860\n",
            "Epoch 81/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8873 - val_loss: 0.3068 - val_acc: 0.8868\n",
            "Epoch 82/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8866 - val_loss: 0.3072 - val_acc: 0.8844\n",
            "Epoch 83/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3051 - acc: 0.8873 - val_loss: 0.3066 - val_acc: 0.8859\n",
            "Epoch 84/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8866 - val_loss: 0.3060 - val_acc: 0.8865\n",
            "Epoch 85/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3068 - val_acc: 0.8850\n",
            "Epoch 86/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8873 - val_loss: 0.3062 - val_acc: 0.8860\n",
            "Epoch 87/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8878 - val_loss: 0.3065 - val_acc: 0.8839\n",
            "Epoch 88/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8847\n",
            "Epoch 89/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8874 - val_loss: 0.3061 - val_acc: 0.8844\n",
            "Epoch 90/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8866 - val_loss: 0.3062 - val_acc: 0.8869\n",
            "Epoch 91/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3057 - val_acc: 0.8858\n",
            "Epoch 92/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3060 - val_acc: 0.8861\n",
            "Epoch 93/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3065 - val_acc: 0.8867\n",
            "Epoch 94/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8870 - val_loss: 0.3060 - val_acc: 0.8868\n",
            "Epoch 95/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3065 - val_acc: 0.8864\n",
            "Epoch 96/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3066 - val_acc: 0.8869\n",
            "Epoch 97/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8870 - val_loss: 0.3063 - val_acc: 0.8853\n",
            "Epoch 98/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3047 - acc: 0.8868 - val_loss: 0.3065 - val_acc: 0.8856\n",
            "Epoch 99/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3062 - val_acc: 0.8844\n",
            "Epoch 100/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8868 - val_loss: 0.3069 - val_acc: 0.8861\n",
            "Epoch 101/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3074 - val_acc: 0.8862\n",
            "Epoch 102/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3056 - val_acc: 0.8865\n",
            "Epoch 103/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8870 - val_loss: 0.3074 - val_acc: 0.8836\n",
            "Epoch 104/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8868 - val_loss: 0.3065 - val_acc: 0.8847\n",
            "Epoch 105/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8871 - val_loss: 0.3063 - val_acc: 0.8877\n",
            "Epoch 106/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3057 - val_acc: 0.8876\n",
            "Epoch 107/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8866 - val_loss: 0.3071 - val_acc: 0.8852\n",
            "Epoch 108/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3050 - acc: 0.8868 - val_loss: 0.3065 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8868 - val_loss: 0.3060 - val_acc: 0.8860\n",
            "Epoch 110/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8870 - val_loss: 0.3070 - val_acc: 0.8842\n",
            "Epoch 111/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8868 - val_loss: 0.3076 - val_acc: 0.8839\n",
            "Epoch 112/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8880 - val_loss: 0.3083 - val_acc: 0.8843\n",
            "Epoch 113/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3057 - val_acc: 0.8866\n",
            "Epoch 114/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3072 - val_acc: 0.8840\n",
            "Epoch 115/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3046 - acc: 0.8867 - val_loss: 0.3058 - val_acc: 0.8870\n",
            "Epoch 116/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8867 - val_loss: 0.3059 - val_acc: 0.8860\n",
            "Epoch 117/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3077 - val_acc: 0.8819\n",
            "Epoch 118/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8867 - val_loss: 0.3060 - val_acc: 0.8861\n",
            "Epoch 119/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8868 - val_loss: 0.3061 - val_acc: 0.8855\n",
            "Epoch 120/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8871 - val_loss: 0.3059 - val_acc: 0.8860\n",
            "Epoch 121/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8864 - val_loss: 0.3068 - val_acc: 0.8874\n",
            "Epoch 122/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3064 - val_acc: 0.8861\n",
            "Epoch 123/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8866 - val_loss: 0.3070 - val_acc: 0.8860\n",
            "Epoch 124/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3061 - val_acc: 0.8859\n",
            "Epoch 125/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8871 - val_loss: 0.3067 - val_acc: 0.8856\n",
            "Epoch 126/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3046 - acc: 0.8875 - val_loss: 0.3069 - val_acc: 0.8859\n",
            "Epoch 127/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3067 - val_acc: 0.8872\n",
            "Epoch 128/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3072 - val_acc: 0.8868\n",
            "Epoch 129/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3061 - val_acc: 0.8862\n",
            "Epoch 130/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8870 - val_loss: 0.3058 - val_acc: 0.8852\n",
            "Epoch 131/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8865 - val_loss: 0.3061 - val_acc: 0.8859\n",
            "Epoch 132/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3074 - val_acc: 0.8847\n",
            "Epoch 133/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3066 - val_acc: 0.8848\n",
            "Epoch 134/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3045 - acc: 0.8868 - val_loss: 0.3070 - val_acc: 0.8852\n",
            "Epoch 135/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8873 - val_loss: 0.3065 - val_acc: 0.8859\n",
            "Epoch 136/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3064 - val_acc: 0.8841\n",
            "Epoch 137/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3057 - val_acc: 0.8868\n",
            "Epoch 138/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8876 - val_loss: 0.3065 - val_acc: 0.8841\n",
            "Epoch 139/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3070 - val_acc: 0.8853\n",
            "Epoch 140/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3046 - acc: 0.8869 - val_loss: 0.3064 - val_acc: 0.8843\n",
            "Epoch 141/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8869 - val_loss: 0.3089 - val_acc: 0.8836\n",
            "Epoch 142/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3077 - val_acc: 0.8861\n",
            "Epoch 143/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8867 - val_loss: 0.3066 - val_acc: 0.8870\n",
            "Epoch 144/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3043 - acc: 0.8875 - val_loss: 0.3072 - val_acc: 0.8839\n",
            "Epoch 145/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8870 - val_loss: 0.3065 - val_acc: 0.8865\n",
            "Epoch 146/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3069 - val_acc: 0.8833\n",
            "Epoch 147/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8870 - val_loss: 0.3078 - val_acc: 0.8841\n",
            "Epoch 148/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3066 - val_acc: 0.8845\n",
            "Epoch 149/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8869 - val_loss: 0.3073 - val_acc: 0.8846\n",
            "Epoch 150/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3063 - val_acc: 0.8845\n",
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   234    1088        yes\n",
            "   218    9763        no\n",
            "Accuracy: 0.884455454304167\n",
            "Sensitivity: 0.17700453857791226\n",
            "Specificity: 0.9781585011521892\n",
            "Precision: 0.5176991150442478\n",
            "f_score: 0.26381059751972946\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmjXXMxuTRR4",
        "colab_type": "code",
        "outputId": "8d331d7e-8cc7-4baf-ebf9-a1246d5a7f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "crossvalidate_NN_PCA(10,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40689 samples, validate on 4522 samples\n",
            "Epoch 1/150\n",
            "40689/40689 [==============================] - 1s 19us/step - loss: 0.3348 - acc: 0.8761 - val_loss: 0.3214 - val_acc: 0.8824\n",
            "Epoch 2/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3127 - acc: 0.8854 - val_loss: 0.3186 - val_acc: 0.8837\n",
            "Epoch 3/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3114 - acc: 0.8845 - val_loss: 0.3173 - val_acc: 0.8839\n",
            "Epoch 4/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3112 - acc: 0.8846 - val_loss: 0.3178 - val_acc: 0.8819\n",
            "Epoch 5/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3109 - acc: 0.8847 - val_loss: 0.3164 - val_acc: 0.8830\n",
            "Epoch 6/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3103 - acc: 0.8850 - val_loss: 0.3149 - val_acc: 0.8861\n",
            "Epoch 7/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3097 - acc: 0.8858 - val_loss: 0.3141 - val_acc: 0.8841\n",
            "Epoch 8/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3094 - acc: 0.8857 - val_loss: 0.3147 - val_acc: 0.8852\n",
            "Epoch 9/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3094 - acc: 0.8860 - val_loss: 0.3151 - val_acc: 0.8859\n",
            "Epoch 10/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3091 - acc: 0.8863 - val_loss: 0.3135 - val_acc: 0.8832\n",
            "Epoch 11/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3091 - acc: 0.8865 - val_loss: 0.3126 - val_acc: 0.8863\n",
            "Epoch 12/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8854 - val_loss: 0.3118 - val_acc: 0.8863\n",
            "Epoch 13/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8867 - val_loss: 0.3133 - val_acc: 0.8874\n",
            "Epoch 14/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8860 - val_loss: 0.3119 - val_acc: 0.8863\n",
            "Epoch 15/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8867 - val_loss: 0.3125 - val_acc: 0.8859\n",
            "Epoch 16/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8872 - val_loss: 0.3127 - val_acc: 0.8857\n",
            "Epoch 17/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8865 - val_loss: 0.3115 - val_acc: 0.8857\n",
            "Epoch 18/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8869 - val_loss: 0.3128 - val_acc: 0.8852\n",
            "Epoch 19/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8869 - val_loss: 0.3127 - val_acc: 0.8854\n",
            "Epoch 20/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3074 - acc: 0.8865 - val_loss: 0.3113 - val_acc: 0.8870\n",
            "Epoch 21/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8861\n",
            "Epoch 22/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3070 - acc: 0.8861 - val_loss: 0.3120 - val_acc: 0.8843\n",
            "Epoch 23/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8863 - val_loss: 0.3103 - val_acc: 0.8879\n",
            "Epoch 24/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8863 - val_loss: 0.3113 - val_acc: 0.8857\n",
            "Epoch 25/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8865 - val_loss: 0.3099 - val_acc: 0.8859\n",
            "Epoch 26/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8863 - val_loss: 0.3149 - val_acc: 0.8824\n",
            "Epoch 27/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8865 - val_loss: 0.3116 - val_acc: 0.8863\n",
            "Epoch 28/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8861 - val_loss: 0.3106 - val_acc: 0.8859\n",
            "Epoch 29/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3057 - acc: 0.8865 - val_loss: 0.3104 - val_acc: 0.8866\n",
            "Epoch 30/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8866 - val_loss: 0.3098 - val_acc: 0.8837\n",
            "Epoch 31/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8863 - val_loss: 0.3103 - val_acc: 0.8854\n",
            "Epoch 32/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8863 - val_loss: 0.3102 - val_acc: 0.8850\n",
            "Epoch 33/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8868 - val_loss: 0.3096 - val_acc: 0.8846\n",
            "Epoch 34/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8863 - val_loss: 0.3098 - val_acc: 0.8832\n",
            "Epoch 35/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8861 - val_loss: 0.3106 - val_acc: 0.8854\n",
            "Epoch 36/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8870 - val_loss: 0.3096 - val_acc: 0.8852\n",
            "Epoch 37/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8855 - val_loss: 0.3096 - val_acc: 0.8863\n",
            "Epoch 38/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8861 - val_loss: 0.3116 - val_acc: 0.8837\n",
            "Epoch 39/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8857 - val_loss: 0.3106 - val_acc: 0.8846\n",
            "Epoch 40/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3048 - acc: 0.8866 - val_loss: 0.3092 - val_acc: 0.8861\n",
            "Epoch 41/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3045 - acc: 0.8865 - val_loss: 0.3107 - val_acc: 0.8877\n",
            "Epoch 42/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8859 - val_loss: 0.3105 - val_acc: 0.8848\n",
            "Epoch 43/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8867 - val_loss: 0.3101 - val_acc: 0.8852\n",
            "Epoch 44/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8865 - val_loss: 0.3098 - val_acc: 0.8874\n",
            "Epoch 45/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8869 - val_loss: 0.3101 - val_acc: 0.8846\n",
            "Epoch 46/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8861 - val_loss: 0.3090 - val_acc: 0.8854\n",
            "Epoch 47/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8861 - val_loss: 0.3089 - val_acc: 0.8854\n",
            "Epoch 48/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8861 - val_loss: 0.3095 - val_acc: 0.8848\n",
            "Epoch 49/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8857 - val_loss: 0.3102 - val_acc: 0.8846\n",
            "Epoch 50/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8860 - val_loss: 0.3097 - val_acc: 0.8846\n",
            "Epoch 51/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8862 - val_loss: 0.3095 - val_acc: 0.8854\n",
            "Epoch 52/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8857 - val_loss: 0.3103 - val_acc: 0.8866\n",
            "Epoch 53/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8863 - val_loss: 0.3102 - val_acc: 0.8857\n",
            "Epoch 54/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8859 - val_loss: 0.3101 - val_acc: 0.8854\n",
            "Epoch 55/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8867 - val_loss: 0.3094 - val_acc: 0.8854\n",
            "Epoch 56/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8862 - val_loss: 0.3089 - val_acc: 0.8866\n",
            "Epoch 57/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3044 - acc: 0.8862 - val_loss: 0.3091 - val_acc: 0.8863\n",
            "Epoch 58/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3042 - acc: 0.8864 - val_loss: 0.3089 - val_acc: 0.8857\n",
            "Epoch 59/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3043 - acc: 0.8868 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 60/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8859 - val_loss: 0.3099 - val_acc: 0.8846\n",
            "Epoch 61/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3043 - acc: 0.8862 - val_loss: 0.3097 - val_acc: 0.8854\n",
            "Epoch 62/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3040 - acc: 0.8868 - val_loss: 0.3099 - val_acc: 0.8870\n",
            "Epoch 63/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8864 - val_loss: 0.3102 - val_acc: 0.8868\n",
            "Epoch 64/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3045 - acc: 0.8864 - val_loss: 0.3111 - val_acc: 0.8843\n",
            "Epoch 65/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8854 - val_loss: 0.3103 - val_acc: 0.8863\n",
            "Epoch 66/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3044 - acc: 0.8857 - val_loss: 0.3097 - val_acc: 0.8852\n",
            "Epoch 67/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8858 - val_loss: 0.3083 - val_acc: 0.8848\n",
            "Epoch 68/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8863 - val_loss: 0.3090 - val_acc: 0.8837\n",
            "Epoch 69/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8861 - val_loss: 0.3098 - val_acc: 0.8859\n",
            "Epoch 70/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8861 - val_loss: 0.3109 - val_acc: 0.8830\n",
            "Epoch 71/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8854 - val_loss: 0.3090 - val_acc: 0.8846\n",
            "Epoch 72/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8861 - val_loss: 0.3087 - val_acc: 0.8868\n",
            "Epoch 73/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3045 - acc: 0.8858 - val_loss: 0.3091 - val_acc: 0.8874\n",
            "Epoch 74/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8858 - val_loss: 0.3099 - val_acc: 0.8850\n",
            "Epoch 75/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8860 - val_loss: 0.3098 - val_acc: 0.8848\n",
            "Epoch 76/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3041 - acc: 0.8862 - val_loss: 0.3098 - val_acc: 0.8859\n",
            "Epoch 77/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8860 - val_loss: 0.3083 - val_acc: 0.8868\n",
            "Epoch 78/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3124 - val_acc: 0.8870\n",
            "Epoch 79/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8861 - val_loss: 0.3096 - val_acc: 0.8872\n",
            "Epoch 80/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8859 - val_loss: 0.3092 - val_acc: 0.8859\n",
            "Epoch 81/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8868 - val_loss: 0.3093 - val_acc: 0.8859\n",
            "Epoch 82/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3042 - acc: 0.8861 - val_loss: 0.3098 - val_acc: 0.8857\n",
            "Epoch 83/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8864 - val_loss: 0.3105 - val_acc: 0.8859\n",
            "Epoch 84/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8861 - val_loss: 0.3104 - val_acc: 0.8839\n",
            "Epoch 85/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8862 - val_loss: 0.3095 - val_acc: 0.8861\n",
            "Epoch 86/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8866 - val_loss: 0.3089 - val_acc: 0.8866\n",
            "Epoch 87/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3041 - acc: 0.8858 - val_loss: 0.3086 - val_acc: 0.8874\n",
            "Epoch 88/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3042 - acc: 0.8858 - val_loss: 0.3093 - val_acc: 0.8881\n",
            "Epoch 89/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8865 - val_loss: 0.3102 - val_acc: 0.8846\n",
            "Epoch 90/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8869 - val_loss: 0.3099 - val_acc: 0.8863\n",
            "Epoch 91/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8864 - val_loss: 0.3087 - val_acc: 0.8872\n",
            "Epoch 92/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8869 - val_loss: 0.3104 - val_acc: 0.8859\n",
            "Epoch 93/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8865 - val_loss: 0.3115 - val_acc: 0.8852\n",
            "Epoch 94/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3041 - acc: 0.8865 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 95/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8862 - val_loss: 0.3086 - val_acc: 0.8877\n",
            "Epoch 96/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8866 - val_loss: 0.3100 - val_acc: 0.8868\n",
            "Epoch 97/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8868 - val_loss: 0.3093 - val_acc: 0.8879\n",
            "Epoch 98/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3040 - acc: 0.8864 - val_loss: 0.3111 - val_acc: 0.8857\n",
            "Epoch 99/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8857 - val_loss: 0.3124 - val_acc: 0.8859\n",
            "Epoch 100/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8859 - val_loss: 0.3101 - val_acc: 0.8848\n",
            "Epoch 101/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8859 - val_loss: 0.3107 - val_acc: 0.8881\n",
            "Epoch 102/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8865 - val_loss: 0.3099 - val_acc: 0.8883\n",
            "Epoch 103/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3039 - acc: 0.8858 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 104/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8862 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 105/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8865 - val_loss: 0.3102 - val_acc: 0.8857\n",
            "Epoch 106/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8863 - val_loss: 0.3107 - val_acc: 0.8870\n",
            "Epoch 107/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8859 - val_loss: 0.3109 - val_acc: 0.8857\n",
            "Epoch 108/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8860 - val_loss: 0.3103 - val_acc: 0.8854\n",
            "Epoch 109/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8862 - val_loss: 0.3100 - val_acc: 0.8879\n",
            "Epoch 110/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3109 - val_acc: 0.8852\n",
            "Epoch 111/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8863 - val_loss: 0.3111 - val_acc: 0.8861\n",
            "Epoch 112/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8861 - val_loss: 0.3101 - val_acc: 0.8861\n",
            "Epoch 113/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8864 - val_loss: 0.3105 - val_acc: 0.8877\n",
            "Epoch 114/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8860 - val_loss: 0.3097 - val_acc: 0.8872\n",
            "Epoch 115/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8863 - val_loss: 0.3116 - val_acc: 0.8879\n",
            "Epoch 116/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8868 - val_loss: 0.3118 - val_acc: 0.8868\n",
            "Epoch 117/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8864 - val_loss: 0.3111 - val_acc: 0.8881\n",
            "Epoch 118/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8865 - val_loss: 0.3113 - val_acc: 0.8879\n",
            "Epoch 119/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8862 - val_loss: 0.3101 - val_acc: 0.8877\n",
            "Epoch 120/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8861 - val_loss: 0.3126 - val_acc: 0.8866\n",
            "Epoch 121/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8862 - val_loss: 0.3101 - val_acc: 0.8881\n",
            "Epoch 122/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8865 - val_loss: 0.3110 - val_acc: 0.8850\n",
            "Epoch 123/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8854 - val_loss: 0.3105 - val_acc: 0.8866\n",
            "Epoch 124/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8860 - val_loss: 0.3103 - val_acc: 0.8863\n",
            "Epoch 125/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8866 - val_loss: 0.3096 - val_acc: 0.8866\n",
            "Epoch 126/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3036 - acc: 0.8868 - val_loss: 0.3104 - val_acc: 0.8852\n",
            "Epoch 127/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3038 - acc: 0.8859 - val_loss: 0.3100 - val_acc: 0.8861\n",
            "Epoch 128/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8863 - val_loss: 0.3103 - val_acc: 0.8868\n",
            "Epoch 129/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3096 - val_acc: 0.8877\n",
            "Epoch 130/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8862 - val_loss: 0.3118 - val_acc: 0.8857\n",
            "Epoch 131/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3099 - val_acc: 0.8877\n",
            "Epoch 132/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8863 - val_loss: 0.3106 - val_acc: 0.8870\n",
            "Epoch 133/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8859 - val_loss: 0.3116 - val_acc: 0.8861\n",
            "Epoch 134/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8861 - val_loss: 0.3104 - val_acc: 0.8859\n",
            "Epoch 135/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8866 - val_loss: 0.3114 - val_acc: 0.8870\n",
            "Epoch 136/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8862 - val_loss: 0.3099 - val_acc: 0.8872\n",
            "Epoch 137/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8863 - val_loss: 0.3101 - val_acc: 0.8863\n",
            "Epoch 138/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8869 - val_loss: 0.3103 - val_acc: 0.8877\n",
            "Epoch 139/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8865 - val_loss: 0.3100 - val_acc: 0.8861\n",
            "Epoch 140/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8865 - val_loss: 0.3118 - val_acc: 0.8872\n",
            "Epoch 141/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8862 - val_loss: 0.3093 - val_acc: 0.8861\n",
            "Epoch 142/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8869 - val_loss: 0.3106 - val_acc: 0.8872\n",
            "Epoch 143/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3032 - acc: 0.8865 - val_loss: 0.3109 - val_acc: 0.8868\n",
            "Epoch 144/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8863 - val_loss: 0.3115 - val_acc: 0.8843\n",
            "Epoch 145/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8863 - val_loss: 0.3115 - val_acc: 0.8857\n",
            "Epoch 146/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8866 - val_loss: 0.3103 - val_acc: 0.8879\n",
            "Epoch 147/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8870 - val_loss: 0.3108 - val_acc: 0.8868\n",
            "Epoch 148/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8863 - val_loss: 0.3105 - val_acc: 0.8863\n",
            "Epoch 149/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8859 - val_loss: 0.3107 - val_acc: 0.8877\n",
            "Epoch 150/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3032 - acc: 0.8862 - val_loss: 0.3121 - val_acc: 0.8863\n",
            "acc: 88.63%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.3453 - acc: 0.8679 - val_loss: 0.3142 - val_acc: 0.8879\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3118 - acc: 0.8870 - val_loss: 0.3128 - val_acc: 0.8879\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3099 - acc: 0.8868 - val_loss: 0.3133 - val_acc: 0.8872\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3091 - acc: 0.8873 - val_loss: 0.3125 - val_acc: 0.8881\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3088 - acc: 0.8870 - val_loss: 0.3152 - val_acc: 0.8808\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3086 - acc: 0.8873 - val_loss: 0.3116 - val_acc: 0.8885\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3083 - acc: 0.8867 - val_loss: 0.3106 - val_acc: 0.8876\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8862 - val_loss: 0.3111 - val_acc: 0.8881\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3078 - acc: 0.8876 - val_loss: 0.3110 - val_acc: 0.8856\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3077 - acc: 0.8869 - val_loss: 0.3121 - val_acc: 0.8874\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3072 - acc: 0.8867 - val_loss: 0.3111 - val_acc: 0.8868\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3066 - acc: 0.8871 - val_loss: 0.3110 - val_acc: 0.8856\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3067 - acc: 0.8871 - val_loss: 0.3099 - val_acc: 0.8863\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.3100 - val_acc: 0.8854\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8871 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3064 - acc: 0.8875 - val_loss: 0.3091 - val_acc: 0.8887\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3063 - acc: 0.8872 - val_loss: 0.3104 - val_acc: 0.8872\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3105 - val_acc: 0.8830\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8877 - val_loss: 0.3098 - val_acc: 0.8850\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8872 - val_loss: 0.3134 - val_acc: 0.8830\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8876 - val_loss: 0.3076 - val_acc: 0.8859\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8877 - val_loss: 0.3113 - val_acc: 0.8872\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8874 - val_loss: 0.3079 - val_acc: 0.8868\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3050 - acc: 0.8880 - val_loss: 0.3099 - val_acc: 0.8870\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8878 - val_loss: 0.3101 - val_acc: 0.8861\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3105 - val_acc: 0.8883\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8877 - val_loss: 0.3102 - val_acc: 0.8865\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8879 - val_loss: 0.3085 - val_acc: 0.8845\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8873 - val_loss: 0.3097 - val_acc: 0.8821\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3090 - val_acc: 0.8852\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3100 - val_acc: 0.8872\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8873 - val_loss: 0.3086 - val_acc: 0.8863\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8870 - val_loss: 0.3082 - val_acc: 0.8861\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8874 - val_loss: 0.3087 - val_acc: 0.8881\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8873 - val_loss: 0.3088 - val_acc: 0.8872\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3040 - acc: 0.8876 - val_loss: 0.3108 - val_acc: 0.8883\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3039 - acc: 0.8884 - val_loss: 0.3078 - val_acc: 0.8872\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8879 - val_loss: 0.3094 - val_acc: 0.8854\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8878 - val_loss: 0.3102 - val_acc: 0.8872\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8883 - val_loss: 0.3084 - val_acc: 0.8859\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8877 - val_loss: 0.3072 - val_acc: 0.8879\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8877 - val_loss: 0.3082 - val_acc: 0.8868\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8882 - val_loss: 0.3111 - val_acc: 0.8870\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8881 - val_loss: 0.3081 - val_acc: 0.8865\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8880 - val_loss: 0.3100 - val_acc: 0.8839\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8881 - val_loss: 0.3087 - val_acc: 0.8861\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8873 - val_loss: 0.3089 - val_acc: 0.8839\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8876 - val_loss: 0.3073 - val_acc: 0.8859\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8883 - val_loss: 0.3093 - val_acc: 0.8834\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8881 - val_loss: 0.3083 - val_acc: 0.8848\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8877 - val_loss: 0.3096 - val_acc: 0.8854\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8878 - val_loss: 0.3096 - val_acc: 0.8870\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8880 - val_loss: 0.3084 - val_acc: 0.8850\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3101 - val_acc: 0.8856\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3035 - acc: 0.8878 - val_loss: 0.3092 - val_acc: 0.8848\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8874 - val_loss: 0.3081 - val_acc: 0.8861\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8875 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8878 - val_loss: 0.3094 - val_acc: 0.8861\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8880 - val_loss: 0.3093 - val_acc: 0.8863\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8874 - val_loss: 0.3093 - val_acc: 0.8863\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8875 - val_loss: 0.3078 - val_acc: 0.8852\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8879 - val_loss: 0.3129 - val_acc: 0.8841\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8878 - val_loss: 0.3096 - val_acc: 0.8861\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8880 - val_loss: 0.3083 - val_acc: 0.8868\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8879 - val_loss: 0.3093 - val_acc: 0.8854\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8876 - val_loss: 0.3090 - val_acc: 0.8854\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8883 - val_loss: 0.3089 - val_acc: 0.8848\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8881 - val_loss: 0.3090 - val_acc: 0.8874\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8882 - val_loss: 0.3078 - val_acc: 0.8845\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3031 - acc: 0.8880 - val_loss: 0.3086 - val_acc: 0.8848\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8874 - val_loss: 0.3101 - val_acc: 0.8854\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8879 - val_loss: 0.3076 - val_acc: 0.8850\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8879 - val_loss: 0.3079 - val_acc: 0.8852\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8881 - val_loss: 0.3080 - val_acc: 0.8856\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8882 - val_loss: 0.3082 - val_acc: 0.8845\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8877 - val_loss: 0.3078 - val_acc: 0.8854\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3033 - acc: 0.8882 - val_loss: 0.3074 - val_acc: 0.8861\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8876 - val_loss: 0.3082 - val_acc: 0.8861\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8874 - val_loss: 0.3082 - val_acc: 0.8868\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8876 - val_loss: 0.3079 - val_acc: 0.8868\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8883 - val_loss: 0.3094 - val_acc: 0.8845\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8875 - val_loss: 0.3082 - val_acc: 0.8872\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3076 - val_acc: 0.8852\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3089 - val_acc: 0.8870\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8884 - val_loss: 0.3103 - val_acc: 0.8859\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3034 - acc: 0.8881 - val_loss: 0.3076 - val_acc: 0.8870\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8873 - val_loss: 0.3088 - val_acc: 0.8852\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3084 - val_acc: 0.8850\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8880 - val_loss: 0.3084 - val_acc: 0.8865\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8877 - val_loss: 0.3087 - val_acc: 0.8861\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8878 - val_loss: 0.3095 - val_acc: 0.8865\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8880 - val_loss: 0.3082 - val_acc: 0.8854\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8883 - val_loss: 0.3068 - val_acc: 0.8865\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3032 - acc: 0.8876 - val_loss: 0.3090 - val_acc: 0.8865\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8873 - val_loss: 0.3077 - val_acc: 0.8856\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3075 - val_acc: 0.8856\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3077 - val_acc: 0.8863\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3100 - val_acc: 0.8876\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8879 - val_loss: 0.3091 - val_acc: 0.8848\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3075 - val_acc: 0.8856\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3084 - val_acc: 0.8863\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3030 - acc: 0.8880 - val_loss: 0.3072 - val_acc: 0.8854\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8883 - val_loss: 0.3115 - val_acc: 0.8852\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8877 - val_loss: 0.3075 - val_acc: 0.8863\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8880 - val_loss: 0.3074 - val_acc: 0.8845\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3078 - val_acc: 0.8859\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3074 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8877 - val_loss: 0.3100 - val_acc: 0.8852\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8885 - val_loss: 0.3099 - val_acc: 0.8841\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3098 - val_acc: 0.8872\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3097 - val_acc: 0.8865\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8885 - val_loss: 0.3076 - val_acc: 0.8848\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8876 - val_loss: 0.3086 - val_acc: 0.8852\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8876 - val_loss: 0.3077 - val_acc: 0.8876\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8881 - val_loss: 0.3083 - val_acc: 0.8854\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3081 - val_acc: 0.8879\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8884 - val_loss: 0.3080 - val_acc: 0.8856\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8875 - val_loss: 0.3076 - val_acc: 0.8843\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3085 - val_acc: 0.8852\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8878 - val_loss: 0.3084 - val_acc: 0.8856\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3026 - acc: 0.8877 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8879 - val_loss: 0.3068 - val_acc: 0.8856\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8877 - val_loss: 0.3089 - val_acc: 0.8848\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8877 - val_loss: 0.3114 - val_acc: 0.8865\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8881 - val_loss: 0.3076 - val_acc: 0.8850\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8879 - val_loss: 0.3078 - val_acc: 0.8841\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3030 - acc: 0.8881 - val_loss: 0.3084 - val_acc: 0.8845\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3078 - val_acc: 0.8856\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8882 - val_loss: 0.3096 - val_acc: 0.8870\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8879 - val_loss: 0.3086 - val_acc: 0.8856\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3026 - acc: 0.8884 - val_loss: 0.3094 - val_acc: 0.8854\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3103 - val_acc: 0.8852\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8876 - val_loss: 0.3078 - val_acc: 0.8856\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8884 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3028 - acc: 0.8886 - val_loss: 0.3081 - val_acc: 0.8848\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3030 - acc: 0.8878 - val_loss: 0.3077 - val_acc: 0.8854\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3025 - acc: 0.8880 - val_loss: 0.3100 - val_acc: 0.8850\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3078 - val_acc: 0.8859\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3076 - val_acc: 0.8856\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8877 - val_loss: 0.3080 - val_acc: 0.8854\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8878 - val_loss: 0.3097 - val_acc: 0.8868\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3080 - val_acc: 0.8868\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3115 - val_acc: 0.8852\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8884 - val_loss: 0.3075 - val_acc: 0.8850\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3083 - val_acc: 0.8848\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8883 - val_loss: 0.3080 - val_acc: 0.8852\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3088 - val_acc: 0.8870\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3072 - val_acc: 0.8863\n",
            "acc: 88.63%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.3452 - acc: 0.8732 - val_loss: 0.3138 - val_acc: 0.8894\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3155 - acc: 0.8851 - val_loss: 0.3109 - val_acc: 0.8881\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3136 - acc: 0.8851 - val_loss: 0.3099 - val_acc: 0.8887\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3131 - acc: 0.8849 - val_loss: 0.3108 - val_acc: 0.8887\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3122 - acc: 0.8852 - val_loss: 0.3093 - val_acc: 0.8850\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3122 - acc: 0.8841 - val_loss: 0.3105 - val_acc: 0.8828\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3119 - acc: 0.8847 - val_loss: 0.3083 - val_acc: 0.8850\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3114 - acc: 0.8849 - val_loss: 0.3091 - val_acc: 0.8859\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3110 - acc: 0.8847 - val_loss: 0.3082 - val_acc: 0.8852\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3107 - acc: 0.8851 - val_loss: 0.3087 - val_acc: 0.8856\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3108 - acc: 0.8845 - val_loss: 0.3110 - val_acc: 0.8879\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3104 - acc: 0.8845 - val_loss: 0.3099 - val_acc: 0.8845\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3105 - acc: 0.8848 - val_loss: 0.3083 - val_acc: 0.8874\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3102 - acc: 0.8853 - val_loss: 0.3097 - val_acc: 0.8861\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3103 - acc: 0.8853 - val_loss: 0.3088 - val_acc: 0.8865\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3103 - acc: 0.8850 - val_loss: 0.3087 - val_acc: 0.8874\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8856 - val_loss: 0.3098 - val_acc: 0.8868\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8852 - val_loss: 0.3096 - val_acc: 0.8861\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3100 - acc: 0.8857 - val_loss: 0.3082 - val_acc: 0.8870\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3103 - acc: 0.8854 - val_loss: 0.3082 - val_acc: 0.8863\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3098 - acc: 0.8853 - val_loss: 0.3093 - val_acc: 0.8863\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3097 - acc: 0.8852 - val_loss: 0.3088 - val_acc: 0.8854\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8855 - val_loss: 0.3087 - val_acc: 0.8856\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8846 - val_loss: 0.3090 - val_acc: 0.8865\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8852 - val_loss: 0.3096 - val_acc: 0.8856\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3094 - acc: 0.8851 - val_loss: 0.3096 - val_acc: 0.8854\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3096 - acc: 0.8859 - val_loss: 0.3097 - val_acc: 0.8865\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3096 - acc: 0.8851 - val_loss: 0.3086 - val_acc: 0.8861\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3096 - acc: 0.8856 - val_loss: 0.3091 - val_acc: 0.8859\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3096 - acc: 0.8855 - val_loss: 0.3087 - val_acc: 0.8854\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8858 - val_loss: 0.3097 - val_acc: 0.8870\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3094 - acc: 0.8853 - val_loss: 0.3101 - val_acc: 0.8837\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8852 - val_loss: 0.3096 - val_acc: 0.8859\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3094 - acc: 0.8854 - val_loss: 0.3094 - val_acc: 0.8861\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8856 - val_loss: 0.3103 - val_acc: 0.8863\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3094 - acc: 0.8853 - val_loss: 0.3108 - val_acc: 0.8856\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8857 - val_loss: 0.3095 - val_acc: 0.8865\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3092 - acc: 0.8856 - val_loss: 0.3159 - val_acc: 0.8832\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3092 - acc: 0.8852 - val_loss: 0.3097 - val_acc: 0.8868\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8857 - val_loss: 0.3099 - val_acc: 0.8861\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3093 - acc: 0.8857 - val_loss: 0.3092 - val_acc: 0.8868\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8858 - val_loss: 0.3090 - val_acc: 0.8872\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8862 - val_loss: 0.3096 - val_acc: 0.8876\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3089 - acc: 0.8858 - val_loss: 0.3123 - val_acc: 0.8854\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3089 - acc: 0.8856 - val_loss: 0.3095 - val_acc: 0.8890\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8858 - val_loss: 0.3095 - val_acc: 0.8881\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3088 - acc: 0.8857 - val_loss: 0.3096 - val_acc: 0.8861\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8852 - val_loss: 0.3094 - val_acc: 0.8881\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8860 - val_loss: 0.3088 - val_acc: 0.8879\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8863 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8862 - val_loss: 0.3109 - val_acc: 0.8852\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3086 - acc: 0.8856 - val_loss: 0.3096 - val_acc: 0.8876\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8859 - val_loss: 0.3088 - val_acc: 0.8883\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8862 - val_loss: 0.3086 - val_acc: 0.8892\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8858 - val_loss: 0.3090 - val_acc: 0.8876\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8857 - val_loss: 0.3084 - val_acc: 0.8890\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8862 - val_loss: 0.3095 - val_acc: 0.8883\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8861 - val_loss: 0.3094 - val_acc: 0.8859\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8857 - val_loss: 0.3087 - val_acc: 0.8881\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8861 - val_loss: 0.3081 - val_acc: 0.8874\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3080 - acc: 0.8857 - val_loss: 0.3091 - val_acc: 0.8892\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8862 - val_loss: 0.3089 - val_acc: 0.8870\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3081 - acc: 0.8862 - val_loss: 0.3093 - val_acc: 0.8885\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3084 - acc: 0.8866 - val_loss: 0.3079 - val_acc: 0.8887\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3081 - acc: 0.8858 - val_loss: 0.3089 - val_acc: 0.8901\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8856 - val_loss: 0.3078 - val_acc: 0.8892\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3081 - acc: 0.8859 - val_loss: 0.3094 - val_acc: 0.8872\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3078 - acc: 0.8857 - val_loss: 0.3078 - val_acc: 0.8890\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8859 - val_loss: 0.3081 - val_acc: 0.8887\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8853 - val_loss: 0.3085 - val_acc: 0.8890\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8864 - val_loss: 0.3089 - val_acc: 0.8890\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3079 - acc: 0.8855 - val_loss: 0.3087 - val_acc: 0.8876\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8856 - val_loss: 0.3086 - val_acc: 0.8879\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8855 - val_loss: 0.3085 - val_acc: 0.8892\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3079 - acc: 0.8861 - val_loss: 0.3086 - val_acc: 0.8883\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8855 - val_loss: 0.3084 - val_acc: 0.8876\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8852 - val_loss: 0.3086 - val_acc: 0.8879\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8861 - val_loss: 0.3095 - val_acc: 0.8887\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3077 - acc: 0.8863 - val_loss: 0.3083 - val_acc: 0.8865\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8860 - val_loss: 0.3081 - val_acc: 0.8898\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3078 - acc: 0.8859 - val_loss: 0.3075 - val_acc: 0.8879\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8858 - val_loss: 0.3084 - val_acc: 0.8892\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8857 - val_loss: 0.3090 - val_acc: 0.8885\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3079 - acc: 0.8861 - val_loss: 0.3091 - val_acc: 0.8885\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8853 - val_loss: 0.3097 - val_acc: 0.8876\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8858 - val_loss: 0.3101 - val_acc: 0.8881\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8858 - val_loss: 0.3088 - val_acc: 0.8890\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3083 - val_acc: 0.8887\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3077 - acc: 0.8864 - val_loss: 0.3085 - val_acc: 0.8883\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3077 - acc: 0.8858 - val_loss: 0.3103 - val_acc: 0.8870\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8863 - val_loss: 0.3091 - val_acc: 0.8883\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3077 - acc: 0.8861 - val_loss: 0.3086 - val_acc: 0.8883\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8867 - val_loss: 0.3076 - val_acc: 0.8881\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8862 - val_loss: 0.3112 - val_acc: 0.8837\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8864 - val_loss: 0.3085 - val_acc: 0.8901\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8860 - val_loss: 0.3088 - val_acc: 0.8881\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8858 - val_loss: 0.3088 - val_acc: 0.8859\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3076 - acc: 0.8865 - val_loss: 0.3095 - val_acc: 0.8870\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3089 - val_acc: 0.8872\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8855 - val_loss: 0.3089 - val_acc: 0.8892\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3072 - acc: 0.8868 - val_loss: 0.3086 - val_acc: 0.8887\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8860 - val_loss: 0.3083 - val_acc: 0.8881\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3076 - acc: 0.8862 - val_loss: 0.3082 - val_acc: 0.8881\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8861 - val_loss: 0.3085 - val_acc: 0.8870\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3097 - val_acc: 0.8887\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8862 - val_loss: 0.3087 - val_acc: 0.8894\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8863 - val_loss: 0.3083 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8869 - val_loss: 0.3087 - val_acc: 0.8887\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8864 - val_loss: 0.3080 - val_acc: 0.8879\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8862 - val_loss: 0.3085 - val_acc: 0.8868\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3082 - val_acc: 0.8876\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8863 - val_loss: 0.3088 - val_acc: 0.8876\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8866 - val_loss: 0.3078 - val_acc: 0.8879\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8859 - val_loss: 0.3079 - val_acc: 0.8881\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8872 - val_loss: 0.3104 - val_acc: 0.8843\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8872 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8861 - val_loss: 0.3092 - val_acc: 0.8848\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8861 - val_loss: 0.3084 - val_acc: 0.8883\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8865 - val_loss: 0.3083 - val_acc: 0.8898\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3088 - val_acc: 0.8885\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3091 - val_acc: 0.8874\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3072 - acc: 0.8865 - val_loss: 0.3093 - val_acc: 0.8885\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8858 - val_loss: 0.3088 - val_acc: 0.8863\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8863 - val_loss: 0.3093 - val_acc: 0.8874\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8863 - val_loss: 0.3081 - val_acc: 0.8872\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8859 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8869 - val_loss: 0.3085 - val_acc: 0.8879\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8866 - val_loss: 0.3093 - val_acc: 0.8874\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3074 - acc: 0.8864 - val_loss: 0.3084 - val_acc: 0.8879\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3072 - acc: 0.8866 - val_loss: 0.3107 - val_acc: 0.8865\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8860 - val_loss: 0.3089 - val_acc: 0.8868\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8864 - val_loss: 0.3099 - val_acc: 0.8868\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3094 - val_acc: 0.8883\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8862 - val_loss: 0.3079 - val_acc: 0.8883\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8864 - val_loss: 0.3073 - val_acc: 0.8885\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8867 - val_loss: 0.3098 - val_acc: 0.8885\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8861 - val_loss: 0.3082 - val_acc: 0.8868\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8869 - val_loss: 0.3073 - val_acc: 0.8883\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3073 - acc: 0.8866 - val_loss: 0.3097 - val_acc: 0.8874\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8870 - val_loss: 0.3080 - val_acc: 0.8872\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3072 - acc: 0.8866 - val_loss: 0.3090 - val_acc: 0.8874\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8870 - val_loss: 0.3094 - val_acc: 0.8879\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8862 - val_loss: 0.3092 - val_acc: 0.8892\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8865 - val_loss: 0.3084 - val_acc: 0.8876\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3073 - acc: 0.8866 - val_loss: 0.3076 - val_acc: 0.8881\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3074 - acc: 0.8862 - val_loss: 0.3078 - val_acc: 0.8890\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8870 - val_loss: 0.3099 - val_acc: 0.8887\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8858 - val_loss: 0.3081 - val_acc: 0.8883\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.3078 - val_acc: 0.8876\n",
            "acc: 88.76%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.3492 - acc: 0.8691 - val_loss: 0.3230 - val_acc: 0.8863\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3139 - acc: 0.8863 - val_loss: 0.3172 - val_acc: 0.8865\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3121 - acc: 0.8863 - val_loss: 0.3172 - val_acc: 0.8852\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3113 - acc: 0.8870 - val_loss: 0.3164 - val_acc: 0.8850\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3103 - acc: 0.8868 - val_loss: 0.3163 - val_acc: 0.8854\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3097 - acc: 0.8868 - val_loss: 0.3162 - val_acc: 0.8876\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8867 - val_loss: 0.3163 - val_acc: 0.8863\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8863 - val_loss: 0.3141 - val_acc: 0.8859\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8882 - val_loss: 0.3147 - val_acc: 0.8841\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3077 - acc: 0.8870 - val_loss: 0.3130 - val_acc: 0.8841\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8870 - val_loss: 0.3159 - val_acc: 0.8825\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.3134 - val_acc: 0.8850\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8874 - val_loss: 0.3127 - val_acc: 0.8874\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8871 - val_loss: 0.3132 - val_acc: 0.8885\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8867 - val_loss: 0.3114 - val_acc: 0.8863\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8863 - val_loss: 0.3120 - val_acc: 0.8865\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8870 - val_loss: 0.3152 - val_acc: 0.8861\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8866 - val_loss: 0.3117 - val_acc: 0.8841\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8875 - val_loss: 0.3111 - val_acc: 0.8879\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8868 - val_loss: 0.3117 - val_acc: 0.8870\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8869 - val_loss: 0.3121 - val_acc: 0.8823\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8866 - val_loss: 0.3114 - val_acc: 0.8883\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8876 - val_loss: 0.3103 - val_acc: 0.8868\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8866 - val_loss: 0.3107 - val_acc: 0.8874\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8868 - val_loss: 0.3116 - val_acc: 0.8872\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8864 - val_loss: 0.3116 - val_acc: 0.8874\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3053 - acc: 0.8872 - val_loss: 0.3130 - val_acc: 0.8825\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8870 - val_loss: 0.3106 - val_acc: 0.8868\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8863 - val_loss: 0.3117 - val_acc: 0.8885\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3109 - val_acc: 0.8856\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8868 - val_loss: 0.3113 - val_acc: 0.8872\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3109 - val_acc: 0.8821\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8863 - val_loss: 0.3120 - val_acc: 0.8856\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8870 - val_loss: 0.3123 - val_acc: 0.8828\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8864 - val_loss: 0.3117 - val_acc: 0.8839\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8868 - val_loss: 0.3154 - val_acc: 0.8861\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8864 - val_loss: 0.3160 - val_acc: 0.8825\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8869 - val_loss: 0.3112 - val_acc: 0.8854\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8863 - val_loss: 0.3128 - val_acc: 0.8841\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8869 - val_loss: 0.3115 - val_acc: 0.8876\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3041 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8848\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3037 - acc: 0.8865 - val_loss: 0.3124 - val_acc: 0.8850\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3041 - acc: 0.8875 - val_loss: 0.3110 - val_acc: 0.8832\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8864 - val_loss: 0.3146 - val_acc: 0.8841\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8871 - val_loss: 0.3117 - val_acc: 0.8845\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8870 - val_loss: 0.3124 - val_acc: 0.8870\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8872 - val_loss: 0.3120 - val_acc: 0.8832\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8873 - val_loss: 0.3114 - val_acc: 0.8865\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3041 - acc: 0.8860 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8860 - val_loss: 0.3116 - val_acc: 0.8865\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8868 - val_loss: 0.3131 - val_acc: 0.8859\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8869 - val_loss: 0.3120 - val_acc: 0.8868\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8867 - val_loss: 0.3126 - val_acc: 0.8841\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8865 - val_loss: 0.3119 - val_acc: 0.8870\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8871 - val_loss: 0.3114 - val_acc: 0.8852\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8869 - val_loss: 0.3139 - val_acc: 0.8806\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8871 - val_loss: 0.3121 - val_acc: 0.8854\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3039 - acc: 0.8868 - val_loss: 0.3133 - val_acc: 0.8823\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3122 - val_acc: 0.8834\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8873 - val_loss: 0.3121 - val_acc: 0.8841\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8864 - val_loss: 0.3125 - val_acc: 0.8823\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8871 - val_loss: 0.3115 - val_acc: 0.8865\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3038 - acc: 0.8870 - val_loss: 0.3119 - val_acc: 0.8830\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8870 - val_loss: 0.3140 - val_acc: 0.8841\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3037 - acc: 0.8870 - val_loss: 0.3115 - val_acc: 0.8845\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8870 - val_loss: 0.3118 - val_acc: 0.8872\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8871 - val_loss: 0.3148 - val_acc: 0.8814\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8867 - val_loss: 0.3118 - val_acc: 0.8856\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3118 - val_acc: 0.8852\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8863 - val_loss: 0.3130 - val_acc: 0.8830\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8841\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8865 - val_loss: 0.3110 - val_acc: 0.8859\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8869 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8867 - val_loss: 0.3134 - val_acc: 0.8850\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8863 - val_loss: 0.3134 - val_acc: 0.8870\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8867 - val_loss: 0.3124 - val_acc: 0.8852\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8874 - val_loss: 0.3142 - val_acc: 0.8839\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8867 - val_loss: 0.3126 - val_acc: 0.8865\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8869 - val_loss: 0.3123 - val_acc: 0.8848\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8867 - val_loss: 0.3152 - val_acc: 0.8819\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8869 - val_loss: 0.3136 - val_acc: 0.8832\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8873 - val_loss: 0.3132 - val_acc: 0.8863\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8872 - val_loss: 0.3117 - val_acc: 0.8828\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3122 - val_acc: 0.8845\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8868 - val_loss: 0.3129 - val_acc: 0.8845\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8867 - val_loss: 0.3118 - val_acc: 0.8870\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3121 - val_acc: 0.8868\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8866 - val_loss: 0.3125 - val_acc: 0.8856\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8869 - val_loss: 0.3114 - val_acc: 0.8845\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8865 - val_loss: 0.3131 - val_acc: 0.8845\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8869 - val_loss: 0.3139 - val_acc: 0.8823\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8870 - val_loss: 0.3125 - val_acc: 0.8848\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8870 - val_loss: 0.3139 - val_acc: 0.8821\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8873 - val_loss: 0.3137 - val_acc: 0.8817\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8872 - val_loss: 0.3129 - val_acc: 0.8850\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8864 - val_loss: 0.3133 - val_acc: 0.8832\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3037 - acc: 0.8869 - val_loss: 0.3147 - val_acc: 0.8812\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8873 - val_loss: 0.3112 - val_acc: 0.8845\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8873 - val_loss: 0.3114 - val_acc: 0.8830\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8872 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3133 - val_acc: 0.8837\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8874 - val_loss: 0.3142 - val_acc: 0.8806\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8870 - val_loss: 0.3119 - val_acc: 0.8843\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8876 - val_loss: 0.3135 - val_acc: 0.8865\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3036 - acc: 0.8876 - val_loss: 0.3115 - val_acc: 0.8856\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3036 - acc: 0.8861 - val_loss: 0.3142 - val_acc: 0.8845\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8868 - val_loss: 0.3118 - val_acc: 0.8848\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8863 - val_loss: 0.3119 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8872 - val_loss: 0.3118 - val_acc: 0.8863\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8873 - val_loss: 0.3136 - val_acc: 0.8839\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3118 - val_acc: 0.8856\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3038 - acc: 0.8869 - val_loss: 0.3126 - val_acc: 0.8843\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8867 - val_loss: 0.3125 - val_acc: 0.8834\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8866 - val_loss: 0.3127 - val_acc: 0.8845\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8876 - val_loss: 0.3122 - val_acc: 0.8861\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8871 - val_loss: 0.3124 - val_acc: 0.8839\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8873 - val_loss: 0.3126 - val_acc: 0.8828\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3037 - acc: 0.8865 - val_loss: 0.3125 - val_acc: 0.8854\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8870 - val_loss: 0.3107 - val_acc: 0.8868\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8867 - val_loss: 0.3124 - val_acc: 0.8850\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8871 - val_loss: 0.3109 - val_acc: 0.8861\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3127 - val_acc: 0.8839\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8866 - val_loss: 0.3131 - val_acc: 0.8850\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3038 - acc: 0.8868 - val_loss: 0.3119 - val_acc: 0.8850\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8861 - val_loss: 0.3123 - val_acc: 0.8834\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8868 - val_loss: 0.3121 - val_acc: 0.8854\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8870 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8865 - val_loss: 0.3110 - val_acc: 0.8870\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8868 - val_loss: 0.3123 - val_acc: 0.8848\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3033 - acc: 0.8870 - val_loss: 0.3125 - val_acc: 0.8845\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8867 - val_loss: 0.3116 - val_acc: 0.8841\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8868 - val_loss: 0.3117 - val_acc: 0.8856\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3134 - val_acc: 0.8823\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8871 - val_loss: 0.3117 - val_acc: 0.8832\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8868 - val_loss: 0.3117 - val_acc: 0.8843\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8872 - val_loss: 0.3100 - val_acc: 0.8859\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8867 - val_loss: 0.3164 - val_acc: 0.8806\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8869 - val_loss: 0.3121 - val_acc: 0.8856\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8872 - val_loss: 0.3130 - val_acc: 0.8830\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8864 - val_loss: 0.3108 - val_acc: 0.8839\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8866 - val_loss: 0.3109 - val_acc: 0.8874\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8867 - val_loss: 0.3123 - val_acc: 0.8865\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8873 - val_loss: 0.3118 - val_acc: 0.8830\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8871 - val_loss: 0.3137 - val_acc: 0.8843\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8870 - val_loss: 0.3127 - val_acc: 0.8850\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8841\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8866 - val_loss: 0.3121 - val_acc: 0.8848\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8873 - val_loss: 0.3124 - val_acc: 0.8868\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3038 - acc: 0.8866 - val_loss: 0.3121 - val_acc: 0.8883\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8871 - val_loss: 0.3124 - val_acc: 0.8841\n",
            "acc: 88.41%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.3435 - acc: 0.8733 - val_loss: 0.3191 - val_acc: 0.8868\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3130 - acc: 0.8852 - val_loss: 0.3145 - val_acc: 0.8859\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3112 - acc: 0.8861 - val_loss: 0.3140 - val_acc: 0.8850\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3107 - acc: 0.8856 - val_loss: 0.3127 - val_acc: 0.8832\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3102 - acc: 0.8865 - val_loss: 0.3119 - val_acc: 0.8883\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3096 - acc: 0.8868 - val_loss: 0.3124 - val_acc: 0.8845\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8873 - val_loss: 0.3107 - val_acc: 0.8868\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3087 - acc: 0.8884 - val_loss: 0.3099 - val_acc: 0.8885\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8879 - val_loss: 0.3129 - val_acc: 0.8868\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8889 - val_loss: 0.3092 - val_acc: 0.8865\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8873 - val_loss: 0.3084 - val_acc: 0.8887\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8884 - val_loss: 0.3094 - val_acc: 0.8870\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8877 - val_loss: 0.3095 - val_acc: 0.8883\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8882 - val_loss: 0.3079 - val_acc: 0.8881\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8879 - val_loss: 0.3095 - val_acc: 0.8876\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8886 - val_loss: 0.3082 - val_acc: 0.8872\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8882 - val_loss: 0.3079 - val_acc: 0.8894\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8882 - val_loss: 0.3080 - val_acc: 0.8872\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3069 - acc: 0.8884 - val_loss: 0.3074 - val_acc: 0.8887\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8884 - val_loss: 0.3080 - val_acc: 0.8872\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8878 - val_loss: 0.3085 - val_acc: 0.8879\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8887 - val_loss: 0.3113 - val_acc: 0.8854\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8887 - val_loss: 0.3077 - val_acc: 0.8868\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8880 - val_loss: 0.3083 - val_acc: 0.8850\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3066 - acc: 0.8881 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8884 - val_loss: 0.3111 - val_acc: 0.8876\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8885 - val_loss: 0.3079 - val_acc: 0.8868\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8891 - val_loss: 0.3074 - val_acc: 0.8872\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3064 - acc: 0.8886 - val_loss: 0.3073 - val_acc: 0.8879\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8888 - val_loss: 0.3081 - val_acc: 0.8872\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8889 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8884 - val_loss: 0.3089 - val_acc: 0.8872\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8881 - val_loss: 0.3067 - val_acc: 0.8874\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8885 - val_loss: 0.3066 - val_acc: 0.8885\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3064 - acc: 0.8883 - val_loss: 0.3080 - val_acc: 0.8870\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8887 - val_loss: 0.3068 - val_acc: 0.8861\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8887 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8883 - val_loss: 0.3062 - val_acc: 0.8876\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8894 - val_loss: 0.3091 - val_acc: 0.8872\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8881 - val_loss: 0.3081 - val_acc: 0.8874\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3064 - acc: 0.8882 - val_loss: 0.3078 - val_acc: 0.8870\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8883 - val_loss: 0.3087 - val_acc: 0.8874\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3064 - acc: 0.8887 - val_loss: 0.3082 - val_acc: 0.8887\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8888 - val_loss: 0.3070 - val_acc: 0.8883\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8885 - val_loss: 0.3072 - val_acc: 0.8881\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8890 - val_loss: 0.3063 - val_acc: 0.8887\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8889 - val_loss: 0.3074 - val_acc: 0.8883\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8888 - val_loss: 0.3080 - val_acc: 0.8870\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3073 - val_acc: 0.8859\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8887 - val_loss: 0.3078 - val_acc: 0.8859\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8886 - val_loss: 0.3085 - val_acc: 0.8854\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8884 - val_loss: 0.3061 - val_acc: 0.8885\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8879 - val_loss: 0.3070 - val_acc: 0.8885\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3063 - acc: 0.8887 - val_loss: 0.3066 - val_acc: 0.8883\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8882 - val_loss: 0.3062 - val_acc: 0.8863\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3063 - val_acc: 0.8885\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8883 - val_loss: 0.3079 - val_acc: 0.8883\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8881 - val_loss: 0.3069 - val_acc: 0.8868\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8881 - val_loss: 0.3057 - val_acc: 0.8883\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8884 - val_loss: 0.3053 - val_acc: 0.8887\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8892 - val_loss: 0.3072 - val_acc: 0.8885\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8887 - val_loss: 0.3057 - val_acc: 0.8876\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3062 - acc: 0.8878 - val_loss: 0.3074 - val_acc: 0.8874\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8885 - val_loss: 0.3063 - val_acc: 0.8879\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8885 - val_loss: 0.3064 - val_acc: 0.8894\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8884 - val_loss: 0.3075 - val_acc: 0.8885\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8881 - val_loss: 0.3061 - val_acc: 0.8892\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8886 - val_loss: 0.3089 - val_acc: 0.8861\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8888 - val_loss: 0.3061 - val_acc: 0.8901\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8886 - val_loss: 0.3068 - val_acc: 0.8870\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8879 - val_loss: 0.3073 - val_acc: 0.8874\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8888 - val_loss: 0.3063 - val_acc: 0.8892\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8881 - val_loss: 0.3065 - val_acc: 0.8883\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8882 - val_loss: 0.3070 - val_acc: 0.8876\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3061 - acc: 0.8885 - val_loss: 0.3069 - val_acc: 0.8876\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8882 - val_loss: 0.3070 - val_acc: 0.8879\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8879 - val_loss: 0.3058 - val_acc: 0.8874\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8890 - val_loss: 0.3072 - val_acc: 0.8863\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3061 - acc: 0.8882 - val_loss: 0.3064 - val_acc: 0.8868\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3058 - acc: 0.8879 - val_loss: 0.3063 - val_acc: 0.8881\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8879 - val_loss: 0.3069 - val_acc: 0.8881\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8886 - val_loss: 0.3063 - val_acc: 0.8883\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8885 - val_loss: 0.3061 - val_acc: 0.8890\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8876 - val_loss: 0.3078 - val_acc: 0.8879\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8886 - val_loss: 0.3065 - val_acc: 0.8883\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8877 - val_loss: 0.3061 - val_acc: 0.8896\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8886 - val_loss: 0.3083 - val_acc: 0.8879\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3076 - val_acc: 0.8898\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8890 - val_loss: 0.3065 - val_acc: 0.8883\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8888 - val_loss: 0.3063 - val_acc: 0.8896\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3057 - val_acc: 0.8894\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8885 - val_loss: 0.3077 - val_acc: 0.8863\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3067 - val_acc: 0.8879\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8887 - val_loss: 0.3079 - val_acc: 0.8885\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3055 - acc: 0.8883 - val_loss: 0.3069 - val_acc: 0.8883\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8886 - val_loss: 0.3054 - val_acc: 0.8881\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3061 - acc: 0.8884 - val_loss: 0.3062 - val_acc: 0.8887\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8879 - val_loss: 0.3067 - val_acc: 0.8881\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8883 - val_loss: 0.3064 - val_acc: 0.8903\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8885 - val_loss: 0.3064 - val_acc: 0.8876\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8881 - val_loss: 0.3063 - val_acc: 0.8894\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8881 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8880 - val_loss: 0.3058 - val_acc: 0.8896\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8884 - val_loss: 0.3060 - val_acc: 0.8916\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8885 - val_loss: 0.3066 - val_acc: 0.8876\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3058 - acc: 0.8890 - val_loss: 0.3061 - val_acc: 0.8883\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3062 - acc: 0.8882 - val_loss: 0.3056 - val_acc: 0.8907\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8889 - val_loss: 0.3061 - val_acc: 0.8887\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8883 - val_loss: 0.3069 - val_acc: 0.8876\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8888 - val_loss: 0.3058 - val_acc: 0.8879\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8888 - val_loss: 0.3063 - val_acc: 0.8896\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3063 - acc: 0.8882 - val_loss: 0.3064 - val_acc: 0.8890\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8889 - val_loss: 0.3061 - val_acc: 0.8901\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8885 - val_loss: 0.3063 - val_acc: 0.8901\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3062 - val_acc: 0.8912\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8883 - val_loss: 0.3068 - val_acc: 0.8905\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8884 - val_loss: 0.3080 - val_acc: 0.8883\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8884 - val_loss: 0.3060 - val_acc: 0.8887\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8887 - val_loss: 0.3065 - val_acc: 0.8890\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8887 - val_loss: 0.3059 - val_acc: 0.8890\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8883 - val_loss: 0.3057 - val_acc: 0.8910\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3064 - val_acc: 0.8896\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8890 - val_loss: 0.3054 - val_acc: 0.8876\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8886 - val_loss: 0.3068 - val_acc: 0.8903\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8885 - val_loss: 0.3083 - val_acc: 0.8903\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8882 - val_loss: 0.3065 - val_acc: 0.8887\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8887 - val_loss: 0.3057 - val_acc: 0.8883\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3057 - acc: 0.8883 - val_loss: 0.3063 - val_acc: 0.8907\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8884 - val_loss: 0.3065 - val_acc: 0.8898\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8883 - val_loss: 0.3055 - val_acc: 0.8898\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8880 - val_loss: 0.3057 - val_acc: 0.8898\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8881 - val_loss: 0.3063 - val_acc: 0.8887\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8884 - val_loss: 0.3071 - val_acc: 0.8890\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8889 - val_loss: 0.3057 - val_acc: 0.8881\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8887 - val_loss: 0.3069 - val_acc: 0.8898\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8888 - val_loss: 0.3076 - val_acc: 0.8874\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8881 - val_loss: 0.3053 - val_acc: 0.8883\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8888 - val_loss: 0.3069 - val_acc: 0.8885\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8884 - val_loss: 0.3068 - val_acc: 0.8903\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8884 - val_loss: 0.3069 - val_acc: 0.8879\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8885 - val_loss: 0.3073 - val_acc: 0.8883\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8879 - val_loss: 0.3055 - val_acc: 0.8905\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8882 - val_loss: 0.3052 - val_acc: 0.8887\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8882 - val_loss: 0.3063 - val_acc: 0.8892\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3062 - val_acc: 0.8898\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8882 - val_loss: 0.3077 - val_acc: 0.8881\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8879 - val_loss: 0.3059 - val_acc: 0.8894\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8883 - val_loss: 0.3072 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8884 - val_loss: 0.3071 - val_acc: 0.8898\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3059 - acc: 0.8881 - val_loss: 0.3070 - val_acc: 0.8903\n",
            "acc: 89.03%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 28us/step - loss: 0.3372 - acc: 0.8743 - val_loss: 0.3055 - val_acc: 0.8903\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3136 - acc: 0.8866 - val_loss: 0.3019 - val_acc: 0.8907\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3126 - acc: 0.8856 - val_loss: 0.3008 - val_acc: 0.8885\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3118 - acc: 0.8867 - val_loss: 0.3017 - val_acc: 0.8916\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3110 - acc: 0.8864 - val_loss: 0.3015 - val_acc: 0.8894\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3107 - acc: 0.8872 - val_loss: 0.3016 - val_acc: 0.8868\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3104 - acc: 0.8867 - val_loss: 0.3003 - val_acc: 0.8901\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3099 - acc: 0.8871 - val_loss: 0.3003 - val_acc: 0.8901\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3099 - acc: 0.8870 - val_loss: 0.2983 - val_acc: 0.8892\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3098 - acc: 0.8877 - val_loss: 0.2991 - val_acc: 0.8881\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8869 - val_loss: 0.2985 - val_acc: 0.8912\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8864 - val_loss: 0.3024 - val_acc: 0.8865\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3092 - acc: 0.8876 - val_loss: 0.2996 - val_acc: 0.8903\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3092 - acc: 0.8868 - val_loss: 0.2985 - val_acc: 0.8894\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8871 - val_loss: 0.2984 - val_acc: 0.8890\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8865 - val_loss: 0.3000 - val_acc: 0.8861\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3094 - acc: 0.8869 - val_loss: 0.2998 - val_acc: 0.8894\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8876 - val_loss: 0.3006 - val_acc: 0.8894\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8870 - val_loss: 0.2976 - val_acc: 0.8923\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8867 - val_loss: 0.3007 - val_acc: 0.8876\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3087 - acc: 0.8870 - val_loss: 0.2995 - val_acc: 0.8901\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8871 - val_loss: 0.2984 - val_acc: 0.8894\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8879 - val_loss: 0.2997 - val_acc: 0.8883\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8868 - val_loss: 0.2980 - val_acc: 0.8901\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3086 - acc: 0.8868 - val_loss: 0.2978 - val_acc: 0.8910\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3085 - acc: 0.8875 - val_loss: 0.2976 - val_acc: 0.8898\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8867 - val_loss: 0.2977 - val_acc: 0.8905\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8873 - val_loss: 0.2975 - val_acc: 0.8879\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8872 - val_loss: 0.2990 - val_acc: 0.8892\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8873 - val_loss: 0.2994 - val_acc: 0.8845\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8869 - val_loss: 0.2972 - val_acc: 0.8907\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8870 - val_loss: 0.2982 - val_acc: 0.8905\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8877 - val_loss: 0.2967 - val_acc: 0.8870\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8874 - val_loss: 0.2961 - val_acc: 0.8874\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8871 - val_loss: 0.2978 - val_acc: 0.8903\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8867 - val_loss: 0.2973 - val_acc: 0.8903\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8874 - val_loss: 0.2969 - val_acc: 0.8887\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3083 - acc: 0.8875 - val_loss: 0.2955 - val_acc: 0.8905\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8874 - val_loss: 0.2957 - val_acc: 0.8872\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8869 - val_loss: 0.2956 - val_acc: 0.8898\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8875 - val_loss: 0.2981 - val_acc: 0.8901\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8876 - val_loss: 0.2978 - val_acc: 0.8907\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8876 - val_loss: 0.2966 - val_acc: 0.8901\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3080 - acc: 0.8868 - val_loss: 0.2956 - val_acc: 0.8912\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8873 - val_loss: 0.2972 - val_acc: 0.8910\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8867 - val_loss: 0.2990 - val_acc: 0.8865\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8872 - val_loss: 0.2971 - val_acc: 0.8879\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8877 - val_loss: 0.2989 - val_acc: 0.8890\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8869 - val_loss: 0.2967 - val_acc: 0.8885\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8874 - val_loss: 0.2979 - val_acc: 0.8903\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8868 - val_loss: 0.2951 - val_acc: 0.8896\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8872 - val_loss: 0.2961 - val_acc: 0.8887\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8872 - val_loss: 0.2965 - val_acc: 0.8903\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8867 - val_loss: 0.2970 - val_acc: 0.8896\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8868 - val_loss: 0.2954 - val_acc: 0.8903\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8870 - val_loss: 0.2975 - val_acc: 0.8896\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8874 - val_loss: 0.2956 - val_acc: 0.8910\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.2975 - val_acc: 0.8894\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3077 - acc: 0.8878 - val_loss: 0.2996 - val_acc: 0.8872\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.2957 - val_acc: 0.8894\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8865 - val_loss: 0.2954 - val_acc: 0.8910\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8878 - val_loss: 0.2993 - val_acc: 0.8883\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8872 - val_loss: 0.2967 - val_acc: 0.8896\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8872 - val_loss: 0.2963 - val_acc: 0.8901\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3079 - acc: 0.8870 - val_loss: 0.2962 - val_acc: 0.8885\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8865 - val_loss: 0.2978 - val_acc: 0.8885\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8870 - val_loss: 0.2989 - val_acc: 0.8896\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8873 - val_loss: 0.2967 - val_acc: 0.8887\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8871 - val_loss: 0.3019 - val_acc: 0.8870\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8870 - val_loss: 0.2993 - val_acc: 0.8843\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3075 - acc: 0.8867 - val_loss: 0.2990 - val_acc: 0.8898\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3075 - acc: 0.8866 - val_loss: 0.2982 - val_acc: 0.8898\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8869 - val_loss: 0.2967 - val_acc: 0.8885\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8866 - val_loss: 0.2999 - val_acc: 0.8850\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8878 - val_loss: 0.2999 - val_acc: 0.8852\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8868 - val_loss: 0.2986 - val_acc: 0.8890\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8867 - val_loss: 0.2995 - val_acc: 0.8879\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3075 - acc: 0.8869 - val_loss: 0.2976 - val_acc: 0.8885\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8867 - val_loss: 0.2959 - val_acc: 0.8905\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8873 - val_loss: 0.2976 - val_acc: 0.8876\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8879 - val_loss: 0.2962 - val_acc: 0.8883\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8873 - val_loss: 0.2976 - val_acc: 0.8885\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8874 - val_loss: 0.2966 - val_acc: 0.8896\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8869 - val_loss: 0.2968 - val_acc: 0.8894\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8871 - val_loss: 0.2996 - val_acc: 0.8859\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8869 - val_loss: 0.2976 - val_acc: 0.8894\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8874 - val_loss: 0.2966 - val_acc: 0.8898\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8867 - val_loss: 0.2991 - val_acc: 0.8896\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8872 - val_loss: 0.2973 - val_acc: 0.8883\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8872 - val_loss: 0.2972 - val_acc: 0.8912\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8871 - val_loss: 0.2970 - val_acc: 0.8876\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8875 - val_loss: 0.2963 - val_acc: 0.8903\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8875 - val_loss: 0.2960 - val_acc: 0.8896\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8870 - val_loss: 0.2997 - val_acc: 0.8856\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8866 - val_loss: 0.2972 - val_acc: 0.8894\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8872 - val_loss: 0.2959 - val_acc: 0.8910\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8873 - val_loss: 0.2966 - val_acc: 0.8903\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8866 - val_loss: 0.2958 - val_acc: 0.8914\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3074 - acc: 0.8876 - val_loss: 0.2973 - val_acc: 0.8905\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8875 - val_loss: 0.2962 - val_acc: 0.8907\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8868 - val_loss: 0.2972 - val_acc: 0.8905\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8863 - val_loss: 0.2969 - val_acc: 0.8916\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8875 - val_loss: 0.2983 - val_acc: 0.8856\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8873 - val_loss: 0.2979 - val_acc: 0.8885\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3070 - acc: 0.8874 - val_loss: 0.2997 - val_acc: 0.8863\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3073 - acc: 0.8864 - val_loss: 0.2974 - val_acc: 0.8861\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3073 - acc: 0.8869 - val_loss: 0.2966 - val_acc: 0.8910\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8871 - val_loss: 0.2966 - val_acc: 0.8881\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8880 - val_loss: 0.2976 - val_acc: 0.8910\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8863 - val_loss: 0.2966 - val_acc: 0.8883\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8870 - val_loss: 0.2962 - val_acc: 0.8912\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.2965 - val_acc: 0.8892\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8880 - val_loss: 0.2958 - val_acc: 0.8907\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8871 - val_loss: 0.2962 - val_acc: 0.8921\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8862 - val_loss: 0.2956 - val_acc: 0.8923\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.2944 - val_acc: 0.8921\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8871 - val_loss: 0.2986 - val_acc: 0.8883\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8866 - val_loss: 0.2958 - val_acc: 0.8914\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8870 - val_loss: 0.2962 - val_acc: 0.8916\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8862 - val_loss: 0.2976 - val_acc: 0.8894\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8872 - val_loss: 0.2974 - val_acc: 0.8856\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8864 - val_loss: 0.2955 - val_acc: 0.8894\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3067 - acc: 0.8873 - val_loss: 0.2975 - val_acc: 0.8905\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8865 - val_loss: 0.2957 - val_acc: 0.8905\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2982 - val_acc: 0.8879\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8861 - val_loss: 0.2949 - val_acc: 0.8907\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8864 - val_loss: 0.2967 - val_acc: 0.8905\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3068 - acc: 0.8866 - val_loss: 0.2973 - val_acc: 0.8903\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3068 - acc: 0.8874 - val_loss: 0.2966 - val_acc: 0.8892\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2971 - val_acc: 0.8921\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8869 - val_loss: 0.2972 - val_acc: 0.8918\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8863 - val_loss: 0.2960 - val_acc: 0.8901\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8863 - val_loss: 0.2966 - val_acc: 0.8916\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8865 - val_loss: 0.2949 - val_acc: 0.8907\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8860 - val_loss: 0.2959 - val_acc: 0.8916\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2972 - val_acc: 0.8910\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8872 - val_loss: 0.2979 - val_acc: 0.8885\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8870 - val_loss: 0.2971 - val_acc: 0.8923\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8864 - val_loss: 0.2949 - val_acc: 0.8918\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8866 - val_loss: 0.2963 - val_acc: 0.8923\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2941 - val_acc: 0.8910\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8865 - val_loss: 0.2989 - val_acc: 0.8850\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8870 - val_loss: 0.2979 - val_acc: 0.8907\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2956 - val_acc: 0.8912\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8875 - val_loss: 0.2996 - val_acc: 0.8859\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8864 - val_loss: 0.2974 - val_acc: 0.8879\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8865 - val_loss: 0.2964 - val_acc: 0.8921\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3066 - acc: 0.8873 - val_loss: 0.2963 - val_acc: 0.8905\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8870 - val_loss: 0.2978 - val_acc: 0.8907\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8868 - val_loss: 0.3001 - val_acc: 0.8865\n",
            "acc: 88.65%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 29us/step - loss: 0.3439 - acc: 0.8696 - val_loss: 0.3206 - val_acc: 0.8841\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3154 - acc: 0.8857 - val_loss: 0.3159 - val_acc: 0.8852\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3138 - acc: 0.8850 - val_loss: 0.3135 - val_acc: 0.8854\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3128 - acc: 0.8856 - val_loss: 0.3145 - val_acc: 0.8856\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3124 - acc: 0.8858 - val_loss: 0.3129 - val_acc: 0.8852\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3121 - acc: 0.8860 - val_loss: 0.3118 - val_acc: 0.8859\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3108 - acc: 0.8868 - val_loss: 0.3132 - val_acc: 0.8830\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3106 - acc: 0.8867 - val_loss: 0.3105 - val_acc: 0.8863\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3101 - acc: 0.8860 - val_loss: 0.3103 - val_acc: 0.8868\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3100 - acc: 0.8861 - val_loss: 0.3094 - val_acc: 0.8859\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3097 - acc: 0.8864 - val_loss: 0.3094 - val_acc: 0.8856\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3096 - acc: 0.8864 - val_loss: 0.3096 - val_acc: 0.8865\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3091 - acc: 0.8869 - val_loss: 0.3100 - val_acc: 0.8859\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3087 - acc: 0.8867 - val_loss: 0.3081 - val_acc: 0.8863\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3084 - acc: 0.8865 - val_loss: 0.3077 - val_acc: 0.8865\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8865 - val_loss: 0.3088 - val_acc: 0.8865\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8867 - val_loss: 0.3079 - val_acc: 0.8854\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8865 - val_loss: 0.3076 - val_acc: 0.8856\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3076 - acc: 0.8859 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3074 - acc: 0.8860 - val_loss: 0.3060 - val_acc: 0.8863\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3071 - acc: 0.8855 - val_loss: 0.3062 - val_acc: 0.8859\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3071 - acc: 0.8867 - val_loss: 0.3077 - val_acc: 0.8863\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3067 - acc: 0.8863 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3066 - acc: 0.8863 - val_loss: 0.3051 - val_acc: 0.8865\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8869 - val_loss: 0.3065 - val_acc: 0.8856\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8857 - val_loss: 0.3064 - val_acc: 0.8856\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3063 - acc: 0.8870 - val_loss: 0.3051 - val_acc: 0.8863\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8858 - val_loss: 0.3066 - val_acc: 0.8865\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8860 - val_loss: 0.3067 - val_acc: 0.8863\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8863 - val_loss: 0.3079 - val_acc: 0.8854\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8868 - val_loss: 0.3062 - val_acc: 0.8863\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8864 - val_loss: 0.3065 - val_acc: 0.8865\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8867 - val_loss: 0.3078 - val_acc: 0.8863\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8868 - val_loss: 0.3057 - val_acc: 0.8859\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8856 - val_loss: 0.3062 - val_acc: 0.8856\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8867 - val_loss: 0.3069 - val_acc: 0.8856\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8861 - val_loss: 0.3050 - val_acc: 0.8863\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8868 - val_loss: 0.3055 - val_acc: 0.8845\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3066 - val_acc: 0.8863\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3060 - acc: 0.8865 - val_loss: 0.3086 - val_acc: 0.8859\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 20us/step - loss: 0.3062 - acc: 0.8860 - val_loss: 0.3062 - val_acc: 0.8852\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3058 - acc: 0.8866 - val_loss: 0.3061 - val_acc: 0.8865\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8866 - val_loss: 0.3052 - val_acc: 0.8850\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8864 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8858 - val_loss: 0.3061 - val_acc: 0.8856\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8862 - val_loss: 0.3050 - val_acc: 0.8868\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8859 - val_loss: 0.3052 - val_acc: 0.8845\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8860 - val_loss: 0.3045 - val_acc: 0.8865\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3052 - val_acc: 0.8850\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8863 - val_loss: 0.3066 - val_acc: 0.8852\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8863 - val_loss: 0.3067 - val_acc: 0.8868\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8863 - val_loss: 0.3052 - val_acc: 0.8856\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8859 - val_loss: 0.3057 - val_acc: 0.8859\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8864 - val_loss: 0.3048 - val_acc: 0.8872\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8868 - val_loss: 0.3062 - val_acc: 0.8854\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8863 - val_loss: 0.3053 - val_acc: 0.8861\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8866 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8867 - val_loss: 0.3059 - val_acc: 0.8854\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8860 - val_loss: 0.3072 - val_acc: 0.8856\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8867 - val_loss: 0.3057 - val_acc: 0.8859\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8863 - val_loss: 0.3047 - val_acc: 0.8865\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8867 - val_loss: 0.3056 - val_acc: 0.8874\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8860 - val_loss: 0.3055 - val_acc: 0.8859\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8868 - val_loss: 0.3056 - val_acc: 0.8856\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8860 - val_loss: 0.3062 - val_acc: 0.8865\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8868 - val_loss: 0.3066 - val_acc: 0.8845\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8865 - val_loss: 0.3056 - val_acc: 0.8859\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3057 - acc: 0.8866 - val_loss: 0.3047 - val_acc: 0.8863\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3045 - val_acc: 0.8859\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8863 - val_loss: 0.3047 - val_acc: 0.8856\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8869 - val_loss: 0.3052 - val_acc: 0.8854\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8861 - val_loss: 0.3046 - val_acc: 0.8852\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8864 - val_loss: 0.3071 - val_acc: 0.8825\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8860 - val_loss: 0.3051 - val_acc: 0.8859\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8863 - val_loss: 0.3071 - val_acc: 0.8852\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3050 - val_acc: 0.8861\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8863 - val_loss: 0.3043 - val_acc: 0.8854\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8865 - val_loss: 0.3050 - val_acc: 0.8845\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8860 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8867 - val_loss: 0.3045 - val_acc: 0.8868\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8864 - val_loss: 0.3050 - val_acc: 0.8856\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8864 - val_loss: 0.3045 - val_acc: 0.8868\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8870 - val_loss: 0.3048 - val_acc: 0.8861\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8864 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8857 - val_loss: 0.3055 - val_acc: 0.8850\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8861 - val_loss: 0.3052 - val_acc: 0.8861\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8868 - val_loss: 0.3051 - val_acc: 0.8856\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8870 - val_loss: 0.3053 - val_acc: 0.8850\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8864 - val_loss: 0.3047 - val_acc: 0.8856\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8865 - val_loss: 0.3052 - val_acc: 0.8848\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8861 - val_loss: 0.3058 - val_acc: 0.8856\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8848\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8861 - val_loss: 0.3053 - val_acc: 0.8856\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8866 - val_loss: 0.3042 - val_acc: 0.8856\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3056 - acc: 0.8865 - val_loss: 0.3054 - val_acc: 0.8854\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8874 - val_loss: 0.3041 - val_acc: 0.8856\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3049 - val_acc: 0.8861\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8863 - val_loss: 0.3057 - val_acc: 0.8837\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8867 - val_loss: 0.3048 - val_acc: 0.8861\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3047 - val_acc: 0.8848\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8863 - val_loss: 0.3047 - val_acc: 0.8863\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8867 - val_loss: 0.3047 - val_acc: 0.8845\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3056 - val_acc: 0.8823\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3036 - val_acc: 0.8848\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3053 - acc: 0.8865 - val_loss: 0.3054 - val_acc: 0.8854\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8860 - val_loss: 0.3058 - val_acc: 0.8843\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8869 - val_loss: 0.3054 - val_acc: 0.8845\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8868 - val_loss: 0.3053 - val_acc: 0.8852\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8862 - val_loss: 0.3053 - val_acc: 0.8856\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8869 - val_loss: 0.3050 - val_acc: 0.8848\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3053 - acc: 0.8870 - val_loss: 0.3066 - val_acc: 0.8852\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8867 - val_loss: 0.3048 - val_acc: 0.8850\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8869 - val_loss: 0.3042 - val_acc: 0.8856\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8873 - val_loss: 0.3065 - val_acc: 0.8841\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8865 - val_loss: 0.3055 - val_acc: 0.8861\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8864 - val_loss: 0.3049 - val_acc: 0.8850\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8867 - val_loss: 0.3041 - val_acc: 0.8856\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8860 - val_loss: 0.3040 - val_acc: 0.8850\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8874 - val_loss: 0.3041 - val_acc: 0.8870\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8862 - val_loss: 0.3042 - val_acc: 0.8856\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8869 - val_loss: 0.3037 - val_acc: 0.8854\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8874 - val_loss: 0.3042 - val_acc: 0.8859\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8870 - val_loss: 0.3043 - val_acc: 0.8856\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8867 - val_loss: 0.3045 - val_acc: 0.8861\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8866 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8870 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3054 - acc: 0.8867 - val_loss: 0.3049 - val_acc: 0.8872\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8868 - val_loss: 0.3062 - val_acc: 0.8852\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8870 - val_loss: 0.3056 - val_acc: 0.8848\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8867 - val_loss: 0.3048 - val_acc: 0.8861\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8871 - val_loss: 0.3042 - val_acc: 0.8848\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8867 - val_loss: 0.3059 - val_acc: 0.8832\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8864 - val_loss: 0.3045 - val_acc: 0.8863\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8865 - val_loss: 0.3045 - val_acc: 0.8856\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8865 - val_loss: 0.3046 - val_acc: 0.8848\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8865 - val_loss: 0.3054 - val_acc: 0.8856\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8870 - val_loss: 0.3058 - val_acc: 0.8854\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3050 - val_acc: 0.8859\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8852\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8872 - val_loss: 0.3066 - val_acc: 0.8834\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3052 - acc: 0.8866 - val_loss: 0.3070 - val_acc: 0.8832\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8867 - val_loss: 0.3042 - val_acc: 0.8861\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8866 - val_loss: 0.3044 - val_acc: 0.8850\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8869 - val_loss: 0.3052 - val_acc: 0.8854\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8871 - val_loss: 0.3043 - val_acc: 0.8852\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3059 - val_acc: 0.8850\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8871 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8872 - val_loss: 0.3051 - val_acc: 0.8854\n",
            "acc: 88.54%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 30us/step - loss: 0.3472 - acc: 0.8702 - val_loss: 0.3128 - val_acc: 0.8845\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3139 - acc: 0.8857 - val_loss: 0.3095 - val_acc: 0.8837\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3122 - acc: 0.8863 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3115 - acc: 0.8872 - val_loss: 0.3073 - val_acc: 0.8850\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3109 - acc: 0.8867 - val_loss: 0.3081 - val_acc: 0.8843\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3105 - acc: 0.8869 - val_loss: 0.3068 - val_acc: 0.8861\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3107 - acc: 0.8861 - val_loss: 0.3060 - val_acc: 0.8850\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3104 - acc: 0.8867 - val_loss: 0.3063 - val_acc: 0.8854\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3102 - acc: 0.8867 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3104 - acc: 0.8872 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3097 - acc: 0.8873 - val_loss: 0.3075 - val_acc: 0.8848\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3102 - acc: 0.8870 - val_loss: 0.3049 - val_acc: 0.8843\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3096 - acc: 0.8874 - val_loss: 0.3051 - val_acc: 0.8865\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3095 - acc: 0.8882 - val_loss: 0.3062 - val_acc: 0.8845\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3097 - acc: 0.8869 - val_loss: 0.3062 - val_acc: 0.8852\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3095 - acc: 0.8876 - val_loss: 0.3044 - val_acc: 0.8852\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3092 - acc: 0.8882 - val_loss: 0.3059 - val_acc: 0.8852\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3093 - acc: 0.8878 - val_loss: 0.3044 - val_acc: 0.8859\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3088 - acc: 0.8874 - val_loss: 0.3057 - val_acc: 0.8863\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8873 - val_loss: 0.3045 - val_acc: 0.8859\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3087 - acc: 0.8876 - val_loss: 0.3049 - val_acc: 0.8854\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3083 - acc: 0.8880 - val_loss: 0.3075 - val_acc: 0.8839\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8878 - val_loss: 0.3057 - val_acc: 0.8861\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3081 - acc: 0.8876 - val_loss: 0.3042 - val_acc: 0.8854\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8873 - val_loss: 0.3049 - val_acc: 0.8861\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8876 - val_loss: 0.3034 - val_acc: 0.8865\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8880 - val_loss: 0.3033 - val_acc: 0.8861\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8876 - val_loss: 0.3034 - val_acc: 0.8854\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8877 - val_loss: 0.3031 - val_acc: 0.8868\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8877 - val_loss: 0.3036 - val_acc: 0.8859\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8878 - val_loss: 0.3028 - val_acc: 0.8868\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8873 - val_loss: 0.3024 - val_acc: 0.8868\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3069 - acc: 0.8872 - val_loss: 0.3028 - val_acc: 0.8870\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8875 - val_loss: 0.3033 - val_acc: 0.8865\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8876 - val_loss: 0.3036 - val_acc: 0.8848\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8874 - val_loss: 0.3025 - val_acc: 0.8872\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8878 - val_loss: 0.3035 - val_acc: 0.8870\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3067 - acc: 0.8878 - val_loss: 0.3033 - val_acc: 0.8868\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8876 - val_loss: 0.3038 - val_acc: 0.8859\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3066 - acc: 0.8879 - val_loss: 0.3034 - val_acc: 0.8874\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3065 - acc: 0.8880 - val_loss: 0.3025 - val_acc: 0.8885\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3064 - acc: 0.8877 - val_loss: 0.3028 - val_acc: 0.8865\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3065 - acc: 0.8875 - val_loss: 0.3016 - val_acc: 0.8876\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3062 - acc: 0.8884 - val_loss: 0.3031 - val_acc: 0.8868\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3061 - acc: 0.8877 - val_loss: 0.3021 - val_acc: 0.8874\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8880 - val_loss: 0.3015 - val_acc: 0.8870\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8879 - val_loss: 0.3009 - val_acc: 0.8879\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8876 - val_loss: 0.3027 - val_acc: 0.8861\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8876 - val_loss: 0.3016 - val_acc: 0.8870\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8877 - val_loss: 0.3019 - val_acc: 0.8870\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8881 - val_loss: 0.3011 - val_acc: 0.8874\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8876 - val_loss: 0.3051 - val_acc: 0.8868\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8877 - val_loss: 0.3028 - val_acc: 0.8870\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8879 - val_loss: 0.3019 - val_acc: 0.8872\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8877 - val_loss: 0.3022 - val_acc: 0.8868\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3056 - acc: 0.8878 - val_loss: 0.3015 - val_acc: 0.8881\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8880 - val_loss: 0.3023 - val_acc: 0.8881\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8876 - val_loss: 0.3037 - val_acc: 0.8859\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8873 - val_loss: 0.3033 - val_acc: 0.8854\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3039 - val_acc: 0.8876\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3015 - val_acc: 0.8874\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8875 - val_loss: 0.3036 - val_acc: 0.8865\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8871 - val_loss: 0.3016 - val_acc: 0.8881\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8873 - val_loss: 0.3038 - val_acc: 0.8874\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3020 - val_acc: 0.8890\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3050 - acc: 0.8875 - val_loss: 0.3012 - val_acc: 0.8870\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3050 - acc: 0.8876 - val_loss: 0.3028 - val_acc: 0.8885\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3008 - val_acc: 0.8879\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8875 - val_loss: 0.3023 - val_acc: 0.8879\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8871 - val_loss: 0.3015 - val_acc: 0.8883\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3051 - acc: 0.8876 - val_loss: 0.3035 - val_acc: 0.8874\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8875 - val_loss: 0.3053 - val_acc: 0.8872\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8877 - val_loss: 0.3026 - val_acc: 0.8885\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8879 - val_loss: 0.3016 - val_acc: 0.8887\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8876 - val_loss: 0.3026 - val_acc: 0.8868\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3010 - val_acc: 0.8896\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3029 - val_acc: 0.8861\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8875 - val_loss: 0.3022 - val_acc: 0.8883\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8873 - val_loss: 0.3021 - val_acc: 0.8887\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3008 - val_acc: 0.8887\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8876 - val_loss: 0.3015 - val_acc: 0.8890\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8877 - val_loss: 0.3028 - val_acc: 0.8883\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8874 - val_loss: 0.3021 - val_acc: 0.8885\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8865 - val_loss: 0.3023 - val_acc: 0.8887\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3037 - val_acc: 0.8870\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8873 - val_loss: 0.3017 - val_acc: 0.8885\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3009 - val_acc: 0.8883\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8877 - val_loss: 0.3024 - val_acc: 0.8876\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8874 - val_loss: 0.3025 - val_acc: 0.8896\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8878 - val_loss: 0.3018 - val_acc: 0.8881\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3021 - val_acc: 0.8883\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3018 - val_acc: 0.8885\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8878 - val_loss: 0.3024 - val_acc: 0.8885\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3013 - val_acc: 0.8883\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3026 - val_acc: 0.8859\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3018 - val_acc: 0.8876\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8874 - val_loss: 0.3021 - val_acc: 0.8885\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8874 - val_loss: 0.3010 - val_acc: 0.8887\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8880 - val_loss: 0.3011 - val_acc: 0.8885\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8875 - val_loss: 0.3021 - val_acc: 0.8852\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8878 - val_loss: 0.3038 - val_acc: 0.8865\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8874 - val_loss: 0.3027 - val_acc: 0.8881\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8874 - val_loss: 0.3027 - val_acc: 0.8868\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3044 - acc: 0.8872 - val_loss: 0.3035 - val_acc: 0.8870\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8877 - val_loss: 0.3015 - val_acc: 0.8879\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3045 - acc: 0.8875 - val_loss: 0.3038 - val_acc: 0.8865\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8879 - val_loss: 0.3043 - val_acc: 0.8859\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3020 - val_acc: 0.8894\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3029 - val_acc: 0.8870\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8873 - val_loss: 0.3012 - val_acc: 0.8879\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8877 - val_loss: 0.3018 - val_acc: 0.8876\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8871 - val_loss: 0.3016 - val_acc: 0.8892\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3013 - val_acc: 0.8885\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3015 - val_acc: 0.8885\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8871 - val_loss: 0.3022 - val_acc: 0.8896\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8870 - val_loss: 0.3012 - val_acc: 0.8890\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3028 - val_acc: 0.8868\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8874 - val_loss: 0.3019 - val_acc: 0.8885\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3018 - val_acc: 0.8885\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8876 - val_loss: 0.3019 - val_acc: 0.8887\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3017 - val_acc: 0.8883\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3034 - val_acc: 0.8879\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8877 - val_loss: 0.3026 - val_acc: 0.8887\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8876 - val_loss: 0.3044 - val_acc: 0.8850\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8874 - val_loss: 0.3025 - val_acc: 0.8876\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8878 - val_loss: 0.3026 - val_acc: 0.8887\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8879 - val_loss: 0.3017 - val_acc: 0.8879\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3025 - val_acc: 0.8876\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8873 - val_loss: 0.3024 - val_acc: 0.8892\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8873 - val_loss: 0.3027 - val_acc: 0.8883\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3045 - acc: 0.8875 - val_loss: 0.3012 - val_acc: 0.8876\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3048 - acc: 0.8873 - val_loss: 0.3032 - val_acc: 0.8887\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8877 - val_loss: 0.3018 - val_acc: 0.8865\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8872 - val_loss: 0.3032 - val_acc: 0.8874\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8875 - val_loss: 0.3027 - val_acc: 0.8890\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3014 - val_acc: 0.8890\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8875 - val_loss: 0.3038 - val_acc: 0.8876\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8873 - val_loss: 0.3021 - val_acc: 0.8881\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8866 - val_loss: 0.3025 - val_acc: 0.8881\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8873 - val_loss: 0.3026 - val_acc: 0.8876\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8874 - val_loss: 0.3030 - val_acc: 0.8870\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3021 - val_acc: 0.8881\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8873 - val_loss: 0.3016 - val_acc: 0.8885\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8872 - val_loss: 0.3026 - val_acc: 0.8883\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8878 - val_loss: 0.3016 - val_acc: 0.8890\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8878 - val_loss: 0.3022 - val_acc: 0.8885\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8876 - val_loss: 0.3016 - val_acc: 0.8894\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8873 - val_loss: 0.3023 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8870 - val_loss: 0.3013 - val_acc: 0.8887\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8871 - val_loss: 0.3014 - val_acc: 0.8901\n",
            "acc: 89.01%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 32us/step - loss: 0.3414 - acc: 0.8748 - val_loss: 0.3104 - val_acc: 0.8872\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3132 - acc: 0.8856 - val_loss: 0.3075 - val_acc: 0.8850\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3101 - acc: 0.8870 - val_loss: 0.3073 - val_acc: 0.8845\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3088 - acc: 0.8863 - val_loss: 0.3089 - val_acc: 0.8832\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3088 - acc: 0.8866 - val_loss: 0.3066 - val_acc: 0.8856\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3082 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8852\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.3102 - val_acc: 0.8861\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8872 - val_loss: 0.3071 - val_acc: 0.8854\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.3065 - val_acc: 0.8845\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8874 - val_loss: 0.3068 - val_acc: 0.8843\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8874 - val_loss: 0.3075 - val_acc: 0.8819\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8869 - val_loss: 0.3045 - val_acc: 0.8850\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8872 - val_loss: 0.3050 - val_acc: 0.8859\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8871 - val_loss: 0.3069 - val_acc: 0.8876\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8863 - val_loss: 0.3050 - val_acc: 0.8868\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8867 - val_loss: 0.3045 - val_acc: 0.8881\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8868 - val_loss: 0.3047 - val_acc: 0.8808\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8864 - val_loss: 0.3048 - val_acc: 0.8850\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8872 - val_loss: 0.3045 - val_acc: 0.8868\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8864 - val_loss: 0.3031 - val_acc: 0.8861\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8876 - val_loss: 0.3022 - val_acc: 0.8879\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8874 - val_loss: 0.3018 - val_acc: 0.8856\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8877 - val_loss: 0.3026 - val_acc: 0.8863\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8881 - val_loss: 0.3025 - val_acc: 0.8848\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8881 - val_loss: 0.3027 - val_acc: 0.8874\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8880 - val_loss: 0.3026 - val_acc: 0.8845\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3044 - acc: 0.8875 - val_loss: 0.3021 - val_acc: 0.8870\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8881 - val_loss: 0.3023 - val_acc: 0.8856\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8878 - val_loss: 0.3013 - val_acc: 0.8874\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3040 - acc: 0.8879 - val_loss: 0.3070 - val_acc: 0.8856\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8879 - val_loss: 0.3011 - val_acc: 0.8863\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3040 - acc: 0.8884 - val_loss: 0.3017 - val_acc: 0.8870\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3040 - acc: 0.8887 - val_loss: 0.3042 - val_acc: 0.8852\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3038 - acc: 0.8881 - val_loss: 0.3010 - val_acc: 0.8870\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8879 - val_loss: 0.3030 - val_acc: 0.8854\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8876 - val_loss: 0.3019 - val_acc: 0.8859\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3038 - acc: 0.8885 - val_loss: 0.3005 - val_acc: 0.8859\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8883 - val_loss: 0.3035 - val_acc: 0.8850\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8879 - val_loss: 0.2999 - val_acc: 0.8865\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3037 - acc: 0.8883 - val_loss: 0.3011 - val_acc: 0.8854\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8879 - val_loss: 0.3012 - val_acc: 0.8885\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8882 - val_loss: 0.3016 - val_acc: 0.8865\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3035 - acc: 0.8878 - val_loss: 0.3017 - val_acc: 0.8870\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8875 - val_loss: 0.3003 - val_acc: 0.8879\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8879 - val_loss: 0.3007 - val_acc: 0.8852\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8886 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3035 - acc: 0.8882 - val_loss: 0.3008 - val_acc: 0.8859\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8884 - val_loss: 0.3013 - val_acc: 0.8870\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3003 - val_acc: 0.8868\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8883 - val_loss: 0.3018 - val_acc: 0.8874\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8877 - val_loss: 0.3002 - val_acc: 0.8879\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3030 - acc: 0.8881 - val_loss: 0.3008 - val_acc: 0.8874\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3030 - acc: 0.8886 - val_loss: 0.3009 - val_acc: 0.8814\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8879 - val_loss: 0.3015 - val_acc: 0.8865\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3009 - val_acc: 0.8850\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8880 - val_loss: 0.2993 - val_acc: 0.8872\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8882 - val_loss: 0.3008 - val_acc: 0.8865\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3002 - val_acc: 0.8883\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3024 - acc: 0.8883 - val_loss: 0.3035 - val_acc: 0.8885\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3025 - acc: 0.8881 - val_loss: 0.3021 - val_acc: 0.8856\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8873 - val_loss: 0.3011 - val_acc: 0.8861\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3009 - val_acc: 0.8832\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8891 - val_loss: 0.3001 - val_acc: 0.8852\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8880 - val_loss: 0.3000 - val_acc: 0.8879\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8880 - val_loss: 0.2998 - val_acc: 0.8876\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3027 - acc: 0.8880 - val_loss: 0.3002 - val_acc: 0.8863\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3021 - val_acc: 0.8874\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8883 - val_loss: 0.3013 - val_acc: 0.8874\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3026 - acc: 0.8885 - val_loss: 0.3009 - val_acc: 0.8874\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3026 - acc: 0.8881 - val_loss: 0.3014 - val_acc: 0.8881\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3024 - acc: 0.8884 - val_loss: 0.3006 - val_acc: 0.8868\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3024 - acc: 0.8879 - val_loss: 0.3020 - val_acc: 0.8872\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8876 - val_loss: 0.3016 - val_acc: 0.8870\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3025 - acc: 0.8880 - val_loss: 0.3014 - val_acc: 0.8885\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3025 - acc: 0.8882 - val_loss: 0.3007 - val_acc: 0.8887\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8889 - val_loss: 0.3010 - val_acc: 0.8870\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8882 - val_loss: 0.3007 - val_acc: 0.8859\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8886 - val_loss: 0.3013 - val_acc: 0.8865\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3023 - acc: 0.8881 - val_loss: 0.3007 - val_acc: 0.8876\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3024 - acc: 0.8879 - val_loss: 0.3007 - val_acc: 0.8896\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8888 - val_loss: 0.3012 - val_acc: 0.8859\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3024 - acc: 0.8881 - val_loss: 0.3027 - val_acc: 0.8881\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8883 - val_loss: 0.3023 - val_acc: 0.8870\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8885 - val_loss: 0.3017 - val_acc: 0.8879\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3019 - acc: 0.8882 - val_loss: 0.3031 - val_acc: 0.8870\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3008 - val_acc: 0.8865\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3022 - acc: 0.8884 - val_loss: 0.3004 - val_acc: 0.8872\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3006 - val_acc: 0.8870\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3022 - acc: 0.8883 - val_loss: 0.3011 - val_acc: 0.8881\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8884 - val_loss: 0.3009 - val_acc: 0.8879\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8880 - val_loss: 0.3002 - val_acc: 0.8876\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8886 - val_loss: 0.3015 - val_acc: 0.8863\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8883 - val_loss: 0.3008 - val_acc: 0.8874\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8884 - val_loss: 0.3021 - val_acc: 0.8881\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3015 - val_acc: 0.8885\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3023 - acc: 0.8879 - val_loss: 0.3019 - val_acc: 0.8870\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3022 - acc: 0.8878 - val_loss: 0.3022 - val_acc: 0.8868\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8879 - val_loss: 0.3017 - val_acc: 0.8876\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8884 - val_loss: 0.3021 - val_acc: 0.8883\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8881 - val_loss: 0.3016 - val_acc: 0.8885\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8877 - val_loss: 0.3012 - val_acc: 0.8883\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8882 - val_loss: 0.3012 - val_acc: 0.8892\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3020 - acc: 0.8884 - val_loss: 0.3009 - val_acc: 0.8887\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3020 - acc: 0.8882 - val_loss: 0.3010 - val_acc: 0.8883\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3020 - acc: 0.8881 - val_loss: 0.3006 - val_acc: 0.8885\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8878 - val_loss: 0.3023 - val_acc: 0.8883\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8883 - val_loss: 0.3025 - val_acc: 0.8863\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3019 - acc: 0.8881 - val_loss: 0.3019 - val_acc: 0.8881\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8888 - val_loss: 0.3027 - val_acc: 0.8848\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8881 - val_loss: 0.3013 - val_acc: 0.8872\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8880 - val_loss: 0.3011 - val_acc: 0.8876\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3014 - val_acc: 0.8879\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3019 - acc: 0.8884 - val_loss: 0.3017 - val_acc: 0.8879\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8888 - val_loss: 0.3009 - val_acc: 0.8876\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8884 - val_loss: 0.3005 - val_acc: 0.8887\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3033 - val_acc: 0.8863\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8884 - val_loss: 0.3012 - val_acc: 0.8874\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3016 - val_acc: 0.8879\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8886 - val_loss: 0.3027 - val_acc: 0.8887\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8884 - val_loss: 0.3024 - val_acc: 0.8874\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3019 - acc: 0.8883 - val_loss: 0.3037 - val_acc: 0.8863\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3013 - val_acc: 0.8883\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8878 - val_loss: 0.3023 - val_acc: 0.8870\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8882 - val_loss: 0.3011 - val_acc: 0.8872\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8880 - val_loss: 0.3019 - val_acc: 0.8883\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8883 - val_loss: 0.3025 - val_acc: 0.8861\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3017 - acc: 0.8886 - val_loss: 0.3026 - val_acc: 0.8881\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8883 - val_loss: 0.3013 - val_acc: 0.8890\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8887 - val_loss: 0.3026 - val_acc: 0.8876\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3019 - acc: 0.8885 - val_loss: 0.3027 - val_acc: 0.8863\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8876 - val_loss: 0.3024 - val_acc: 0.8872\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3007 - val_acc: 0.8879\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8885 - val_loss: 0.3011 - val_acc: 0.8879\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8880 - val_loss: 0.3025 - val_acc: 0.8872\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8887 - val_loss: 0.3015 - val_acc: 0.8876\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8880 - val_loss: 0.3015 - val_acc: 0.8874\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8885 - val_loss: 0.3017 - val_acc: 0.8870\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8880 - val_loss: 0.3018 - val_acc: 0.8870\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8881 - val_loss: 0.3016 - val_acc: 0.8868\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8881 - val_loss: 0.3020 - val_acc: 0.8881\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8880 - val_loss: 0.2997 - val_acc: 0.8887\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8889 - val_loss: 0.3025 - val_acc: 0.8876\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3019 - acc: 0.8884 - val_loss: 0.3020 - val_acc: 0.8861\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3019 - acc: 0.8879 - val_loss: 0.3034 - val_acc: 0.8863\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3025 - val_acc: 0.8861\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3017 - acc: 0.8884 - val_loss: 0.3046 - val_acc: 0.8885\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8887 - val_loss: 0.3024 - val_acc: 0.8890\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8881 - val_loss: 0.3013 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3015 - acc: 0.8881 - val_loss: 0.3014 - val_acc: 0.8890\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8877 - val_loss: 0.3013 - val_acc: 0.8874\n",
            "acc: 88.74%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 34us/step - loss: 0.3421 - acc: 0.8717 - val_loss: 0.3224 - val_acc: 0.8856\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3124 - acc: 0.8848 - val_loss: 0.3146 - val_acc: 0.8856\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3099 - acc: 0.8857 - val_loss: 0.3132 - val_acc: 0.8874\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8855 - val_loss: 0.3122 - val_acc: 0.8848\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8851 - val_loss: 0.3110 - val_acc: 0.8828\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8856 - val_loss: 0.3087 - val_acc: 0.8841\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8846 - val_loss: 0.3093 - val_acc: 0.8859\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8849 - val_loss: 0.3080 - val_acc: 0.8834\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3031 - acc: 0.8851 - val_loss: 0.3095 - val_acc: 0.8856\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3031 - acc: 0.8851 - val_loss: 0.3066 - val_acc: 0.8865\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3028 - acc: 0.8853 - val_loss: 0.3077 - val_acc: 0.8879\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3023 - acc: 0.8846 - val_loss: 0.3104 - val_acc: 0.8850\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3023 - acc: 0.8846 - val_loss: 0.3057 - val_acc: 0.8856\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3015 - acc: 0.8853 - val_loss: 0.3069 - val_acc: 0.8859\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3012 - acc: 0.8853 - val_loss: 0.3073 - val_acc: 0.8868\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3016 - acc: 0.8846 - val_loss: 0.3052 - val_acc: 0.8876\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3013 - acc: 0.8847 - val_loss: 0.3060 - val_acc: 0.8892\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3009 - acc: 0.8853 - val_loss: 0.3076 - val_acc: 0.8852\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3007 - acc: 0.8845 - val_loss: 0.3062 - val_acc: 0.8861\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3009 - acc: 0.8844 - val_loss: 0.3068 - val_acc: 0.8865\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3007 - acc: 0.8844 - val_loss: 0.3071 - val_acc: 0.8894\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3006 - acc: 0.8853 - val_loss: 0.3065 - val_acc: 0.8845\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3005 - acc: 0.8846 - val_loss: 0.3079 - val_acc: 0.8870\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3007 - acc: 0.8852 - val_loss: 0.3061 - val_acc: 0.8870\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3003 - acc: 0.8851 - val_loss: 0.3050 - val_acc: 0.8863\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8853 - val_loss: 0.3060 - val_acc: 0.8890\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3004 - acc: 0.8851 - val_loss: 0.3078 - val_acc: 0.8885\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3004 - acc: 0.8849 - val_loss: 0.3072 - val_acc: 0.8850\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8845 - val_loss: 0.3061 - val_acc: 0.8843\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3005 - acc: 0.8848 - val_loss: 0.3051 - val_acc: 0.8854\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3001 - acc: 0.8839 - val_loss: 0.3063 - val_acc: 0.8885\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8848 - val_loss: 0.3055 - val_acc: 0.8865\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3003 - acc: 0.8843 - val_loss: 0.3066 - val_acc: 0.8868\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3000 - acc: 0.8853 - val_loss: 0.3058 - val_acc: 0.8870\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3002 - acc: 0.8841 - val_loss: 0.3074 - val_acc: 0.8863\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3000 - acc: 0.8843 - val_loss: 0.3053 - val_acc: 0.8892\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8850 - val_loss: 0.3053 - val_acc: 0.8868\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2998 - acc: 0.8846 - val_loss: 0.3045 - val_acc: 0.8872\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2998 - acc: 0.8848 - val_loss: 0.3062 - val_acc: 0.8850\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2999 - acc: 0.8856 - val_loss: 0.3055 - val_acc: 0.8865\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2998 - acc: 0.8852 - val_loss: 0.3053 - val_acc: 0.8852\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8853 - val_loss: 0.3076 - val_acc: 0.8881\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2998 - acc: 0.8852 - val_loss: 0.3060 - val_acc: 0.8894\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2995 - acc: 0.8851 - val_loss: 0.3047 - val_acc: 0.8868\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2996 - acc: 0.8852 - val_loss: 0.3062 - val_acc: 0.8881\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8847 - val_loss: 0.3060 - val_acc: 0.8856\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8847 - val_loss: 0.3046 - val_acc: 0.8865\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8850 - val_loss: 0.3071 - val_acc: 0.8896\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8849 - val_loss: 0.3063 - val_acc: 0.8837\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2995 - acc: 0.8855 - val_loss: 0.3067 - val_acc: 0.8850\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2994 - acc: 0.8854 - val_loss: 0.3070 - val_acc: 0.8890\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2995 - acc: 0.8852 - val_loss: 0.3064 - val_acc: 0.8854\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2995 - acc: 0.8854 - val_loss: 0.3064 - val_acc: 0.8876\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2994 - acc: 0.8851 - val_loss: 0.3052 - val_acc: 0.8887\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8848 - val_loss: 0.3068 - val_acc: 0.8859\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8858 - val_loss: 0.3055 - val_acc: 0.8852\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2991 - acc: 0.8857 - val_loss: 0.3066 - val_acc: 0.8885\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2994 - acc: 0.8855 - val_loss: 0.3060 - val_acc: 0.8868\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8846 - val_loss: 0.3042 - val_acc: 0.8859\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8859 - val_loss: 0.3055 - val_acc: 0.8874\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2990 - acc: 0.8856 - val_loss: 0.3067 - val_acc: 0.8863\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2991 - acc: 0.8853 - val_loss: 0.3064 - val_acc: 0.8872\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.2995 - acc: 0.8852 - val_loss: 0.3052 - val_acc: 0.8861\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8852 - val_loss: 0.3042 - val_acc: 0.8870\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8852 - val_loss: 0.3051 - val_acc: 0.8879\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2991 - acc: 0.8847 - val_loss: 0.3050 - val_acc: 0.8850\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8852 - val_loss: 0.3074 - val_acc: 0.8868\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8852 - val_loss: 0.3054 - val_acc: 0.8852\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2989 - acc: 0.8861 - val_loss: 0.3063 - val_acc: 0.8881\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2990 - acc: 0.8850 - val_loss: 0.3075 - val_acc: 0.8856\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8849 - val_loss: 0.3056 - val_acc: 0.8870\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8854 - val_loss: 0.3076 - val_acc: 0.8870\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2990 - acc: 0.8853 - val_loss: 0.3057 - val_acc: 0.8870\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2987 - acc: 0.8860 - val_loss: 0.3066 - val_acc: 0.8859\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3064 - val_acc: 0.8861\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2989 - acc: 0.8854 - val_loss: 0.3061 - val_acc: 0.8859\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8856 - val_loss: 0.3062 - val_acc: 0.8863\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2993 - acc: 0.8857 - val_loss: 0.3072 - val_acc: 0.8865\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2989 - acc: 0.8851 - val_loss: 0.3048 - val_acc: 0.8854\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2987 - acc: 0.8853 - val_loss: 0.3075 - val_acc: 0.8854\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2988 - acc: 0.8853 - val_loss: 0.3076 - val_acc: 0.8863\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8846 - val_loss: 0.3071 - val_acc: 0.8865\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2986 - acc: 0.8857 - val_loss: 0.3055 - val_acc: 0.8881\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8843 - val_loss: 0.3054 - val_acc: 0.8874\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3055 - val_acc: 0.8861\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2990 - acc: 0.8861 - val_loss: 0.3072 - val_acc: 0.8850\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2990 - acc: 0.8855 - val_loss: 0.3056 - val_acc: 0.8845\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8854 - val_loss: 0.3074 - val_acc: 0.8868\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2988 - acc: 0.8852 - val_loss: 0.3067 - val_acc: 0.8874\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8853 - val_loss: 0.3065 - val_acc: 0.8854\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8853 - val_loss: 0.3073 - val_acc: 0.8839\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2990 - acc: 0.8841 - val_loss: 0.3060 - val_acc: 0.8883\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8852 - val_loss: 0.3071 - val_acc: 0.8881\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2988 - acc: 0.8858 - val_loss: 0.3065 - val_acc: 0.8852\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8856 - val_loss: 0.3063 - val_acc: 0.8848\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2988 - acc: 0.8849 - val_loss: 0.3062 - val_acc: 0.8865\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3050 - val_acc: 0.8872\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3048 - val_acc: 0.8892\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2987 - acc: 0.8857 - val_loss: 0.3067 - val_acc: 0.8874\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3053 - val_acc: 0.8868\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8855 - val_loss: 0.3066 - val_acc: 0.8845\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2987 - acc: 0.8847 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8856 - val_loss: 0.3050 - val_acc: 0.8870\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3056 - val_acc: 0.8865\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8853 - val_loss: 0.3061 - val_acc: 0.8881\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2983 - acc: 0.8851 - val_loss: 0.3057 - val_acc: 0.8865\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2990 - acc: 0.8846 - val_loss: 0.3062 - val_acc: 0.8870\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2986 - acc: 0.8846 - val_loss: 0.3069 - val_acc: 0.8881\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8851 - val_loss: 0.3067 - val_acc: 0.8861\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8853 - val_loss: 0.3048 - val_acc: 0.8852\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8849 - val_loss: 0.3057 - val_acc: 0.8863\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8857 - val_loss: 0.3053 - val_acc: 0.8870\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8849 - val_loss: 0.3045 - val_acc: 0.8852\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3071 - val_acc: 0.8876\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2983 - acc: 0.8854 - val_loss: 0.3069 - val_acc: 0.8863\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8855 - val_loss: 0.3045 - val_acc: 0.8863\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8847 - val_loss: 0.3054 - val_acc: 0.8876\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8848 - val_loss: 0.3055 - val_acc: 0.8885\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8854 - val_loss: 0.3059 - val_acc: 0.8859\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3068 - val_acc: 0.8874\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8846 - val_loss: 0.3053 - val_acc: 0.8861\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2986 - acc: 0.8852 - val_loss: 0.3064 - val_acc: 0.8868\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2986 - acc: 0.8853 - val_loss: 0.3067 - val_acc: 0.8881\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8851 - val_loss: 0.3064 - val_acc: 0.8859\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8855 - val_loss: 0.3051 - val_acc: 0.8870\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8852 - val_loss: 0.3053 - val_acc: 0.8870\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.2983 - acc: 0.8859 - val_loss: 0.3052 - val_acc: 0.8872\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8849 - val_loss: 0.3054 - val_acc: 0.8863\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8852 - val_loss: 0.3053 - val_acc: 0.8872\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3078 - val_acc: 0.8876\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8843 - val_loss: 0.3070 - val_acc: 0.8859\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8855 - val_loss: 0.3056 - val_acc: 0.8861\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8853 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2982 - acc: 0.8857 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2986 - acc: 0.8859 - val_loss: 0.3056 - val_acc: 0.8874\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8844 - val_loss: 0.3058 - val_acc: 0.8887\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8851 - val_loss: 0.3062 - val_acc: 0.8859\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8847 - val_loss: 0.3055 - val_acc: 0.8872\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8851 - val_loss: 0.3052 - val_acc: 0.8863\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2984 - acc: 0.8845 - val_loss: 0.3059 - val_acc: 0.8879\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8853 - val_loss: 0.3052 - val_acc: 0.8865\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2982 - acc: 0.8850 - val_loss: 0.3065 - val_acc: 0.8861\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8848 - val_loss: 0.3045 - val_acc: 0.8865\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2986 - acc: 0.8846 - val_loss: 0.3037 - val_acc: 0.8856\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8846 - val_loss: 0.3045 - val_acc: 0.8885\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8849 - val_loss: 0.3046 - val_acc: 0.8856\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8855 - val_loss: 0.3050 - val_acc: 0.8854\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3052 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8850 - val_loss: 0.3060 - val_acc: 0.8868\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8842 - val_loss: 0.3044 - val_acc: 0.8856\n",
            "acc: 88.56%\n",
            "88.70% (+/- 0.19%)\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   68    460        yes\n",
            "   50    3941        no\n",
            "Accuracy: 0.886974408882794\n",
            "Sensitivity: 0.13008130081300814\n",
            "Specificity: 0.9872501377686489\n",
            "Precision: 0.5747702589807854\n",
            "f_score: 0.21214924452667286\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pepM7OrAnFCl",
        "colab_type": "code",
        "outputId": "5b544540-596f-493c-cc6a-8cb9dce1a1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fpr_nn_p,tpr_nn_p ,thresholds_nn_p,auc_nn_p=percentage_split_NN_SMOTE(0.25,X,Y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 59882 samples, validate on 11303 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "59882/59882 [==============================] - 2s 27us/step - loss: 0.3586 - acc: 0.8441 - val_loss: 0.3683 - val_acc: 0.8314\n",
            "Epoch 2/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.3131 - acc: 0.8710 - val_loss: 0.3732 - val_acc: 0.8259\n",
            "Epoch 3/150\n",
            "59882/59882 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8757 - val_loss: 0.3784 - val_acc: 0.8245\n",
            "Epoch 4/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8783 - val_loss: 0.3580 - val_acc: 0.8356\n",
            "Epoch 5/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2955 - acc: 0.8802 - val_loss: 0.3861 - val_acc: 0.8233\n",
            "Epoch 6/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2936 - acc: 0.8804 - val_loss: 0.3466 - val_acc: 0.8384\n",
            "Epoch 7/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2924 - acc: 0.8813 - val_loss: 0.3425 - val_acc: 0.8424\n",
            "Epoch 8/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2916 - acc: 0.8812 - val_loss: 0.3442 - val_acc: 0.8380\n",
            "Epoch 9/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8821 - val_loss: 0.3590 - val_acc: 0.8356\n",
            "Epoch 10/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8815 - val_loss: 0.3659 - val_acc: 0.8316\n",
            "Epoch 11/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8814 - val_loss: 0.3213 - val_acc: 0.8464\n",
            "Epoch 12/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8810 - val_loss: 0.3519 - val_acc: 0.8320\n",
            "Epoch 13/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8813 - val_loss: 0.3308 - val_acc: 0.8459\n",
            "Epoch 14/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2900 - acc: 0.8805 - val_loss: 0.3317 - val_acc: 0.8411\n",
            "Epoch 15/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8816 - val_loss: 0.3448 - val_acc: 0.8407\n",
            "Epoch 16/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2895 - acc: 0.8824 - val_loss: 0.3496 - val_acc: 0.8356\n",
            "Epoch 17/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8819 - val_loss: 0.3260 - val_acc: 0.8492\n",
            "Epoch 18/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2898 - acc: 0.8810 - val_loss: 0.3395 - val_acc: 0.8383\n",
            "Epoch 19/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8818 - val_loss: 0.3424 - val_acc: 0.8317\n",
            "Epoch 20/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8817 - val_loss: 0.3395 - val_acc: 0.8376\n",
            "Epoch 21/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2888 - acc: 0.8815 - val_loss: 0.3492 - val_acc: 0.8410\n",
            "Epoch 22/150\n",
            "59882/59882 [==============================] - 1s 19us/step - loss: 0.2887 - acc: 0.8822 - val_loss: 0.3299 - val_acc: 0.8465\n",
            "Epoch 23/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8817 - val_loss: 0.3594 - val_acc: 0.8257\n",
            "Epoch 24/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8815 - val_loss: 0.3299 - val_acc: 0.8460\n",
            "Epoch 25/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8822 - val_loss: 0.3498 - val_acc: 0.8283\n",
            "Epoch 26/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8818 - val_loss: 0.3503 - val_acc: 0.8265\n",
            "Epoch 27/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8826 - val_loss: 0.3449 - val_acc: 0.8342\n",
            "Epoch 28/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8817 - val_loss: 0.3716 - val_acc: 0.8249\n",
            "Epoch 29/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8829 - val_loss: 0.3530 - val_acc: 0.8277\n",
            "Epoch 30/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8818 - val_loss: 0.3394 - val_acc: 0.8405\n",
            "Epoch 31/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8815 - val_loss: 0.3413 - val_acc: 0.8320\n",
            "Epoch 32/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8823 - val_loss: 0.3463 - val_acc: 0.8321\n",
            "Epoch 33/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8817 - val_loss: 0.3359 - val_acc: 0.8439\n",
            "Epoch 34/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8820 - val_loss: 0.3564 - val_acc: 0.8223\n",
            "Epoch 35/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8819 - val_loss: 0.3357 - val_acc: 0.8404\n",
            "Epoch 36/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8813 - val_loss: 0.3355 - val_acc: 0.8384\n",
            "Epoch 37/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8813 - val_loss: 0.3611 - val_acc: 0.8332\n",
            "Epoch 38/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8826 - val_loss: 0.3218 - val_acc: 0.8429\n",
            "Epoch 39/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2879 - acc: 0.8822 - val_loss: 0.3341 - val_acc: 0.8333\n",
            "Epoch 40/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8817 - val_loss: 0.3383 - val_acc: 0.8413\n",
            "Epoch 41/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8815 - val_loss: 0.3601 - val_acc: 0.8230\n",
            "Epoch 42/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8815 - val_loss: 0.3380 - val_acc: 0.8306\n",
            "Epoch 43/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8815 - val_loss: 0.3430 - val_acc: 0.8393\n",
            "Epoch 44/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8823 - val_loss: 0.3746 - val_acc: 0.8179\n",
            "Epoch 45/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8811 - val_loss: 0.3439 - val_acc: 0.8345\n",
            "Epoch 46/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8817 - val_loss: 0.3160 - val_acc: 0.8505\n",
            "Epoch 47/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8817 - val_loss: 0.3322 - val_acc: 0.8353\n",
            "Epoch 48/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8820 - val_loss: 0.3556 - val_acc: 0.8340\n",
            "Epoch 49/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8816 - val_loss: 0.3348 - val_acc: 0.8379\n",
            "Epoch 50/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8828 - val_loss: 0.3304 - val_acc: 0.8431\n",
            "Epoch 51/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8813 - val_loss: 0.3420 - val_acc: 0.8379\n",
            "Epoch 52/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8818 - val_loss: 0.3226 - val_acc: 0.8458\n",
            "Epoch 53/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8818 - val_loss: 0.3342 - val_acc: 0.8433\n",
            "Epoch 54/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8825 - val_loss: 0.3477 - val_acc: 0.8321\n",
            "Epoch 55/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8820 - val_loss: 0.3510 - val_acc: 0.8257\n",
            "Epoch 56/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8814 - val_loss: 0.3422 - val_acc: 0.8404\n",
            "Epoch 57/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8829 - val_loss: 0.3428 - val_acc: 0.8358\n",
            "Epoch 58/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8827 - val_loss: 0.3465 - val_acc: 0.8350\n",
            "Epoch 59/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8826 - val_loss: 0.3269 - val_acc: 0.8461\n",
            "Epoch 60/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8827 - val_loss: 0.3485 - val_acc: 0.8300\n",
            "Epoch 61/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8811 - val_loss: 0.3291 - val_acc: 0.8440\n",
            "Epoch 62/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8816 - val_loss: 0.3438 - val_acc: 0.8365\n",
            "Epoch 63/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8826 - val_loss: 0.3305 - val_acc: 0.8355\n",
            "Epoch 64/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8824 - val_loss: 0.3571 - val_acc: 0.8299\n",
            "Epoch 65/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8810 - val_loss: 0.3502 - val_acc: 0.8325\n",
            "Epoch 66/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8835 - val_loss: 0.3566 - val_acc: 0.8262\n",
            "Epoch 67/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8822 - val_loss: 0.3513 - val_acc: 0.8273\n",
            "Epoch 68/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8822 - val_loss: 0.3654 - val_acc: 0.8193\n",
            "Epoch 69/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8812 - val_loss: 0.3445 - val_acc: 0.8298\n",
            "Epoch 70/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8826 - val_loss: 0.3627 - val_acc: 0.8246\n",
            "Epoch 71/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8823 - val_loss: 0.3471 - val_acc: 0.8340\n",
            "Epoch 72/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8826 - val_loss: 0.3663 - val_acc: 0.8282\n",
            "Epoch 73/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8813 - val_loss: 0.3442 - val_acc: 0.8379\n",
            "Epoch 74/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8829 - val_loss: 0.3578 - val_acc: 0.8261\n",
            "Epoch 75/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8813 - val_loss: 0.3536 - val_acc: 0.8362\n",
            "Epoch 76/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8821 - val_loss: 0.3392 - val_acc: 0.8356\n",
            "Epoch 77/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8838 - val_loss: 0.3391 - val_acc: 0.8343\n",
            "Epoch 78/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8822 - val_loss: 0.3538 - val_acc: 0.8246\n",
            "Epoch 79/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8819 - val_loss: 0.3272 - val_acc: 0.8442\n",
            "Epoch 80/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8821 - val_loss: 0.3789 - val_acc: 0.8147\n",
            "Epoch 81/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8817 - val_loss: 0.3383 - val_acc: 0.8400\n",
            "Epoch 82/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8820 - val_loss: 0.3278 - val_acc: 0.8421\n",
            "Epoch 83/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8811 - val_loss: 0.3472 - val_acc: 0.8242\n",
            "Epoch 84/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8822 - val_loss: 0.3489 - val_acc: 0.8318\n",
            "Epoch 85/150\n",
            "59882/59882 [==============================] - 1s 15us/step - loss: 0.2872 - acc: 0.8815 - val_loss: 0.3322 - val_acc: 0.8408\n",
            "Epoch 86/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8809 - val_loss: 0.3575 - val_acc: 0.8215\n",
            "Epoch 87/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8824 - val_loss: 0.3317 - val_acc: 0.8394\n",
            "Epoch 88/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8819 - val_loss: 0.3490 - val_acc: 0.8283\n",
            "Epoch 89/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8823 - val_loss: 0.3463 - val_acc: 0.8385\n",
            "Epoch 90/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8830 - val_loss: 0.3294 - val_acc: 0.8413\n",
            "Epoch 91/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8829 - val_loss: 0.3619 - val_acc: 0.8347\n",
            "Epoch 92/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8823 - val_loss: 0.3388 - val_acc: 0.8359\n",
            "Epoch 93/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8815 - val_loss: 0.3576 - val_acc: 0.8352\n",
            "Epoch 94/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8824 - val_loss: 0.3233 - val_acc: 0.8471\n",
            "Epoch 95/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8823 - val_loss: 0.3601 - val_acc: 0.8181\n",
            "Epoch 96/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8818 - val_loss: 0.3332 - val_acc: 0.8438\n",
            "Epoch 97/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8829 - val_loss: 0.3410 - val_acc: 0.8404\n",
            "Epoch 98/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8822 - val_loss: 0.3418 - val_acc: 0.8315\n",
            "Epoch 99/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8827 - val_loss: 0.3440 - val_acc: 0.8290\n",
            "Epoch 100/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8816 - val_loss: 0.3494 - val_acc: 0.8346\n",
            "Epoch 101/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8816 - val_loss: 0.3412 - val_acc: 0.8301\n",
            "Epoch 102/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2869 - acc: 0.8820 - val_loss: 0.3502 - val_acc: 0.8338\n",
            "Epoch 103/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8821 - val_loss: 0.3415 - val_acc: 0.8304\n",
            "Epoch 104/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8823 - val_loss: 0.3523 - val_acc: 0.8276\n",
            "Epoch 105/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8826 - val_loss: 0.3457 - val_acc: 0.8277\n",
            "Epoch 106/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8820 - val_loss: 0.3292 - val_acc: 0.8453\n",
            "Epoch 107/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8822 - val_loss: 0.3635 - val_acc: 0.8227\n",
            "Epoch 108/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8832 - val_loss: 0.3496 - val_acc: 0.8316\n",
            "Epoch 109/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8820 - val_loss: 0.3572 - val_acc: 0.8353\n",
            "Epoch 110/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2869 - acc: 0.8820 - val_loss: 0.3450 - val_acc: 0.8318\n",
            "Epoch 111/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2874 - acc: 0.8827 - val_loss: 0.3510 - val_acc: 0.8293\n",
            "Epoch 112/150\n",
            "59882/59882 [==============================] - 1s 19us/step - loss: 0.2875 - acc: 0.8826 - val_loss: 0.3319 - val_acc: 0.8421\n",
            "Epoch 113/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2872 - acc: 0.8828 - val_loss: 0.3581 - val_acc: 0.8215\n",
            "Epoch 114/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8821 - val_loss: 0.3518 - val_acc: 0.8262\n",
            "Epoch 115/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2876 - acc: 0.8815 - val_loss: 0.3529 - val_acc: 0.8335\n",
            "Epoch 116/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8826 - val_loss: 0.3383 - val_acc: 0.8301\n",
            "Epoch 117/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8821 - val_loss: 0.3715 - val_acc: 0.8315\n",
            "Epoch 118/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8815 - val_loss: 0.3398 - val_acc: 0.8334\n",
            "Epoch 119/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8821 - val_loss: 0.3640 - val_acc: 0.8234\n",
            "Epoch 120/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8823 - val_loss: 0.3662 - val_acc: 0.8208\n",
            "Epoch 121/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8815 - val_loss: 0.3522 - val_acc: 0.8384\n",
            "Epoch 122/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8829 - val_loss: 0.3384 - val_acc: 0.8383\n",
            "Epoch 123/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8826 - val_loss: 0.3418 - val_acc: 0.8400\n",
            "Epoch 124/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8829 - val_loss: 0.3487 - val_acc: 0.8288\n",
            "Epoch 125/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8823 - val_loss: 0.3299 - val_acc: 0.8400\n",
            "Epoch 126/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8815 - val_loss: 0.3346 - val_acc: 0.8381\n",
            "Epoch 127/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8825 - val_loss: 0.3471 - val_acc: 0.8340\n",
            "Epoch 128/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8832 - val_loss: 0.3362 - val_acc: 0.8413\n",
            "Epoch 129/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8822 - val_loss: 0.3593 - val_acc: 0.8329\n",
            "Epoch 130/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2867 - acc: 0.8825 - val_loss: 0.3348 - val_acc: 0.8421\n",
            "Epoch 131/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8827 - val_loss: 0.3447 - val_acc: 0.8382\n",
            "Epoch 132/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2860 - acc: 0.8830 - val_loss: 0.3359 - val_acc: 0.8435\n",
            "Epoch 133/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8826 - val_loss: 0.3416 - val_acc: 0.8271\n",
            "Epoch 134/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8832 - val_loss: 0.3425 - val_acc: 0.8397\n",
            "Epoch 135/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8820 - val_loss: 0.3355 - val_acc: 0.8338\n",
            "Epoch 136/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8825 - val_loss: 0.3422 - val_acc: 0.8307\n",
            "Epoch 137/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8826 - val_loss: 0.3617 - val_acc: 0.8240\n",
            "Epoch 138/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8823 - val_loss: 0.3349 - val_acc: 0.8396\n",
            "Epoch 139/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8827 - val_loss: 0.3444 - val_acc: 0.8333\n",
            "Epoch 140/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8821 - val_loss: 0.3569 - val_acc: 0.8294\n",
            "Epoch 141/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8825 - val_loss: 0.3355 - val_acc: 0.8331\n",
            "Epoch 142/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8833 - val_loss: 0.3466 - val_acc: 0.8346\n",
            "Epoch 143/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8828 - val_loss: 0.3307 - val_acc: 0.8408\n",
            "Epoch 144/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8831 - val_loss: 0.3273 - val_acc: 0.8448\n",
            "Epoch 145/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8833 - val_loss: 0.3370 - val_acc: 0.8375\n",
            "Epoch 146/150\n",
            "59882/59882 [==============================] - 1s 15us/step - loss: 0.2868 - acc: 0.8817 - val_loss: 0.3374 - val_acc: 0.8378\n",
            "Epoch 147/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8827 - val_loss: 0.3342 - val_acc: 0.8359\n",
            "Epoch 148/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8830 - val_loss: 0.3199 - val_acc: 0.8444\n",
            "Epoch 149/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8824 - val_loss: 0.3807 - val_acc: 0.8154\n",
            "Epoch 150/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8828 - val_loss: 0.3230 - val_acc: 0.8430\n",
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1146    176        yes\n",
            "   1599    8382        no\n",
            "Accuracy: 0.8429620454746527\n",
            "Sensitivity: 0.8668683812405447\n",
            "Specificity: 0.8397956116621581\n",
            "Precision: 0.4174863387978142\n",
            "f_score: 0.563560363904598\n",
            "AUC: 0.8533319964513514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WZNHH5VaIiv",
        "colab_type": "code",
        "outputId": "34fb7aa4-e4cf-4d47-b777-2ca783ae4f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fpr_nn_c,tpr_nn_c ,thresholds_nn_c,auc_nn_c=crossvalidate_NN_SMOTE(10,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 71858 samples, validate on 4522 samples\n",
            "Epoch 1/150\n",
            "71858/71858 [==============================] - 1s 19us/step - loss: 0.3603 - acc: 0.8436 - val_loss: 0.3494 - val_acc: 0.8280\n",
            "Epoch 2/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3175 - acc: 0.8668 - val_loss: 0.3475 - val_acc: 0.8361\n",
            "Epoch 3/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3092 - acc: 0.8729 - val_loss: 0.3575 - val_acc: 0.8244\n",
            "Epoch 4/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8754 - val_loss: 0.3573 - val_acc: 0.8284\n",
            "Epoch 5/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3026 - acc: 0.8765 - val_loss: 0.3277 - val_acc: 0.8381\n",
            "Epoch 6/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8773 - val_loss: 0.3386 - val_acc: 0.8399\n",
            "Epoch 7/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8790 - val_loss: 0.3443 - val_acc: 0.8383\n",
            "Epoch 8/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3003 - acc: 0.8781 - val_loss: 0.3445 - val_acc: 0.8421\n",
            "Epoch 9/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8785 - val_loss: 0.3191 - val_acc: 0.8543\n",
            "Epoch 10/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2988 - acc: 0.8781 - val_loss: 0.3474 - val_acc: 0.8375\n",
            "Epoch 11/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2983 - acc: 0.8791 - val_loss: 0.3458 - val_acc: 0.8315\n",
            "Epoch 12/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2983 - acc: 0.8789 - val_loss: 0.3262 - val_acc: 0.8428\n",
            "Epoch 13/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2968 - acc: 0.8794 - val_loss: 0.3342 - val_acc: 0.8410\n",
            "Epoch 14/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2974 - acc: 0.8786 - val_loss: 0.3362 - val_acc: 0.8361\n",
            "Epoch 15/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8798 - val_loss: 0.3244 - val_acc: 0.8366\n",
            "Epoch 16/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8804 - val_loss: 0.3351 - val_acc: 0.8381\n",
            "Epoch 17/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2959 - acc: 0.8790 - val_loss: 0.3291 - val_acc: 0.8437\n",
            "Epoch 18/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2955 - acc: 0.8804 - val_loss: 0.3416 - val_acc: 0.8344\n",
            "Epoch 19/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2951 - acc: 0.8800 - val_loss: 0.3465 - val_acc: 0.8264\n",
            "Epoch 20/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2951 - acc: 0.8805 - val_loss: 0.3196 - val_acc: 0.8432\n",
            "Epoch 21/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2940 - acc: 0.8807 - val_loss: 0.3464 - val_acc: 0.8264\n",
            "Epoch 22/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2950 - acc: 0.8805 - val_loss: 0.3484 - val_acc: 0.8350\n",
            "Epoch 23/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2946 - acc: 0.8812 - val_loss: 0.3477 - val_acc: 0.8249\n",
            "Epoch 24/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.2946 - acc: 0.8812 - val_loss: 0.3284 - val_acc: 0.8425\n",
            "Epoch 25/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2944 - acc: 0.8814 - val_loss: 0.3508 - val_acc: 0.8306\n",
            "Epoch 26/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2942 - acc: 0.8818 - val_loss: 0.3394 - val_acc: 0.8313\n",
            "Epoch 27/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2941 - acc: 0.8825 - val_loss: 0.3427 - val_acc: 0.8317\n",
            "Epoch 28/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8819 - val_loss: 0.3321 - val_acc: 0.8432\n",
            "Epoch 29/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2940 - acc: 0.8826 - val_loss: 0.3349 - val_acc: 0.8359\n",
            "Epoch 30/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.2931 - acc: 0.8825 - val_loss: 0.3252 - val_acc: 0.8410\n",
            "Epoch 31/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2934 - acc: 0.8822 - val_loss: 0.3358 - val_acc: 0.8364\n",
            "Epoch 32/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2932 - acc: 0.8826 - val_loss: 0.3367 - val_acc: 0.8386\n",
            "Epoch 33/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2928 - acc: 0.8826 - val_loss: 0.3657 - val_acc: 0.8235\n",
            "Epoch 34/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2930 - acc: 0.8831 - val_loss: 0.3144 - val_acc: 0.8474\n",
            "Epoch 35/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2925 - acc: 0.8824 - val_loss: 0.3478 - val_acc: 0.8288\n",
            "Epoch 36/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2928 - acc: 0.8822 - val_loss: 0.3398 - val_acc: 0.8352\n",
            "Epoch 37/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2925 - acc: 0.8832 - val_loss: 0.3465 - val_acc: 0.8350\n",
            "Epoch 38/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2922 - acc: 0.8834 - val_loss: 0.3263 - val_acc: 0.8425\n",
            "Epoch 39/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2919 - acc: 0.8826 - val_loss: 0.3206 - val_acc: 0.8467\n",
            "Epoch 40/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2924 - acc: 0.8823 - val_loss: 0.3334 - val_acc: 0.8377\n",
            "Epoch 41/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8832 - val_loss: 0.3572 - val_acc: 0.8253\n",
            "Epoch 42/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2917 - acc: 0.8837 - val_loss: 0.3355 - val_acc: 0.8395\n",
            "Epoch 43/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8834 - val_loss: 0.3369 - val_acc: 0.8401\n",
            "Epoch 44/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3426 - val_acc: 0.8335\n",
            "Epoch 45/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2919 - acc: 0.8829 - val_loss: 0.3431 - val_acc: 0.8388\n",
            "Epoch 46/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8831 - val_loss: 0.3439 - val_acc: 0.8390\n",
            "Epoch 47/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2916 - acc: 0.8829 - val_loss: 0.3321 - val_acc: 0.8379\n",
            "Epoch 48/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2916 - acc: 0.8838 - val_loss: 0.3243 - val_acc: 0.8397\n",
            "Epoch 49/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3562 - val_acc: 0.8306\n",
            "Epoch 50/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8837 - val_loss: 0.3283 - val_acc: 0.8476\n",
            "Epoch 51/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8836 - val_loss: 0.3223 - val_acc: 0.8417\n",
            "Epoch 52/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2913 - acc: 0.8835 - val_loss: 0.3444 - val_acc: 0.8341\n",
            "Epoch 53/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8832 - val_loss: 0.3354 - val_acc: 0.8406\n",
            "Epoch 54/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8831 - val_loss: 0.3222 - val_acc: 0.8443\n",
            "Epoch 55/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2912 - acc: 0.8838 - val_loss: 0.3149 - val_acc: 0.8527\n",
            "Epoch 56/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8835 - val_loss: 0.3227 - val_acc: 0.8481\n",
            "Epoch 57/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8834 - val_loss: 0.3166 - val_acc: 0.8503\n",
            "Epoch 58/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8834 - val_loss: 0.3206 - val_acc: 0.8459\n",
            "Epoch 59/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8847 - val_loss: 0.3197 - val_acc: 0.8463\n",
            "Epoch 60/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8832 - val_loss: 0.3254 - val_acc: 0.8443\n",
            "Epoch 61/150\n",
            "71858/71858 [==============================] - 1s 18us/step - loss: 0.2907 - acc: 0.8847 - val_loss: 0.3434 - val_acc: 0.8381\n",
            "Epoch 62/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2909 - acc: 0.8834 - val_loss: 0.3287 - val_acc: 0.8375\n",
            "Epoch 63/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8835 - val_loss: 0.3339 - val_acc: 0.8401\n",
            "Epoch 64/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8836 - val_loss: 0.3543 - val_acc: 0.8322\n",
            "Epoch 65/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8840 - val_loss: 0.3277 - val_acc: 0.8379\n",
            "Epoch 66/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8838 - val_loss: 0.3394 - val_acc: 0.8379\n",
            "Epoch 67/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8837 - val_loss: 0.3473 - val_acc: 0.8386\n",
            "Epoch 68/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8840 - val_loss: 0.3284 - val_acc: 0.8463\n",
            "Epoch 69/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2903 - acc: 0.8836 - val_loss: 0.3471 - val_acc: 0.8381\n",
            "Epoch 70/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2907 - acc: 0.8839 - val_loss: 0.3285 - val_acc: 0.8375\n",
            "Epoch 71/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8847 - val_loss: 0.3357 - val_acc: 0.8395\n",
            "Epoch 72/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8842 - val_loss: 0.3289 - val_acc: 0.8410\n",
            "Epoch 73/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8839 - val_loss: 0.3315 - val_acc: 0.8441\n",
            "Epoch 74/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2909 - acc: 0.8842 - val_loss: 0.3227 - val_acc: 0.8430\n",
            "Epoch 75/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8832 - val_loss: 0.3376 - val_acc: 0.8339\n",
            "Epoch 76/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2901 - acc: 0.8836 - val_loss: 0.3442 - val_acc: 0.8333\n",
            "Epoch 77/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2908 - acc: 0.8838 - val_loss: 0.3434 - val_acc: 0.8357\n",
            "Epoch 78/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8830 - val_loss: 0.3381 - val_acc: 0.8392\n",
            "Epoch 79/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2906 - acc: 0.8845 - val_loss: 0.3572 - val_acc: 0.8364\n",
            "Epoch 80/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2905 - acc: 0.8835 - val_loss: 0.3182 - val_acc: 0.8445\n",
            "Epoch 81/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8834 - val_loss: 0.3307 - val_acc: 0.8412\n",
            "Epoch 82/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2906 - acc: 0.8834 - val_loss: 0.3509 - val_acc: 0.8328\n",
            "Epoch 83/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8836 - val_loss: 0.3300 - val_acc: 0.8428\n",
            "Epoch 84/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8842 - val_loss: 0.3520 - val_acc: 0.8330\n",
            "Epoch 85/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2907 - acc: 0.8839 - val_loss: 0.3524 - val_acc: 0.8319\n",
            "Epoch 86/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8837 - val_loss: 0.3350 - val_acc: 0.8428\n",
            "Epoch 87/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2906 - acc: 0.8838 - val_loss: 0.3352 - val_acc: 0.8408\n",
            "Epoch 88/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8842 - val_loss: 0.3270 - val_acc: 0.8494\n",
            "Epoch 89/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8845 - val_loss: 0.3308 - val_acc: 0.8425\n",
            "Epoch 90/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8843 - val_loss: 0.3408 - val_acc: 0.8383\n",
            "Epoch 91/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8838 - val_loss: 0.3213 - val_acc: 0.8487\n",
            "Epoch 92/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8850 - val_loss: 0.3353 - val_acc: 0.8421\n",
            "Epoch 93/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8845 - val_loss: 0.3272 - val_acc: 0.8443\n",
            "Epoch 94/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8845 - val_loss: 0.3298 - val_acc: 0.8437\n",
            "Epoch 95/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8850 - val_loss: 0.3322 - val_acc: 0.8386\n",
            "Epoch 96/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8835 - val_loss: 0.3306 - val_acc: 0.8445\n",
            "Epoch 97/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2901 - acc: 0.8841 - val_loss: 0.3251 - val_acc: 0.8437\n",
            "Epoch 98/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2907 - acc: 0.8843 - val_loss: 0.3306 - val_acc: 0.8419\n",
            "Epoch 99/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2903 - acc: 0.8843 - val_loss: 0.3573 - val_acc: 0.8315\n",
            "Epoch 100/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8846 - val_loss: 0.3396 - val_acc: 0.8417\n",
            "Epoch 101/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8837 - val_loss: 0.3366 - val_acc: 0.8406\n",
            "Epoch 102/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2902 - acc: 0.8843 - val_loss: 0.3489 - val_acc: 0.8399\n",
            "Epoch 103/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8845 - val_loss: 0.3126 - val_acc: 0.8461\n",
            "Epoch 104/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8845 - val_loss: 0.3274 - val_acc: 0.8501\n",
            "Epoch 105/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8846 - val_loss: 0.3489 - val_acc: 0.8352\n",
            "Epoch 106/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2902 - acc: 0.8839 - val_loss: 0.3542 - val_acc: 0.8337\n",
            "Epoch 107/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8841 - val_loss: 0.3343 - val_acc: 0.8412\n",
            "Epoch 108/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8843 - val_loss: 0.3351 - val_acc: 0.8406\n",
            "Epoch 109/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8851 - val_loss: 0.3449 - val_acc: 0.8425\n",
            "Epoch 110/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8843 - val_loss: 0.3279 - val_acc: 0.8472\n",
            "Epoch 111/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8844 - val_loss: 0.3498 - val_acc: 0.8366\n",
            "Epoch 112/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8838 - val_loss: 0.3363 - val_acc: 0.8439\n",
            "Epoch 113/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3452 - val_acc: 0.8399\n",
            "Epoch 114/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8844 - val_loss: 0.3235 - val_acc: 0.8483\n",
            "Epoch 115/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8847 - val_loss: 0.3457 - val_acc: 0.8386\n",
            "Epoch 116/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2896 - acc: 0.8844 - val_loss: 0.3562 - val_acc: 0.8348\n",
            "Epoch 117/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8847 - val_loss: 0.3536 - val_acc: 0.8388\n",
            "Epoch 118/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8845 - val_loss: 0.3448 - val_acc: 0.8414\n",
            "Epoch 119/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8846 - val_loss: 0.3433 - val_acc: 0.8390\n",
            "Epoch 120/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8851 - val_loss: 0.3315 - val_acc: 0.8417\n",
            "Epoch 121/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8850 - val_loss: 0.3249 - val_acc: 0.8454\n",
            "Epoch 122/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8852 - val_loss: 0.3473 - val_acc: 0.8364\n",
            "Epoch 123/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8851 - val_loss: 0.3290 - val_acc: 0.8419\n",
            "Epoch 124/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8856 - val_loss: 0.3520 - val_acc: 0.8388\n",
            "Epoch 125/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2896 - acc: 0.8851 - val_loss: 0.3330 - val_acc: 0.8430\n",
            "Epoch 126/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8856 - val_loss: 0.3328 - val_acc: 0.8467\n",
            "Epoch 127/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8847 - val_loss: 0.3425 - val_acc: 0.8395\n",
            "Epoch 128/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8859 - val_loss: 0.3326 - val_acc: 0.8412\n",
            "Epoch 129/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8849 - val_loss: 0.3362 - val_acc: 0.8445\n",
            "Epoch 130/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8857 - val_loss: 0.3326 - val_acc: 0.8417\n",
            "Epoch 131/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8856 - val_loss: 0.3429 - val_acc: 0.8408\n",
            "Epoch 132/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8857 - val_loss: 0.3324 - val_acc: 0.8432\n",
            "Epoch 133/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8859 - val_loss: 0.3366 - val_acc: 0.8501\n",
            "Epoch 134/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8854 - val_loss: 0.3439 - val_acc: 0.8408\n",
            "Epoch 135/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8852 - val_loss: 0.3153 - val_acc: 0.8534\n",
            "Epoch 136/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8861 - val_loss: 0.3215 - val_acc: 0.8428\n",
            "Epoch 137/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8856 - val_loss: 0.3252 - val_acc: 0.8492\n",
            "Epoch 138/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8861 - val_loss: 0.3523 - val_acc: 0.8370\n",
            "Epoch 139/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8860 - val_loss: 0.3455 - val_acc: 0.8423\n",
            "Epoch 140/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8855 - val_loss: 0.3401 - val_acc: 0.8430\n",
            "Epoch 141/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8854 - val_loss: 0.3656 - val_acc: 0.8313\n",
            "Epoch 142/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8856 - val_loss: 0.3507 - val_acc: 0.8403\n",
            "Epoch 143/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8853 - val_loss: 0.3415 - val_acc: 0.8439\n",
            "Epoch 144/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8856 - val_loss: 0.3403 - val_acc: 0.8417\n",
            "Epoch 145/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8851 - val_loss: 0.3293 - val_acc: 0.8456\n",
            "Epoch 146/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8861 - val_loss: 0.3401 - val_acc: 0.8450\n",
            "Epoch 147/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8859 - val_loss: 0.3286 - val_acc: 0.8474\n",
            "Epoch 148/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8858 - val_loss: 0.3473 - val_acc: 0.8430\n",
            "Epoch 149/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8856 - val_loss: 0.3524 - val_acc: 0.8355\n",
            "Epoch 150/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8860 - val_loss: 0.3431 - val_acc: 0.8388\n",
            "acc: 83.88%\n",
            "Train on 71858 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71858/71858 [==============================] - 1s 20us/step - loss: 0.3627 - acc: 0.8432 - val_loss: 0.3964 - val_acc: 0.8042\n",
            "Epoch 2/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3245 - acc: 0.8657 - val_loss: 0.3473 - val_acc: 0.8363\n",
            "Epoch 3/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3096 - acc: 0.8757 - val_loss: 0.3655 - val_acc: 0.8246\n",
            "Epoch 4/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8769 - val_loss: 0.3494 - val_acc: 0.8330\n",
            "Epoch 5/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8770 - val_loss: 0.3823 - val_acc: 0.8151\n",
            "Epoch 6/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8786 - val_loss: 0.3481 - val_acc: 0.8295\n",
            "Epoch 7/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3010 - acc: 0.8784 - val_loss: 0.3652 - val_acc: 0.8261\n",
            "Epoch 8/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3003 - acc: 0.8798 - val_loss: 0.3646 - val_acc: 0.8264\n",
            "Epoch 9/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8790 - val_loss: 0.3729 - val_acc: 0.8230\n",
            "Epoch 10/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2981 - acc: 0.8799 - val_loss: 0.3716 - val_acc: 0.8242\n",
            "Epoch 11/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2978 - acc: 0.8797 - val_loss: 0.3508 - val_acc: 0.8268\n",
            "Epoch 12/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2972 - acc: 0.8795 - val_loss: 0.3782 - val_acc: 0.8166\n",
            "Epoch 13/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2974 - acc: 0.8792 - val_loss: 0.3667 - val_acc: 0.8226\n",
            "Epoch 14/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8781 - val_loss: 0.3698 - val_acc: 0.8186\n",
            "Epoch 15/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2965 - acc: 0.8787 - val_loss: 0.3582 - val_acc: 0.8317\n",
            "Epoch 16/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2966 - acc: 0.8789 - val_loss: 0.3457 - val_acc: 0.8345\n",
            "Epoch 17/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2970 - acc: 0.8784 - val_loss: 0.3647 - val_acc: 0.8242\n",
            "Epoch 18/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2961 - acc: 0.8793 - val_loss: 0.3676 - val_acc: 0.8211\n",
            "Epoch 19/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2958 - acc: 0.8787 - val_loss: 0.3607 - val_acc: 0.8224\n",
            "Epoch 20/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2962 - acc: 0.8789 - val_loss: 0.3573 - val_acc: 0.8359\n",
            "Epoch 21/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2958 - acc: 0.8792 - val_loss: 0.3590 - val_acc: 0.8222\n",
            "Epoch 22/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2955 - acc: 0.8789 - val_loss: 0.3581 - val_acc: 0.8277\n",
            "Epoch 23/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2952 - acc: 0.8788 - val_loss: 0.3512 - val_acc: 0.8381\n",
            "Epoch 24/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8792 - val_loss: 0.3618 - val_acc: 0.8319\n",
            "Epoch 25/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2953 - acc: 0.8788 - val_loss: 0.3624 - val_acc: 0.8301\n",
            "Epoch 26/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2951 - acc: 0.8800 - val_loss: 0.3567 - val_acc: 0.8332\n",
            "Epoch 27/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8799 - val_loss: 0.3631 - val_acc: 0.8235\n",
            "Epoch 28/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2951 - acc: 0.8798 - val_loss: 0.3625 - val_acc: 0.8275\n",
            "Epoch 29/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8790 - val_loss: 0.3401 - val_acc: 0.8452\n",
            "Epoch 30/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2949 - acc: 0.8794 - val_loss: 0.3664 - val_acc: 0.8257\n",
            "Epoch 31/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8798 - val_loss: 0.3444 - val_acc: 0.8365\n",
            "Epoch 32/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2945 - acc: 0.8801 - val_loss: 0.3658 - val_acc: 0.8359\n",
            "Epoch 33/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2943 - acc: 0.8807 - val_loss: 0.3561 - val_acc: 0.8370\n",
            "Epoch 34/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8811 - val_loss: 0.3343 - val_acc: 0.8487\n",
            "Epoch 35/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8810 - val_loss: 0.3621 - val_acc: 0.8368\n",
            "Epoch 36/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8811 - val_loss: 0.3611 - val_acc: 0.8323\n",
            "Epoch 37/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2938 - acc: 0.8807 - val_loss: 0.3486 - val_acc: 0.8414\n",
            "Epoch 38/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2935 - acc: 0.8811 - val_loss: 0.3401 - val_acc: 0.8407\n",
            "Epoch 39/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8808 - val_loss: 0.3539 - val_acc: 0.8315\n",
            "Epoch 40/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8801 - val_loss: 0.3446 - val_acc: 0.8430\n",
            "Epoch 41/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2928 - acc: 0.8806 - val_loss: 0.3628 - val_acc: 0.8266\n",
            "Epoch 42/150\n",
            "71858/71858 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8798 - val_loss: 0.3546 - val_acc: 0.8374\n",
            "Epoch 43/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2929 - acc: 0.8814 - val_loss: 0.3455 - val_acc: 0.8390\n",
            "Epoch 44/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2928 - acc: 0.8807 - val_loss: 0.3711 - val_acc: 0.8277\n",
            "Epoch 45/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2928 - acc: 0.8814 - val_loss: 0.3757 - val_acc: 0.8250\n",
            "Epoch 46/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2925 - acc: 0.8811 - val_loss: 0.3373 - val_acc: 0.8430\n",
            "Epoch 47/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2928 - acc: 0.8814 - val_loss: 0.3609 - val_acc: 0.8388\n",
            "Epoch 48/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2930 - acc: 0.8810 - val_loss: 0.3702 - val_acc: 0.8334\n",
            "Epoch 49/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2926 - acc: 0.8809 - val_loss: 0.3671 - val_acc: 0.8219\n",
            "Epoch 50/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2928 - acc: 0.8814 - val_loss: 0.3463 - val_acc: 0.8396\n",
            "Epoch 51/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2924 - acc: 0.8814 - val_loss: 0.3745 - val_acc: 0.8284\n",
            "Epoch 52/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2926 - acc: 0.8820 - val_loss: 0.3119 - val_acc: 0.8536\n",
            "Epoch 53/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2927 - acc: 0.8807 - val_loss: 0.3637 - val_acc: 0.8286\n",
            "Epoch 54/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2926 - acc: 0.8818 - val_loss: 0.3571 - val_acc: 0.8345\n",
            "Epoch 55/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2923 - acc: 0.8808 - val_loss: 0.3592 - val_acc: 0.8310\n",
            "Epoch 56/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2920 - acc: 0.8824 - val_loss: 0.3781 - val_acc: 0.8226\n",
            "Epoch 57/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2928 - acc: 0.8811 - val_loss: 0.3523 - val_acc: 0.8295\n",
            "Epoch 58/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8816 - val_loss: 0.3252 - val_acc: 0.8496\n",
            "Epoch 59/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2916 - acc: 0.8819 - val_loss: 0.3506 - val_acc: 0.8343\n",
            "Epoch 60/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2919 - acc: 0.8823 - val_loss: 0.3396 - val_acc: 0.8441\n",
            "Epoch 61/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2917 - acc: 0.8838 - val_loss: 0.3537 - val_acc: 0.8376\n",
            "Epoch 62/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2912 - acc: 0.8837 - val_loss: 0.3702 - val_acc: 0.8352\n",
            "Epoch 63/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8840 - val_loss: 0.3572 - val_acc: 0.8407\n",
            "Epoch 64/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8843 - val_loss: 0.3690 - val_acc: 0.8368\n",
            "Epoch 65/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8841 - val_loss: 0.3709 - val_acc: 0.8354\n",
            "Epoch 66/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8838 - val_loss: 0.3666 - val_acc: 0.8345\n",
            "Epoch 67/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8848 - val_loss: 0.3722 - val_acc: 0.8348\n",
            "Epoch 68/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2903 - acc: 0.8838 - val_loss: 0.3270 - val_acc: 0.8452\n",
            "Epoch 69/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8841 - val_loss: 0.3542 - val_acc: 0.8396\n",
            "Epoch 70/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8845 - val_loss: 0.3605 - val_acc: 0.8365\n",
            "Epoch 71/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8844 - val_loss: 0.3481 - val_acc: 0.8454\n",
            "Epoch 72/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8856 - val_loss: 0.3401 - val_acc: 0.8443\n",
            "Epoch 73/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2909 - acc: 0.8848 - val_loss: 0.3539 - val_acc: 0.8401\n",
            "Epoch 74/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8852 - val_loss: 0.3403 - val_acc: 0.8436\n",
            "Epoch 75/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8854 - val_loss: 0.3470 - val_acc: 0.8423\n",
            "Epoch 76/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8847 - val_loss: 0.3471 - val_acc: 0.8416\n",
            "Epoch 77/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8839 - val_loss: 0.3465 - val_acc: 0.8407\n",
            "Epoch 78/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8851 - val_loss: 0.3669 - val_acc: 0.8323\n",
            "Epoch 79/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8853 - val_loss: 0.3528 - val_acc: 0.8401\n",
            "Epoch 80/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8845 - val_loss: 0.3484 - val_acc: 0.8412\n",
            "Epoch 81/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8840 - val_loss: 0.3409 - val_acc: 0.8496\n",
            "Epoch 82/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8849 - val_loss: 0.3272 - val_acc: 0.8483\n",
            "Epoch 83/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3585 - val_acc: 0.8381\n",
            "Epoch 84/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3634 - val_acc: 0.8365\n",
            "Epoch 85/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8850 - val_loss: 0.3688 - val_acc: 0.8323\n",
            "Epoch 86/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8850 - val_loss: 0.3690 - val_acc: 0.8365\n",
            "Epoch 87/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8850 - val_loss: 0.3665 - val_acc: 0.8330\n",
            "Epoch 88/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8852 - val_loss: 0.3316 - val_acc: 0.8449\n",
            "Epoch 89/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8852 - val_loss: 0.3405 - val_acc: 0.8412\n",
            "Epoch 90/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8845 - val_loss: 0.3441 - val_acc: 0.8399\n",
            "Epoch 91/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8851 - val_loss: 0.3537 - val_acc: 0.8407\n",
            "Epoch 92/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3487 - val_acc: 0.8500\n",
            "Epoch 93/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8852 - val_loss: 0.3659 - val_acc: 0.8354\n",
            "Epoch 94/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8852 - val_loss: 0.3508 - val_acc: 0.8396\n",
            "Epoch 95/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8858 - val_loss: 0.3373 - val_acc: 0.8452\n",
            "Epoch 96/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8859 - val_loss: 0.3851 - val_acc: 0.8275\n",
            "Epoch 97/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3610 - val_acc: 0.8394\n",
            "Epoch 98/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8848 - val_loss: 0.3562 - val_acc: 0.8423\n",
            "Epoch 99/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8857 - val_loss: 0.3531 - val_acc: 0.8321\n",
            "Epoch 100/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8857 - val_loss: 0.3403 - val_acc: 0.8436\n",
            "Epoch 101/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8847 - val_loss: 0.3760 - val_acc: 0.8297\n",
            "Epoch 102/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8860 - val_loss: 0.3472 - val_acc: 0.8361\n",
            "Epoch 103/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8855 - val_loss: 0.3394 - val_acc: 0.8478\n",
            "Epoch 104/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8855 - val_loss: 0.3282 - val_acc: 0.8456\n",
            "Epoch 105/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8852 - val_loss: 0.3367 - val_acc: 0.8467\n",
            "Epoch 106/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8847 - val_loss: 0.3395 - val_acc: 0.8447\n",
            "Epoch 107/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8850 - val_loss: 0.3601 - val_acc: 0.8341\n",
            "Epoch 108/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8856 - val_loss: 0.3505 - val_acc: 0.8370\n",
            "Epoch 109/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8855 - val_loss: 0.3425 - val_acc: 0.8476\n",
            "Epoch 110/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8845 - val_loss: 0.3610 - val_acc: 0.8363\n",
            "Epoch 111/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8857 - val_loss: 0.3703 - val_acc: 0.8339\n",
            "Epoch 112/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8861 - val_loss: 0.3771 - val_acc: 0.8359\n",
            "Epoch 113/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8852 - val_loss: 0.3453 - val_acc: 0.8436\n",
            "Epoch 114/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8856 - val_loss: 0.3511 - val_acc: 0.8447\n",
            "Epoch 115/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8862 - val_loss: 0.3575 - val_acc: 0.8407\n",
            "Epoch 116/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8858 - val_loss: 0.3327 - val_acc: 0.8476\n",
            "Epoch 117/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8846 - val_loss: 0.3470 - val_acc: 0.8434\n",
            "Epoch 118/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8852 - val_loss: 0.3505 - val_acc: 0.8385\n",
            "Epoch 119/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8856 - val_loss: 0.3557 - val_acc: 0.8401\n",
            "Epoch 120/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8856 - val_loss: 0.3551 - val_acc: 0.8381\n",
            "Epoch 121/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8852 - val_loss: 0.3634 - val_acc: 0.8330\n",
            "Epoch 122/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8853 - val_loss: 0.3724 - val_acc: 0.8326\n",
            "Epoch 123/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3478 - val_acc: 0.8423\n",
            "Epoch 124/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8854 - val_loss: 0.3576 - val_acc: 0.8374\n",
            "Epoch 125/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8868 - val_loss: 0.3746 - val_acc: 0.8297\n",
            "Epoch 126/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3711 - val_acc: 0.8310\n",
            "Epoch 127/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8856 - val_loss: 0.3536 - val_acc: 0.8383\n",
            "Epoch 128/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8860 - val_loss: 0.3664 - val_acc: 0.8350\n",
            "Epoch 129/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8859 - val_loss: 0.3518 - val_acc: 0.8445\n",
            "Epoch 130/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3535 - val_acc: 0.8463\n",
            "Epoch 131/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8851 - val_loss: 0.3464 - val_acc: 0.8438\n",
            "Epoch 132/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8862 - val_loss: 0.3612 - val_acc: 0.8363\n",
            "Epoch 133/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8861 - val_loss: 0.3461 - val_acc: 0.8430\n",
            "Epoch 134/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2880 - acc: 0.8857 - val_loss: 0.3524 - val_acc: 0.8414\n",
            "Epoch 135/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8856 - val_loss: 0.3659 - val_acc: 0.8359\n",
            "Epoch 136/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8849 - val_loss: 0.3473 - val_acc: 0.8394\n",
            "Epoch 137/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3647 - val_acc: 0.8301\n",
            "Epoch 138/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8856 - val_loss: 0.3528 - val_acc: 0.8421\n",
            "Epoch 139/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3557 - val_acc: 0.8381\n",
            "Epoch 140/150\n",
            "71858/71858 [==============================] - 1s 18us/step - loss: 0.2881 - acc: 0.8859 - val_loss: 0.3385 - val_acc: 0.8474\n",
            "Epoch 141/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8851 - val_loss: 0.3373 - val_acc: 0.8432\n",
            "Epoch 142/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8858 - val_loss: 0.3771 - val_acc: 0.8323\n",
            "Epoch 143/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8858 - val_loss: 0.3566 - val_acc: 0.8394\n",
            "Epoch 144/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8850 - val_loss: 0.3577 - val_acc: 0.8405\n",
            "Epoch 145/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8853 - val_loss: 0.3425 - val_acc: 0.8412\n",
            "Epoch 146/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8859 - val_loss: 0.3663 - val_acc: 0.8381\n",
            "Epoch 147/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8857 - val_loss: 0.3371 - val_acc: 0.8461\n",
            "Epoch 148/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8853 - val_loss: 0.3583 - val_acc: 0.8394\n",
            "Epoch 149/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2879 - acc: 0.8860 - val_loss: 0.3727 - val_acc: 0.8350\n",
            "Epoch 150/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8859 - val_loss: 0.3651 - val_acc: 0.8343\n",
            "acc: 83.43%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 1s 20us/step - loss: 0.3556 - acc: 0.8503 - val_loss: 0.3425 - val_acc: 0.8394\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 14us/step - loss: 0.3118 - acc: 0.8717 - val_loss: 0.3344 - val_acc: 0.8396\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3014 - acc: 0.8770 - val_loss: 0.3482 - val_acc: 0.8354\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2978 - acc: 0.8795 - val_loss: 0.3237 - val_acc: 0.8461\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8789 - val_loss: 0.3160 - val_acc: 0.8518\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 14us/step - loss: 0.2946 - acc: 0.8800 - val_loss: 0.3343 - val_acc: 0.8465\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2940 - acc: 0.8805 - val_loss: 0.3414 - val_acc: 0.8385\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2935 - acc: 0.8807 - val_loss: 0.3070 - val_acc: 0.8560\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2926 - acc: 0.8811 - val_loss: 0.3223 - val_acc: 0.8498\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2918 - acc: 0.8812 - val_loss: 0.3104 - val_acc: 0.8558\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2918 - acc: 0.8821 - val_loss: 0.3224 - val_acc: 0.8449\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2910 - acc: 0.8823 - val_loss: 0.3287 - val_acc: 0.8483\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8827 - val_loss: 0.3500 - val_acc: 0.8390\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8826 - val_loss: 0.3084 - val_acc: 0.8593\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8837 - val_loss: 0.2993 - val_acc: 0.8514\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8832 - val_loss: 0.3084 - val_acc: 0.8571\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2901 - acc: 0.8833 - val_loss: 0.3203 - val_acc: 0.8514\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8839 - val_loss: 0.3399 - val_acc: 0.8425\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8834 - val_loss: 0.3264 - val_acc: 0.8489\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2896 - acc: 0.8828 - val_loss: 0.3288 - val_acc: 0.8487\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8846 - val_loss: 0.3311 - val_acc: 0.8418\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8837 - val_loss: 0.3353 - val_acc: 0.8416\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2893 - acc: 0.8842 - val_loss: 0.3033 - val_acc: 0.8604\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8840 - val_loss: 0.3168 - val_acc: 0.8503\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8835 - val_loss: 0.3247 - val_acc: 0.8516\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8833 - val_loss: 0.2989 - val_acc: 0.8613\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8829 - val_loss: 0.3080 - val_acc: 0.8536\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8839 - val_loss: 0.3262 - val_acc: 0.8491\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8840 - val_loss: 0.3296 - val_acc: 0.8452\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8845 - val_loss: 0.3118 - val_acc: 0.8545\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8836 - val_loss: 0.3311 - val_acc: 0.8467\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8838 - val_loss: 0.3337 - val_acc: 0.8430\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3281 - val_acc: 0.8476\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3507 - val_acc: 0.8396\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8846 - val_loss: 0.3002 - val_acc: 0.8653\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8839 - val_loss: 0.3160 - val_acc: 0.8551\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8841 - val_loss: 0.3321 - val_acc: 0.8469\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8854 - val_loss: 0.3204 - val_acc: 0.8542\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8842 - val_loss: 0.3165 - val_acc: 0.8516\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8847 - val_loss: 0.3341 - val_acc: 0.8465\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8842 - val_loss: 0.3080 - val_acc: 0.8602\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8831 - val_loss: 0.3295 - val_acc: 0.8483\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8842 - val_loss: 0.3278 - val_acc: 0.8489\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8838 - val_loss: 0.3263 - val_acc: 0.8461\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8844 - val_loss: 0.3448 - val_acc: 0.8361\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3145 - val_acc: 0.8514\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8842 - val_loss: 0.3395 - val_acc: 0.8452\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8840 - val_loss: 0.3329 - val_acc: 0.8472\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8838 - val_loss: 0.3298 - val_acc: 0.8540\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8854 - val_loss: 0.3211 - val_acc: 0.8531\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3125 - val_acc: 0.8571\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8850 - val_loss: 0.3496 - val_acc: 0.8350\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8850 - val_loss: 0.3398 - val_acc: 0.8421\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3034 - val_acc: 0.8571\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8853 - val_loss: 0.3231 - val_acc: 0.8443\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2893 - acc: 0.8842 - val_loss: 0.3491 - val_acc: 0.8388\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8845 - val_loss: 0.3362 - val_acc: 0.8430\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8844 - val_loss: 0.3349 - val_acc: 0.8474\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3177 - val_acc: 0.8485\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8846 - val_loss: 0.3225 - val_acc: 0.8531\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8847 - val_loss: 0.3114 - val_acc: 0.8571\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8845 - val_loss: 0.3312 - val_acc: 0.8478\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8857 - val_loss: 0.3160 - val_acc: 0.8531\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8842 - val_loss: 0.3219 - val_acc: 0.8496\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3146 - val_acc: 0.8500\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8845 - val_loss: 0.3226 - val_acc: 0.8491\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8847 - val_loss: 0.3172 - val_acc: 0.8551\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8843 - val_loss: 0.3154 - val_acc: 0.8520\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3215 - val_acc: 0.8518\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8843 - val_loss: 0.3309 - val_acc: 0.8483\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8847 - val_loss: 0.3249 - val_acc: 0.8478\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8843 - val_loss: 0.3206 - val_acc: 0.8516\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8847 - val_loss: 0.3535 - val_acc: 0.8381\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8845 - val_loss: 0.3260 - val_acc: 0.8496\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8849 - val_loss: 0.3233 - val_acc: 0.8507\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8848 - val_loss: 0.3146 - val_acc: 0.8483\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3170 - val_acc: 0.8551\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3319 - val_acc: 0.8503\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8840 - val_loss: 0.3419 - val_acc: 0.8465\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8842 - val_loss: 0.3479 - val_acc: 0.8376\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8842 - val_loss: 0.3396 - val_acc: 0.8352\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8844 - val_loss: 0.3090 - val_acc: 0.8545\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8855 - val_loss: 0.3440 - val_acc: 0.8412\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8850 - val_loss: 0.3138 - val_acc: 0.8527\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2888 - acc: 0.8844 - val_loss: 0.3301 - val_acc: 0.8503\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8844 - val_loss: 0.3099 - val_acc: 0.8564\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8847 - val_loss: 0.3295 - val_acc: 0.8425\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8852 - val_loss: 0.3103 - val_acc: 0.8534\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8854 - val_loss: 0.3397 - val_acc: 0.8399\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8848 - val_loss: 0.3228 - val_acc: 0.8489\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8844 - val_loss: 0.3255 - val_acc: 0.8529\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8845 - val_loss: 0.3017 - val_acc: 0.8602\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8848 - val_loss: 0.3344 - val_acc: 0.8476\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8838 - val_loss: 0.3451 - val_acc: 0.8352\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8849 - val_loss: 0.3519 - val_acc: 0.8436\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8848 - val_loss: 0.3187 - val_acc: 0.8540\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8845 - val_loss: 0.3435 - val_acc: 0.8447\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8847 - val_loss: 0.3126 - val_acc: 0.8529\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8846 - val_loss: 0.3234 - val_acc: 0.8520\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8847 - val_loss: 0.3274 - val_acc: 0.8509\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8852 - val_loss: 0.3238 - val_acc: 0.8540\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3016 - val_acc: 0.8600\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8839 - val_loss: 0.3266 - val_acc: 0.8449\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8844 - val_loss: 0.3282 - val_acc: 0.8480\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8834 - val_loss: 0.3354 - val_acc: 0.8434\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2880 - acc: 0.8849 - val_loss: 0.3111 - val_acc: 0.8545\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8843 - val_loss: 0.3434 - val_acc: 0.8423\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8844 - val_loss: 0.3163 - val_acc: 0.8582\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8841 - val_loss: 0.3084 - val_acc: 0.8540\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3126 - val_acc: 0.8516\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8838 - val_loss: 0.3465 - val_acc: 0.8352\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8848 - val_loss: 0.3381 - val_acc: 0.8441\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8839 - val_loss: 0.3348 - val_acc: 0.8483\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8846 - val_loss: 0.3068 - val_acc: 0.8564\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8854 - val_loss: 0.3462 - val_acc: 0.8392\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8849 - val_loss: 0.3293 - val_acc: 0.8489\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8841 - val_loss: 0.3175 - val_acc: 0.8534\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8838 - val_loss: 0.3334 - val_acc: 0.8456\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3123 - val_acc: 0.8551\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8835 - val_loss: 0.3085 - val_acc: 0.8540\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8846 - val_loss: 0.3359 - val_acc: 0.8489\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8842 - val_loss: 0.3201 - val_acc: 0.8511\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3336 - val_acc: 0.8474\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8842 - val_loss: 0.3291 - val_acc: 0.8427\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8858 - val_loss: 0.3221 - val_acc: 0.8500\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3145 - val_acc: 0.8494\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8839 - val_loss: 0.3260 - val_acc: 0.8531\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8843 - val_loss: 0.3331 - val_acc: 0.8467\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3048 - val_acc: 0.8580\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8845 - val_loss: 0.3297 - val_acc: 0.8478\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3429 - val_acc: 0.8345\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3375 - val_acc: 0.8463\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8841 - val_loss: 0.3322 - val_acc: 0.8509\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8842 - val_loss: 0.3057 - val_acc: 0.8584\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8838 - val_loss: 0.3419 - val_acc: 0.8410\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3149 - val_acc: 0.8562\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8852 - val_loss: 0.3028 - val_acc: 0.8584\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2880 - acc: 0.8848 - val_loss: 0.3311 - val_acc: 0.8441\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8850 - val_loss: 0.3267 - val_acc: 0.8483\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8849 - val_loss: 0.3221 - val_acc: 0.8487\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8844 - val_loss: 0.3262 - val_acc: 0.8476\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3282 - val_acc: 0.8465\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8847 - val_loss: 0.3337 - val_acc: 0.8427\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8848 - val_loss: 0.3448 - val_acc: 0.8430\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8843 - val_loss: 0.3290 - val_acc: 0.8483\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8847 - val_loss: 0.3105 - val_acc: 0.8573\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8842 - val_loss: 0.3272 - val_acc: 0.8494\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8850 - val_loss: 0.3102 - val_acc: 0.8556\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8847 - val_loss: 0.3158 - val_acc: 0.8516\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8839 - val_loss: 0.3384 - val_acc: 0.8423\n",
            "acc: 84.23%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 21us/step - loss: 0.3491 - acc: 0.8537 - val_loss: 0.3614 - val_acc: 0.8284\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3129 - acc: 0.8722 - val_loss: 0.3719 - val_acc: 0.8321\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8752 - val_loss: 0.3530 - val_acc: 0.8374\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3045 - acc: 0.8760 - val_loss: 0.3375 - val_acc: 0.8385\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8764 - val_loss: 0.3370 - val_acc: 0.8421\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8766 - val_loss: 0.3145 - val_acc: 0.8463\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3022 - acc: 0.8772 - val_loss: 0.3435 - val_acc: 0.8328\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3011 - acc: 0.8774 - val_loss: 0.3359 - val_acc: 0.8418\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.3010 - acc: 0.8780 - val_loss: 0.3218 - val_acc: 0.8452\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3004 - acc: 0.8786 - val_loss: 0.3508 - val_acc: 0.8350\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3007 - acc: 0.8769 - val_loss: 0.3537 - val_acc: 0.8365\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3004 - acc: 0.8778 - val_loss: 0.3315 - val_acc: 0.8361\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2998 - acc: 0.8786 - val_loss: 0.3503 - val_acc: 0.8368\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2999 - acc: 0.8785 - val_loss: 0.3398 - val_acc: 0.8427\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2998 - acc: 0.8782 - val_loss: 0.3471 - val_acc: 0.8359\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2996 - acc: 0.8787 - val_loss: 0.3160 - val_acc: 0.8452\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2994 - acc: 0.8785 - val_loss: 0.3396 - val_acc: 0.8392\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2993 - acc: 0.8791 - val_loss: 0.3684 - val_acc: 0.8321\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2993 - acc: 0.8785 - val_loss: 0.3187 - val_acc: 0.8469\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8789 - val_loss: 0.3202 - val_acc: 0.8465\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8787 - val_loss: 0.3426 - val_acc: 0.8394\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2983 - acc: 0.8779 - val_loss: 0.3181 - val_acc: 0.8456\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8792 - val_loss: 0.3410 - val_acc: 0.8441\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8787 - val_loss: 0.3625 - val_acc: 0.8259\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2982 - acc: 0.8791 - val_loss: 0.3267 - val_acc: 0.8452\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8787 - val_loss: 0.3402 - val_acc: 0.8423\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8791 - val_loss: 0.3178 - val_acc: 0.8463\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8791 - val_loss: 0.3399 - val_acc: 0.8372\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8788 - val_loss: 0.3330 - val_acc: 0.8418\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8781 - val_loss: 0.3319 - val_acc: 0.8423\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8788 - val_loss: 0.3237 - val_acc: 0.8469\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8789 - val_loss: 0.3414 - val_acc: 0.8412\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8792 - val_loss: 0.3264 - val_acc: 0.8410\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8792 - val_loss: 0.3307 - val_acc: 0.8461\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8788 - val_loss: 0.3214 - val_acc: 0.8480\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8790 - val_loss: 0.3333 - val_acc: 0.8469\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8788 - val_loss: 0.3235 - val_acc: 0.8449\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8788 - val_loss: 0.3285 - val_acc: 0.8456\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8785 - val_loss: 0.3177 - val_acc: 0.8511\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2980 - acc: 0.8787 - val_loss: 0.3485 - val_acc: 0.8354\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8787 - val_loss: 0.3638 - val_acc: 0.8317\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8782 - val_loss: 0.3321 - val_acc: 0.8423\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8786 - val_loss: 0.3351 - val_acc: 0.8425\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8783 - val_loss: 0.3299 - val_acc: 0.8454\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2976 - acc: 0.8789 - val_loss: 0.3297 - val_acc: 0.8427\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8790 - val_loss: 0.3358 - val_acc: 0.8474\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2983 - acc: 0.8788 - val_loss: 0.3538 - val_acc: 0.8328\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2982 - acc: 0.8790 - val_loss: 0.3132 - val_acc: 0.8527\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8790 - val_loss: 0.3354 - val_acc: 0.8438\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2982 - acc: 0.8783 - val_loss: 0.3459 - val_acc: 0.8405\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2979 - acc: 0.8796 - val_loss: 0.3386 - val_acc: 0.8418\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8799 - val_loss: 0.3282 - val_acc: 0.8480\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8797 - val_loss: 0.3290 - val_acc: 0.8465\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8785 - val_loss: 0.3382 - val_acc: 0.8454\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8794 - val_loss: 0.3347 - val_acc: 0.8461\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2979 - acc: 0.8797 - val_loss: 0.3373 - val_acc: 0.8432\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8788 - val_loss: 0.3442 - val_acc: 0.8401\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8789 - val_loss: 0.3139 - val_acc: 0.8494\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2981 - acc: 0.8790 - val_loss: 0.3299 - val_acc: 0.8461\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8792 - val_loss: 0.3186 - val_acc: 0.8507\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8786 - val_loss: 0.3531 - val_acc: 0.8372\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2973 - acc: 0.8786 - val_loss: 0.3393 - val_acc: 0.8410\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2974 - acc: 0.8790 - val_loss: 0.3423 - val_acc: 0.8376\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2973 - acc: 0.8791 - val_loss: 0.3331 - val_acc: 0.8421\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2972 - acc: 0.8786 - val_loss: 0.3396 - val_acc: 0.8414\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2972 - acc: 0.8796 - val_loss: 0.3250 - val_acc: 0.8463\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2978 - acc: 0.8790 - val_loss: 0.3403 - val_acc: 0.8421\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2976 - acc: 0.8784 - val_loss: 0.3241 - val_acc: 0.8430\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8785 - val_loss: 0.3459 - val_acc: 0.8372\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8778 - val_loss: 0.3554 - val_acc: 0.8370\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8797 - val_loss: 0.3359 - val_acc: 0.8370\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8794 - val_loss: 0.3284 - val_acc: 0.8507\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8786 - val_loss: 0.3397 - val_acc: 0.8388\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2978 - acc: 0.8790 - val_loss: 0.3333 - val_acc: 0.8423\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2979 - acc: 0.8785 - val_loss: 0.3342 - val_acc: 0.8447\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2975 - acc: 0.8793 - val_loss: 0.3426 - val_acc: 0.8365\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2971 - acc: 0.8788 - val_loss: 0.3405 - val_acc: 0.8425\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2976 - acc: 0.8792 - val_loss: 0.3360 - val_acc: 0.8425\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2977 - acc: 0.8794 - val_loss: 0.3376 - val_acc: 0.8438\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8786 - val_loss: 0.3223 - val_acc: 0.8503\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2974 - acc: 0.8792 - val_loss: 0.3409 - val_acc: 0.8403\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8789 - val_loss: 0.3333 - val_acc: 0.8390\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8797 - val_loss: 0.3352 - val_acc: 0.8416\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2975 - acc: 0.8793 - val_loss: 0.3318 - val_acc: 0.8441\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8789 - val_loss: 0.3221 - val_acc: 0.8496\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2970 - acc: 0.8788 - val_loss: 0.3272 - val_acc: 0.8500\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8800 - val_loss: 0.3232 - val_acc: 0.8476\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8788 - val_loss: 0.3377 - val_acc: 0.8445\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2973 - acc: 0.8793 - val_loss: 0.3265 - val_acc: 0.8489\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2973 - acc: 0.8791 - val_loss: 0.3354 - val_acc: 0.8412\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8793 - val_loss: 0.3390 - val_acc: 0.8434\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2970 - acc: 0.8795 - val_loss: 0.3258 - val_acc: 0.8496\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8792 - val_loss: 0.3222 - val_acc: 0.8480\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8788 - val_loss: 0.3359 - val_acc: 0.8478\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8797 - val_loss: 0.3299 - val_acc: 0.8478\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2971 - acc: 0.8791 - val_loss: 0.3145 - val_acc: 0.8549\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8797 - val_loss: 0.3416 - val_acc: 0.8423\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8792 - val_loss: 0.3239 - val_acc: 0.8474\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8796 - val_loss: 0.3358 - val_acc: 0.8456\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8791 - val_loss: 0.3365 - val_acc: 0.8465\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8797 - val_loss: 0.3366 - val_acc: 0.8445\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2968 - acc: 0.8798 - val_loss: 0.3293 - val_acc: 0.8480\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8788 - val_loss: 0.3249 - val_acc: 0.8472\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2969 - acc: 0.8793 - val_loss: 0.3252 - val_acc: 0.8496\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8798 - val_loss: 0.3356 - val_acc: 0.8443\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8800 - val_loss: 0.3443 - val_acc: 0.8376\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2970 - acc: 0.8796 - val_loss: 0.3274 - val_acc: 0.8476\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8785 - val_loss: 0.3317 - val_acc: 0.8480\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8802 - val_loss: 0.3562 - val_acc: 0.8381\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8798 - val_loss: 0.3129 - val_acc: 0.8511\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2969 - acc: 0.8795 - val_loss: 0.3206 - val_acc: 0.8507\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8798 - val_loss: 0.3218 - val_acc: 0.8491\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2962 - acc: 0.8800 - val_loss: 0.3056 - val_acc: 0.8560\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2968 - acc: 0.8796 - val_loss: 0.3442 - val_acc: 0.8418\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8799 - val_loss: 0.3268 - val_acc: 0.8443\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8795 - val_loss: 0.3433 - val_acc: 0.8427\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2970 - acc: 0.8791 - val_loss: 0.3433 - val_acc: 0.8452\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8791 - val_loss: 0.3152 - val_acc: 0.8507\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2962 - acc: 0.8808 - val_loss: 0.3248 - val_acc: 0.8489\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8801 - val_loss: 0.3357 - val_acc: 0.8465\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8792 - val_loss: 0.3334 - val_acc: 0.8489\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8801 - val_loss: 0.3353 - val_acc: 0.8452\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8798 - val_loss: 0.3315 - val_acc: 0.8427\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8798 - val_loss: 0.3406 - val_acc: 0.8445\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2962 - acc: 0.8799 - val_loss: 0.3276 - val_acc: 0.8449\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8794 - val_loss: 0.3341 - val_acc: 0.8434\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8796 - val_loss: 0.3233 - val_acc: 0.8476\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8801 - val_loss: 0.3142 - val_acc: 0.8549\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8798 - val_loss: 0.3323 - val_acc: 0.8476\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2967 - acc: 0.8802 - val_loss: 0.3358 - val_acc: 0.8474\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2965 - acc: 0.8797 - val_loss: 0.3185 - val_acc: 0.8507\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8799 - val_loss: 0.3359 - val_acc: 0.8432\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8785 - val_loss: 0.3362 - val_acc: 0.8447\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2965 - acc: 0.8800 - val_loss: 0.3195 - val_acc: 0.8503\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8801 - val_loss: 0.3353 - val_acc: 0.8430\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8802 - val_loss: 0.3338 - val_acc: 0.8449\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2967 - acc: 0.8795 - val_loss: 0.3228 - val_acc: 0.8476\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2970 - acc: 0.8798 - val_loss: 0.3398 - val_acc: 0.8412\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8787 - val_loss: 0.3351 - val_acc: 0.8472\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2968 - acc: 0.8795 - val_loss: 0.3410 - val_acc: 0.8423\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2966 - acc: 0.8798 - val_loss: 0.3343 - val_acc: 0.8500\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2959 - acc: 0.8802 - val_loss: 0.3308 - val_acc: 0.8469\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8799 - val_loss: 0.3257 - val_acc: 0.8469\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2965 - acc: 0.8802 - val_loss: 0.3438 - val_acc: 0.8396\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8806 - val_loss: 0.3321 - val_acc: 0.8376\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2961 - acc: 0.8796 - val_loss: 0.3377 - val_acc: 0.8432\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2969 - acc: 0.8797 - val_loss: 0.3290 - val_acc: 0.8452\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8799 - val_loss: 0.3314 - val_acc: 0.8456\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8800 - val_loss: 0.3338 - val_acc: 0.8441\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8799 - val_loss: 0.3465 - val_acc: 0.8476\n",
            "acc: 84.76%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 22us/step - loss: 0.3547 - acc: 0.8497 - val_loss: 0.4038 - val_acc: 0.8122\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3152 - acc: 0.8712 - val_loss: 0.3738 - val_acc: 0.8186\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3073 - acc: 0.8750 - val_loss: 0.3495 - val_acc: 0.8343\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3036 - acc: 0.8765 - val_loss: 0.3534 - val_acc: 0.8310\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3028 - acc: 0.8769 - val_loss: 0.3506 - val_acc: 0.8385\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3010 - acc: 0.8770 - val_loss: 0.3314 - val_acc: 0.8423\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3008 - acc: 0.8778 - val_loss: 0.3532 - val_acc: 0.8326\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3001 - acc: 0.8783 - val_loss: 0.3578 - val_acc: 0.8312\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2993 - acc: 0.8783 - val_loss: 0.3381 - val_acc: 0.8381\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2995 - acc: 0.8776 - val_loss: 0.3760 - val_acc: 0.8264\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3000 - acc: 0.8778 - val_loss: 0.3466 - val_acc: 0.8383\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8788 - val_loss: 0.3710 - val_acc: 0.8259\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8782 - val_loss: 0.3404 - val_acc: 0.8368\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8786 - val_loss: 0.3563 - val_acc: 0.8279\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8790 - val_loss: 0.3611 - val_acc: 0.8290\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8790 - val_loss: 0.3566 - val_acc: 0.8392\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8780 - val_loss: 0.3970 - val_acc: 0.8197\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2980 - acc: 0.8787 - val_loss: 0.3543 - val_acc: 0.8319\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8782 - val_loss: 0.3300 - val_acc: 0.8467\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8786 - val_loss: 0.3621 - val_acc: 0.8321\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8788 - val_loss: 0.3286 - val_acc: 0.8443\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2972 - acc: 0.8799 - val_loss: 0.3357 - val_acc: 0.8376\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8785 - val_loss: 0.3494 - val_acc: 0.8350\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8789 - val_loss: 0.3389 - val_acc: 0.8407\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8785 - val_loss: 0.3352 - val_acc: 0.8447\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8793 - val_loss: 0.3502 - val_acc: 0.8392\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8794 - val_loss: 0.3502 - val_acc: 0.8312\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2972 - acc: 0.8799 - val_loss: 0.3388 - val_acc: 0.8388\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2976 - acc: 0.8796 - val_loss: 0.3605 - val_acc: 0.8321\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8798 - val_loss: 0.3308 - val_acc: 0.8388\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8797 - val_loss: 0.3471 - val_acc: 0.8390\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8804 - val_loss: 0.3505 - val_acc: 0.8319\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2961 - acc: 0.8808 - val_loss: 0.3563 - val_acc: 0.8372\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8807 - val_loss: 0.3748 - val_acc: 0.8328\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2957 - acc: 0.8807 - val_loss: 0.3415 - val_acc: 0.8352\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2957 - acc: 0.8805 - val_loss: 0.3644 - val_acc: 0.8299\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8804 - val_loss: 0.3503 - val_acc: 0.8372\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2957 - acc: 0.8805 - val_loss: 0.3569 - val_acc: 0.8303\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2953 - acc: 0.8807 - val_loss: 0.3489 - val_acc: 0.8357\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8809 - val_loss: 0.3230 - val_acc: 0.8516\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2952 - acc: 0.8813 - val_loss: 0.3379 - val_acc: 0.8407\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2950 - acc: 0.8811 - val_loss: 0.3630 - val_acc: 0.8326\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2955 - acc: 0.8810 - val_loss: 0.3408 - val_acc: 0.8432\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2955 - acc: 0.8814 - val_loss: 0.3533 - val_acc: 0.8334\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2944 - acc: 0.8822 - val_loss: 0.3581 - val_acc: 0.8341\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8815 - val_loss: 0.3641 - val_acc: 0.8284\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2951 - acc: 0.8822 - val_loss: 0.3337 - val_acc: 0.8412\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8819 - val_loss: 0.3783 - val_acc: 0.8370\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8812 - val_loss: 0.3311 - val_acc: 0.8507\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2948 - acc: 0.8824 - val_loss: 0.3295 - val_acc: 0.8485\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2946 - acc: 0.8826 - val_loss: 0.3525 - val_acc: 0.8368\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2949 - acc: 0.8818 - val_loss: 0.3435 - val_acc: 0.8399\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8826 - val_loss: 0.3689 - val_acc: 0.8345\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8821 - val_loss: 0.3565 - val_acc: 0.8370\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8831 - val_loss: 0.3330 - val_acc: 0.8461\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8818 - val_loss: 0.3707 - val_acc: 0.8330\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8828 - val_loss: 0.3561 - val_acc: 0.8399\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2945 - acc: 0.8816 - val_loss: 0.3606 - val_acc: 0.8317\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8823 - val_loss: 0.3847 - val_acc: 0.8284\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8825 - val_loss: 0.3512 - val_acc: 0.8361\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2947 - acc: 0.8815 - val_loss: 0.3640 - val_acc: 0.8328\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2935 - acc: 0.8828 - val_loss: 0.3361 - val_acc: 0.8469\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2945 - acc: 0.8818 - val_loss: 0.3710 - val_acc: 0.8310\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8809 - val_loss: 0.3341 - val_acc: 0.8463\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8823 - val_loss: 0.3408 - val_acc: 0.8434\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8818 - val_loss: 0.3748 - val_acc: 0.8317\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8825 - val_loss: 0.3403 - val_acc: 0.8385\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8812 - val_loss: 0.3591 - val_acc: 0.8345\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8820 - val_loss: 0.3456 - val_acc: 0.8432\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2933 - acc: 0.8828 - val_loss: 0.3345 - val_acc: 0.8441\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8815 - val_loss: 0.3791 - val_acc: 0.8246\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8812 - val_loss: 0.3341 - val_acc: 0.8405\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8810 - val_loss: 0.3382 - val_acc: 0.8410\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8821 - val_loss: 0.3323 - val_acc: 0.8449\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8814 - val_loss: 0.3482 - val_acc: 0.8374\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2933 - acc: 0.8828 - val_loss: 0.3528 - val_acc: 0.8343\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8820 - val_loss: 0.3471 - val_acc: 0.8396\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8820 - val_loss: 0.3739 - val_acc: 0.8317\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8814 - val_loss: 0.3375 - val_acc: 0.8396\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8823 - val_loss: 0.3418 - val_acc: 0.8421\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8815 - val_loss: 0.3671 - val_acc: 0.8352\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8819 - val_loss: 0.3360 - val_acc: 0.8474\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8821 - val_loss: 0.3570 - val_acc: 0.8359\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8818 - val_loss: 0.3561 - val_acc: 0.8363\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8810 - val_loss: 0.3415 - val_acc: 0.8452\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8824 - val_loss: 0.3441 - val_acc: 0.8436\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2941 - acc: 0.8821 - val_loss: 0.3579 - val_acc: 0.8354\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8820 - val_loss: 0.3479 - val_acc: 0.8423\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8819 - val_loss: 0.3388 - val_acc: 0.8421\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8820 - val_loss: 0.3653 - val_acc: 0.8339\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8825 - val_loss: 0.3626 - val_acc: 0.8385\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2942 - acc: 0.8818 - val_loss: 0.3283 - val_acc: 0.8447\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8811 - val_loss: 0.3341 - val_acc: 0.8491\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8817 - val_loss: 0.3484 - val_acc: 0.8432\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8814 - val_loss: 0.3451 - val_acc: 0.8412\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2937 - acc: 0.8824 - val_loss: 0.3323 - val_acc: 0.8469\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8813 - val_loss: 0.3291 - val_acc: 0.8494\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8816 - val_loss: 0.3661 - val_acc: 0.8317\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8814 - val_loss: 0.3415 - val_acc: 0.8436\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8817 - val_loss: 0.3510 - val_acc: 0.8370\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8812 - val_loss: 0.3470 - val_acc: 0.8430\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2931 - acc: 0.8816 - val_loss: 0.3558 - val_acc: 0.8385\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8820 - val_loss: 0.3520 - val_acc: 0.8361\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8814 - val_loss: 0.3389 - val_acc: 0.8454\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8820 - val_loss: 0.3647 - val_acc: 0.8348\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8828 - val_loss: 0.3524 - val_acc: 0.8418\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2945 - acc: 0.8811 - val_loss: 0.3509 - val_acc: 0.8427\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8820 - val_loss: 0.3533 - val_acc: 0.8410\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8816 - val_loss: 0.3339 - val_acc: 0.8458\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8824 - val_loss: 0.3441 - val_acc: 0.8425\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2937 - acc: 0.8809 - val_loss: 0.3470 - val_acc: 0.8416\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8818 - val_loss: 0.3392 - val_acc: 0.8405\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8815 - val_loss: 0.3542 - val_acc: 0.8385\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8817 - val_loss: 0.3502 - val_acc: 0.8407\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8813 - val_loss: 0.3445 - val_acc: 0.8434\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8809 - val_loss: 0.3480 - val_acc: 0.8425\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8815 - val_loss: 0.3481 - val_acc: 0.8407\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2948 - acc: 0.8813 - val_loss: 0.3499 - val_acc: 0.8443\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2935 - acc: 0.8817 - val_loss: 0.3582 - val_acc: 0.8334\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8818 - val_loss: 0.3629 - val_acc: 0.8376\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2942 - acc: 0.8808 - val_loss: 0.3572 - val_acc: 0.8368\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2936 - acc: 0.8819 - val_loss: 0.3531 - val_acc: 0.8401\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8813 - val_loss: 0.3542 - val_acc: 0.8376\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2942 - acc: 0.8820 - val_loss: 0.3521 - val_acc: 0.8445\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8817 - val_loss: 0.3363 - val_acc: 0.8480\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8821 - val_loss: 0.3545 - val_acc: 0.8390\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2945 - acc: 0.8808 - val_loss: 0.3634 - val_acc: 0.8334\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2941 - acc: 0.8823 - val_loss: 0.3602 - val_acc: 0.8365\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2939 - acc: 0.8819 - val_loss: 0.3749 - val_acc: 0.8319\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2930 - acc: 0.8816 - val_loss: 0.3678 - val_acc: 0.8337\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8817 - val_loss: 0.3813 - val_acc: 0.8299\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8814 - val_loss: 0.3761 - val_acc: 0.8317\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8820 - val_loss: 0.3479 - val_acc: 0.8405\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2932 - acc: 0.8827 - val_loss: 0.3445 - val_acc: 0.8430\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2935 - acc: 0.8819 - val_loss: 0.3330 - val_acc: 0.8491\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2939 - acc: 0.8815 - val_loss: 0.3377 - val_acc: 0.8445\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8808 - val_loss: 0.3533 - val_acc: 0.8359\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2934 - acc: 0.8822 - val_loss: 0.3324 - val_acc: 0.8500\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8820 - val_loss: 0.3639 - val_acc: 0.8365\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8816 - val_loss: 0.3620 - val_acc: 0.8372\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2933 - acc: 0.8816 - val_loss: 0.3514 - val_acc: 0.8392\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2937 - acc: 0.8817 - val_loss: 0.3680 - val_acc: 0.8361\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2936 - acc: 0.8816 - val_loss: 0.3550 - val_acc: 0.8412\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8820 - val_loss: 0.3501 - val_acc: 0.8414\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8818 - val_loss: 0.3268 - val_acc: 0.8472\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2933 - acc: 0.8822 - val_loss: 0.3705 - val_acc: 0.8301\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8819 - val_loss: 0.3621 - val_acc: 0.8354\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8814 - val_loss: 0.3700 - val_acc: 0.8354\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8825 - val_loss: 0.3541 - val_acc: 0.8392\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2932 - acc: 0.8830 - val_loss: 0.3505 - val_acc: 0.8434\n",
            "acc: 84.34%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 23us/step - loss: 0.3483 - acc: 0.8532 - val_loss: 0.3456 - val_acc: 0.8410\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3101 - acc: 0.8749 - val_loss: 0.3368 - val_acc: 0.8454\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8767 - val_loss: 0.3140 - val_acc: 0.8538\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3006 - acc: 0.8783 - val_loss: 0.3324 - val_acc: 0.8458\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2997 - acc: 0.8789 - val_loss: 0.3275 - val_acc: 0.8441\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2978 - acc: 0.8798 - val_loss: 0.3504 - val_acc: 0.8410\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2970 - acc: 0.8801 - val_loss: 0.3484 - val_acc: 0.8368\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8802 - val_loss: 0.3228 - val_acc: 0.8503\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8801 - val_loss: 0.3250 - val_acc: 0.8525\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2955 - acc: 0.8808 - val_loss: 0.3178 - val_acc: 0.8536\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2950 - acc: 0.8797 - val_loss: 0.3338 - val_acc: 0.8452\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2944 - acc: 0.8804 - val_loss: 0.3283 - val_acc: 0.8494\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2944 - acc: 0.8816 - val_loss: 0.3436 - val_acc: 0.8472\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8816 - val_loss: 0.3201 - val_acc: 0.8509\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2944 - acc: 0.8809 - val_loss: 0.3361 - val_acc: 0.8432\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8820 - val_loss: 0.3522 - val_acc: 0.8394\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2935 - acc: 0.8814 - val_loss: 0.3250 - val_acc: 0.8474\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8814 - val_loss: 0.3241 - val_acc: 0.8480\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8818 - val_loss: 0.3280 - val_acc: 0.8480\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8829 - val_loss: 0.3349 - val_acc: 0.8456\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8815 - val_loss: 0.3698 - val_acc: 0.8308\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8817 - val_loss: 0.3436 - val_acc: 0.8458\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2933 - acc: 0.8817 - val_loss: 0.3310 - val_acc: 0.8445\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8819 - val_loss: 0.3168 - val_acc: 0.8553\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2926 - acc: 0.8818 - val_loss: 0.3362 - val_acc: 0.8407\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2929 - acc: 0.8816 - val_loss: 0.3203 - val_acc: 0.8516\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2926 - acc: 0.8821 - val_loss: 0.3127 - val_acc: 0.8518\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3332 - val_acc: 0.8461\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2926 - acc: 0.8828 - val_loss: 0.3395 - val_acc: 0.8449\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2931 - acc: 0.8826 - val_loss: 0.3395 - val_acc: 0.8454\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2925 - acc: 0.8825 - val_loss: 0.3401 - val_acc: 0.8425\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2924 - acc: 0.8827 - val_loss: 0.3352 - val_acc: 0.8456\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2921 - acc: 0.8825 - val_loss: 0.3376 - val_acc: 0.8465\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8822 - val_loss: 0.3320 - val_acc: 0.8483\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2927 - acc: 0.8825 - val_loss: 0.3226 - val_acc: 0.8467\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2924 - acc: 0.8831 - val_loss: 0.3271 - val_acc: 0.8584\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2922 - acc: 0.8829 - val_loss: 0.3373 - val_acc: 0.8452\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2929 - acc: 0.8821 - val_loss: 0.3437 - val_acc: 0.8425\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2919 - acc: 0.8833 - val_loss: 0.3295 - val_acc: 0.8500\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8831 - val_loss: 0.3357 - val_acc: 0.8483\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8831 - val_loss: 0.3172 - val_acc: 0.8525\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2923 - acc: 0.8826 - val_loss: 0.3257 - val_acc: 0.8494\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2920 - acc: 0.8835 - val_loss: 0.3140 - val_acc: 0.8520\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3339 - val_acc: 0.8434\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2916 - acc: 0.8830 - val_loss: 0.3362 - val_acc: 0.8527\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2918 - acc: 0.8833 - val_loss: 0.3207 - val_acc: 0.8551\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3367 - val_acc: 0.8456\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2917 - acc: 0.8832 - val_loss: 0.3288 - val_acc: 0.8445\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2916 - acc: 0.8833 - val_loss: 0.3235 - val_acc: 0.8538\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2917 - acc: 0.8830 - val_loss: 0.3351 - val_acc: 0.8496\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2916 - acc: 0.8832 - val_loss: 0.3216 - val_acc: 0.8498\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2919 - acc: 0.8828 - val_loss: 0.3177 - val_acc: 0.8536\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8832 - val_loss: 0.3186 - val_acc: 0.8576\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8830 - val_loss: 0.3223 - val_acc: 0.8514\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8835 - val_loss: 0.3287 - val_acc: 0.8500\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8839 - val_loss: 0.3268 - val_acc: 0.8556\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2915 - acc: 0.8830 - val_loss: 0.3300 - val_acc: 0.8496\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8830 - val_loss: 0.3339 - val_acc: 0.8491\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2916 - acc: 0.8841 - val_loss: 0.3089 - val_acc: 0.8604\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8843 - val_loss: 0.3164 - val_acc: 0.8487\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8838 - val_loss: 0.3359 - val_acc: 0.8483\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2913 - acc: 0.8834 - val_loss: 0.3315 - val_acc: 0.8487\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8837 - val_loss: 0.3250 - val_acc: 0.8505\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8833 - val_loss: 0.3278 - val_acc: 0.8514\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2917 - acc: 0.8831 - val_loss: 0.3158 - val_acc: 0.8558\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8842 - val_loss: 0.3470 - val_acc: 0.8432\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8835 - val_loss: 0.3227 - val_acc: 0.8556\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8836 - val_loss: 0.3164 - val_acc: 0.8602\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2917 - acc: 0.8835 - val_loss: 0.3329 - val_acc: 0.8500\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8833 - val_loss: 0.3069 - val_acc: 0.8540\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3219 - val_acc: 0.8527\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8837 - val_loss: 0.3297 - val_acc: 0.8562\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2907 - acc: 0.8841 - val_loss: 0.3411 - val_acc: 0.8478\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8835 - val_loss: 0.3348 - val_acc: 0.8483\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8836 - val_loss: 0.3270 - val_acc: 0.8487\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8838 - val_loss: 0.3185 - val_acc: 0.8567\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8833 - val_loss: 0.3246 - val_acc: 0.8518\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8842 - val_loss: 0.3326 - val_acc: 0.8511\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8834 - val_loss: 0.3337 - val_acc: 0.8522\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8840 - val_loss: 0.3298 - val_acc: 0.8531\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8833 - val_loss: 0.3212 - val_acc: 0.8556\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8842 - val_loss: 0.3490 - val_acc: 0.8416\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8842 - val_loss: 0.3289 - val_acc: 0.8511\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8848 - val_loss: 0.3443 - val_acc: 0.8463\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8846 - val_loss: 0.3266 - val_acc: 0.8516\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8844 - val_loss: 0.3383 - val_acc: 0.8474\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8835 - val_loss: 0.3307 - val_acc: 0.8485\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8847 - val_loss: 0.3261 - val_acc: 0.8514\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8848 - val_loss: 0.3343 - val_acc: 0.8511\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2913 - acc: 0.8842 - val_loss: 0.3359 - val_acc: 0.8500\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8840 - val_loss: 0.3179 - val_acc: 0.8511\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8850 - val_loss: 0.3255 - val_acc: 0.8542\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2910 - acc: 0.8838 - val_loss: 0.3205 - val_acc: 0.8576\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8839 - val_loss: 0.3487 - val_acc: 0.8454\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8845 - val_loss: 0.3245 - val_acc: 0.8498\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8839 - val_loss: 0.3394 - val_acc: 0.8507\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2907 - acc: 0.8840 - val_loss: 0.3417 - val_acc: 0.8461\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2915 - acc: 0.8837 - val_loss: 0.3127 - val_acc: 0.8553\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8845 - val_loss: 0.3315 - val_acc: 0.8498\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8847 - val_loss: 0.3418 - val_acc: 0.8483\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2912 - acc: 0.8838 - val_loss: 0.3342 - val_acc: 0.8527\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8845 - val_loss: 0.3349 - val_acc: 0.8467\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8845 - val_loss: 0.3116 - val_acc: 0.8567\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8839 - val_loss: 0.3372 - val_acc: 0.8469\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8841 - val_loss: 0.3442 - val_acc: 0.8489\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8843 - val_loss: 0.3267 - val_acc: 0.8496\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8843 - val_loss: 0.3274 - val_acc: 0.8527\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8843 - val_loss: 0.3274 - val_acc: 0.8514\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8846 - val_loss: 0.3249 - val_acc: 0.8516\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8846 - val_loss: 0.3211 - val_acc: 0.8547\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8847 - val_loss: 0.3257 - val_acc: 0.8529\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3188 - val_acc: 0.8540\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8841 - val_loss: 0.3358 - val_acc: 0.8494\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8837 - val_loss: 0.3409 - val_acc: 0.8456\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8842 - val_loss: 0.3374 - val_acc: 0.8505\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8839 - val_loss: 0.3264 - val_acc: 0.8483\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8846 - val_loss: 0.3497 - val_acc: 0.8425\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8842 - val_loss: 0.3310 - val_acc: 0.8478\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3476 - val_acc: 0.8480\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8841 - val_loss: 0.3255 - val_acc: 0.8529\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8836 - val_loss: 0.3343 - val_acc: 0.8505\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8840 - val_loss: 0.3393 - val_acc: 0.8540\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8837 - val_loss: 0.3289 - val_acc: 0.8553\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8836 - val_loss: 0.3060 - val_acc: 0.8569\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8848 - val_loss: 0.3217 - val_acc: 0.8547\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8844 - val_loss: 0.3367 - val_acc: 0.8500\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8847 - val_loss: 0.3364 - val_acc: 0.8474\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8842 - val_loss: 0.3130 - val_acc: 0.8593\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8841 - val_loss: 0.3301 - val_acc: 0.8522\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2914 - acc: 0.8844 - val_loss: 0.3390 - val_acc: 0.8511\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8846 - val_loss: 0.3186 - val_acc: 0.8573\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2913 - acc: 0.8841 - val_loss: 0.3210 - val_acc: 0.8567\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8841 - val_loss: 0.3500 - val_acc: 0.8449\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8837 - val_loss: 0.3184 - val_acc: 0.8540\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2913 - acc: 0.8842 - val_loss: 0.3277 - val_acc: 0.8529\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8847 - val_loss: 0.3387 - val_acc: 0.8458\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8839 - val_loss: 0.3322 - val_acc: 0.8534\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8841 - val_loss: 0.3190 - val_acc: 0.8564\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8847 - val_loss: 0.3138 - val_acc: 0.8595\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8845 - val_loss: 0.3356 - val_acc: 0.8494\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2905 - acc: 0.8847 - val_loss: 0.3137 - val_acc: 0.8604\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8840 - val_loss: 0.3288 - val_acc: 0.8551\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8849 - val_loss: 0.3177 - val_acc: 0.8569\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8841 - val_loss: 0.3382 - val_acc: 0.8500\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8848 - val_loss: 0.3182 - val_acc: 0.8540\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8839 - val_loss: 0.3298 - val_acc: 0.8509\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8840 - val_loss: 0.3446 - val_acc: 0.8498\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8850 - val_loss: 0.3348 - val_acc: 0.8509\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8844 - val_loss: 0.3368 - val_acc: 0.8518\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8854 - val_loss: 0.3283 - val_acc: 0.8514\n",
            "acc: 85.14%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 25us/step - loss: 0.3595 - acc: 0.8439 - val_loss: 0.3807 - val_acc: 0.8204\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3189 - acc: 0.8680 - val_loss: 0.3789 - val_acc: 0.8237\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3086 - acc: 0.8740 - val_loss: 0.3560 - val_acc: 0.8330\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3043 - acc: 0.8750 - val_loss: 0.3656 - val_acc: 0.8275\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3001 - acc: 0.8784 - val_loss: 0.3451 - val_acc: 0.8461\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8795 - val_loss: 0.3757 - val_acc: 0.8288\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8797 - val_loss: 0.3649 - val_acc: 0.8350\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8804 - val_loss: 0.3478 - val_acc: 0.8434\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2950 - acc: 0.8819 - val_loss: 0.3505 - val_acc: 0.8432\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8810 - val_loss: 0.3642 - val_acc: 0.8414\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2946 - acc: 0.8810 - val_loss: 0.3348 - val_acc: 0.8494\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8818 - val_loss: 0.3563 - val_acc: 0.8432\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2929 - acc: 0.8826 - val_loss: 0.3806 - val_acc: 0.8310\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2921 - acc: 0.8821 - val_loss: 0.3814 - val_acc: 0.8341\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2917 - acc: 0.8825 - val_loss: 0.3445 - val_acc: 0.8421\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8821 - val_loss: 0.3487 - val_acc: 0.8452\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8828 - val_loss: 0.3438 - val_acc: 0.8496\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8832 - val_loss: 0.3349 - val_acc: 0.8527\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8821 - val_loss: 0.3602 - val_acc: 0.8458\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8832 - val_loss: 0.3511 - val_acc: 0.8478\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8830 - val_loss: 0.3601 - val_acc: 0.8394\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8841 - val_loss: 0.3532 - val_acc: 0.8436\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2905 - acc: 0.8831 - val_loss: 0.3636 - val_acc: 0.8432\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8832 - val_loss: 0.3803 - val_acc: 0.8354\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8834 - val_loss: 0.3613 - val_acc: 0.8434\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8824 - val_loss: 0.3623 - val_acc: 0.8430\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2900 - acc: 0.8832 - val_loss: 0.3481 - val_acc: 0.8489\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8837 - val_loss: 0.3534 - val_acc: 0.8489\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8826 - val_loss: 0.3421 - val_acc: 0.8489\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8836 - val_loss: 0.3370 - val_acc: 0.8507\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8827 - val_loss: 0.3662 - val_acc: 0.8381\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3536 - val_acc: 0.8467\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8834 - val_loss: 0.3511 - val_acc: 0.8388\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8842 - val_loss: 0.3725 - val_acc: 0.8388\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8826 - val_loss: 0.3424 - val_acc: 0.8465\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8827 - val_loss: 0.3737 - val_acc: 0.8385\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8835 - val_loss: 0.3294 - val_acc: 0.8545\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2894 - acc: 0.8829 - val_loss: 0.3792 - val_acc: 0.8383\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8839 - val_loss: 0.3470 - val_acc: 0.8500\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8834 - val_loss: 0.3525 - val_acc: 0.8423\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8836 - val_loss: 0.3588 - val_acc: 0.8425\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8839 - val_loss: 0.3620 - val_acc: 0.8458\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8832 - val_loss: 0.3589 - val_acc: 0.8412\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8839 - val_loss: 0.3529 - val_acc: 0.8443\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8834 - val_loss: 0.3527 - val_acc: 0.8445\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8838 - val_loss: 0.3494 - val_acc: 0.8507\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8829 - val_loss: 0.3498 - val_acc: 0.8483\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8834 - val_loss: 0.3375 - val_acc: 0.8514\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8838 - val_loss: 0.3745 - val_acc: 0.8352\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8837 - val_loss: 0.3567 - val_acc: 0.8443\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8831 - val_loss: 0.3633 - val_acc: 0.8385\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8836 - val_loss: 0.3509 - val_acc: 0.8396\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8833 - val_loss: 0.3402 - val_acc: 0.8465\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8833 - val_loss: 0.3324 - val_acc: 0.8525\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8844 - val_loss: 0.3488 - val_acc: 0.8401\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8832 - val_loss: 0.3735 - val_acc: 0.8381\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8837 - val_loss: 0.3552 - val_acc: 0.8423\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2894 - acc: 0.8836 - val_loss: 0.3539 - val_acc: 0.8441\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8830 - val_loss: 0.3442 - val_acc: 0.8522\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8836 - val_loss: 0.3691 - val_acc: 0.8388\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2894 - acc: 0.8842 - val_loss: 0.3539 - val_acc: 0.8407\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2901 - acc: 0.8833 - val_loss: 0.3546 - val_acc: 0.8463\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8831 - val_loss: 0.3643 - val_acc: 0.8401\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3605 - val_acc: 0.8438\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8825 - val_loss: 0.3424 - val_acc: 0.8476\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8830 - val_loss: 0.3584 - val_acc: 0.8472\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2892 - acc: 0.8847 - val_loss: 0.3771 - val_acc: 0.8372\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8842 - val_loss: 0.3451 - val_acc: 0.8467\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8838 - val_loss: 0.3451 - val_acc: 0.8503\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8833 - val_loss: 0.3373 - val_acc: 0.8536\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2898 - acc: 0.8827 - val_loss: 0.3663 - val_acc: 0.8427\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3828 - val_acc: 0.8401\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2895 - acc: 0.8835 - val_loss: 0.3599 - val_acc: 0.8396\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8833 - val_loss: 0.3750 - val_acc: 0.8345\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8840 - val_loss: 0.3286 - val_acc: 0.8534\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8830 - val_loss: 0.3854 - val_acc: 0.8337\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8834 - val_loss: 0.3435 - val_acc: 0.8478\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8832 - val_loss: 0.3429 - val_acc: 0.8564\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8832 - val_loss: 0.3641 - val_acc: 0.8407\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8827 - val_loss: 0.3365 - val_acc: 0.8498\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8838 - val_loss: 0.3521 - val_acc: 0.8487\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8842 - val_loss: 0.3430 - val_acc: 0.8480\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8829 - val_loss: 0.3538 - val_acc: 0.8456\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8829 - val_loss: 0.3315 - val_acc: 0.8485\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3859 - val_acc: 0.8330\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2897 - acc: 0.8832 - val_loss: 0.3441 - val_acc: 0.8498\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2888 - acc: 0.8839 - val_loss: 0.3278 - val_acc: 0.8514\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2903 - acc: 0.8836 - val_loss: 0.3357 - val_acc: 0.8498\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2899 - acc: 0.8832 - val_loss: 0.3246 - val_acc: 0.8483\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2896 - acc: 0.8831 - val_loss: 0.3453 - val_acc: 0.8449\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2896 - acc: 0.8831 - val_loss: 0.3523 - val_acc: 0.8485\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8833 - val_loss: 0.3654 - val_acc: 0.8412\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8838 - val_loss: 0.3614 - val_acc: 0.8421\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8843 - val_loss: 0.3353 - val_acc: 0.8478\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8837 - val_loss: 0.3456 - val_acc: 0.8500\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8836 - val_loss: 0.3510 - val_acc: 0.8410\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8834 - val_loss: 0.3729 - val_acc: 0.8323\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8839 - val_loss: 0.3537 - val_acc: 0.8418\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2893 - acc: 0.8847 - val_loss: 0.3489 - val_acc: 0.8454\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8837 - val_loss: 0.3469 - val_acc: 0.8494\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8844 - val_loss: 0.3376 - val_acc: 0.8438\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8831 - val_loss: 0.3633 - val_acc: 0.8458\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8837 - val_loss: 0.3427 - val_acc: 0.8496\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8844 - val_loss: 0.3664 - val_acc: 0.8405\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8839 - val_loss: 0.3465 - val_acc: 0.8432\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3671 - val_acc: 0.8432\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8837 - val_loss: 0.3372 - val_acc: 0.8536\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8835 - val_loss: 0.3705 - val_acc: 0.8385\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8836 - val_loss: 0.3489 - val_acc: 0.8476\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8837 - val_loss: 0.3534 - val_acc: 0.8438\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8831 - val_loss: 0.3374 - val_acc: 0.8489\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2900 - acc: 0.8840 - val_loss: 0.3259 - val_acc: 0.8569\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2888 - acc: 0.8841 - val_loss: 0.3325 - val_acc: 0.8498\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8840 - val_loss: 0.3709 - val_acc: 0.8381\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8845 - val_loss: 0.3624 - val_acc: 0.8405\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3402 - val_acc: 0.8518\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8837 - val_loss: 0.3453 - val_acc: 0.8536\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8836 - val_loss: 0.3478 - val_acc: 0.8467\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8841 - val_loss: 0.3452 - val_acc: 0.8463\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8840 - val_loss: 0.3313 - val_acc: 0.8529\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8851 - val_loss: 0.3469 - val_acc: 0.8416\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2894 - acc: 0.8834 - val_loss: 0.3452 - val_acc: 0.8496\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8834 - val_loss: 0.3464 - val_acc: 0.8441\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3594 - val_acc: 0.8407\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8836 - val_loss: 0.3312 - val_acc: 0.8489\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8842 - val_loss: 0.3380 - val_acc: 0.8498\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8841 - val_loss: 0.3598 - val_acc: 0.8436\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8836 - val_loss: 0.3572 - val_acc: 0.8443\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8845 - val_loss: 0.3389 - val_acc: 0.8551\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3621 - val_acc: 0.8418\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8843 - val_loss: 0.3481 - val_acc: 0.8423\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8839 - val_loss: 0.3457 - val_acc: 0.8483\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8836 - val_loss: 0.3509 - val_acc: 0.8452\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8837 - val_loss: 0.3703 - val_acc: 0.8421\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8845 - val_loss: 0.3498 - val_acc: 0.8474\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8834 - val_loss: 0.3584 - val_acc: 0.8418\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8835 - val_loss: 0.3549 - val_acc: 0.8414\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8843 - val_loss: 0.3454 - val_acc: 0.8447\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8838 - val_loss: 0.3489 - val_acc: 0.8416\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8831 - val_loss: 0.3329 - val_acc: 0.8518\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8837 - val_loss: 0.3533 - val_acc: 0.8456\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8836 - val_loss: 0.3586 - val_acc: 0.8441\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8839 - val_loss: 0.3518 - val_acc: 0.8478\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8837 - val_loss: 0.3567 - val_acc: 0.8434\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8830 - val_loss: 0.3491 - val_acc: 0.8472\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8834 - val_loss: 0.3401 - val_acc: 0.8498\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2888 - acc: 0.8838 - val_loss: 0.3690 - val_acc: 0.8379\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8839 - val_loss: 0.3471 - val_acc: 0.8483\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8834 - val_loss: 0.3468 - val_acc: 0.8498\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8831 - val_loss: 0.3842 - val_acc: 0.8368\n",
            "acc: 83.68%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 24us/step - loss: 0.3512 - acc: 0.8532 - val_loss: 0.3655 - val_acc: 0.8321\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3117 - acc: 0.8734 - val_loss: 0.2916 - val_acc: 0.8646\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8752 - val_loss: 0.3357 - val_acc: 0.8403\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8771 - val_loss: 0.3276 - val_acc: 0.8505\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3009 - acc: 0.8786 - val_loss: 0.3062 - val_acc: 0.8677\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8808 - val_loss: 0.3362 - val_acc: 0.8498\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2973 - acc: 0.8820 - val_loss: 0.3152 - val_acc: 0.8591\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8823 - val_loss: 0.3196 - val_acc: 0.8582\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2954 - acc: 0.8836 - val_loss: 0.3203 - val_acc: 0.8633\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8845 - val_loss: 0.3235 - val_acc: 0.8567\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2946 - acc: 0.8843 - val_loss: 0.3406 - val_acc: 0.8573\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2935 - acc: 0.8848 - val_loss: 0.3350 - val_acc: 0.8551\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2927 - acc: 0.8853 - val_loss: 0.3147 - val_acc: 0.8607\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2929 - acc: 0.8845 - val_loss: 0.3234 - val_acc: 0.8576\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8861 - val_loss: 0.3051 - val_acc: 0.8657\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2919 - acc: 0.8859 - val_loss: 0.3104 - val_acc: 0.8613\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8858 - val_loss: 0.3313 - val_acc: 0.8569\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2912 - acc: 0.8863 - val_loss: 0.3038 - val_acc: 0.8673\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8862 - val_loss: 0.3445 - val_acc: 0.8454\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8872 - val_loss: 0.3237 - val_acc: 0.8591\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8869 - val_loss: 0.3002 - val_acc: 0.8682\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8863 - val_loss: 0.3240 - val_acc: 0.8631\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8870 - val_loss: 0.3022 - val_acc: 0.8682\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8865 - val_loss: 0.3417 - val_acc: 0.8545\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8865 - val_loss: 0.3018 - val_acc: 0.8653\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8866 - val_loss: 0.3411 - val_acc: 0.8500\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8870 - val_loss: 0.3090 - val_acc: 0.8564\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8874 - val_loss: 0.3138 - val_acc: 0.8587\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8874 - val_loss: 0.3110 - val_acc: 0.8618\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2899 - acc: 0.8871 - val_loss: 0.3207 - val_acc: 0.8553\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8873 - val_loss: 0.3054 - val_acc: 0.8604\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2901 - acc: 0.8868 - val_loss: 0.3197 - val_acc: 0.8551\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8873 - val_loss: 0.3107 - val_acc: 0.8649\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8871 - val_loss: 0.2859 - val_acc: 0.8708\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8881 - val_loss: 0.3191 - val_acc: 0.8629\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2896 - acc: 0.8873 - val_loss: 0.3166 - val_acc: 0.8613\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8880 - val_loss: 0.3109 - val_acc: 0.8556\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2891 - acc: 0.8881 - val_loss: 0.3329 - val_acc: 0.8547\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2892 - acc: 0.8881 - val_loss: 0.3145 - val_acc: 0.8615\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8884 - val_loss: 0.3089 - val_acc: 0.8598\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8876 - val_loss: 0.3242 - val_acc: 0.8560\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8870 - val_loss: 0.2884 - val_acc: 0.8655\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8875 - val_loss: 0.3104 - val_acc: 0.8642\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8880 - val_loss: 0.3094 - val_acc: 0.8560\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8879 - val_loss: 0.3224 - val_acc: 0.8622\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8879 - val_loss: 0.3298 - val_acc: 0.8589\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8877 - val_loss: 0.3194 - val_acc: 0.8631\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2888 - acc: 0.8880 - val_loss: 0.3191 - val_acc: 0.8571\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8880 - val_loss: 0.3174 - val_acc: 0.8631\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2890 - acc: 0.8872 - val_loss: 0.3312 - val_acc: 0.8569\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8875 - val_loss: 0.3238 - val_acc: 0.8580\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2883 - acc: 0.8887 - val_loss: 0.3304 - val_acc: 0.8542\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8884 - val_loss: 0.3194 - val_acc: 0.8564\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8870 - val_loss: 0.2914 - val_acc: 0.8668\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8879 - val_loss: 0.3279 - val_acc: 0.8576\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8878 - val_loss: 0.3143 - val_acc: 0.8609\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8880 - val_loss: 0.3124 - val_acc: 0.8651\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8883 - val_loss: 0.3098 - val_acc: 0.8688\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8887 - val_loss: 0.3262 - val_acc: 0.8607\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8877 - val_loss: 0.3098 - val_acc: 0.8604\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8885 - val_loss: 0.3088 - val_acc: 0.8666\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2886 - acc: 0.8876 - val_loss: 0.3174 - val_acc: 0.8629\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8886 - val_loss: 0.3149 - val_acc: 0.8644\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8879 - val_loss: 0.3288 - val_acc: 0.8558\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8883 - val_loss: 0.3123 - val_acc: 0.8640\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8880 - val_loss: 0.3199 - val_acc: 0.8624\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8886 - val_loss: 0.3125 - val_acc: 0.8587\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8880 - val_loss: 0.3278 - val_acc: 0.8549\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8879 - val_loss: 0.3223 - val_acc: 0.8576\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8889 - val_loss: 0.3168 - val_acc: 0.8584\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8894 - val_loss: 0.3059 - val_acc: 0.8651\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8884 - val_loss: 0.3104 - val_acc: 0.8611\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8889 - val_loss: 0.3265 - val_acc: 0.8587\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2880 - acc: 0.8889 - val_loss: 0.3162 - val_acc: 0.8611\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2883 - acc: 0.8890 - val_loss: 0.3302 - val_acc: 0.8542\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8887 - val_loss: 0.3320 - val_acc: 0.8549\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8881 - val_loss: 0.3130 - val_acc: 0.8620\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8885 - val_loss: 0.2996 - val_acc: 0.8655\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8885 - val_loss: 0.3415 - val_acc: 0.8489\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8887 - val_loss: 0.3153 - val_acc: 0.8609\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8878 - val_loss: 0.3006 - val_acc: 0.8640\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8884 - val_loss: 0.3206 - val_acc: 0.8589\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8886 - val_loss: 0.3058 - val_acc: 0.8675\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8886 - val_loss: 0.3159 - val_acc: 0.8644\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8889 - val_loss: 0.3134 - val_acc: 0.8609\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8887 - val_loss: 0.3150 - val_acc: 0.8600\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8883 - val_loss: 0.3031 - val_acc: 0.8609\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8875 - val_loss: 0.3149 - val_acc: 0.8578\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8880 - val_loss: 0.3262 - val_acc: 0.8567\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8891 - val_loss: 0.3088 - val_acc: 0.8651\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8887 - val_loss: 0.3094 - val_acc: 0.8655\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8883 - val_loss: 0.3284 - val_acc: 0.8511\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8893 - val_loss: 0.3117 - val_acc: 0.8587\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8881 - val_loss: 0.3167 - val_acc: 0.8607\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8883 - val_loss: 0.3109 - val_acc: 0.8637\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8887 - val_loss: 0.3263 - val_acc: 0.8538\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8883 - val_loss: 0.3270 - val_acc: 0.8558\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8884 - val_loss: 0.3051 - val_acc: 0.8635\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8893 - val_loss: 0.3404 - val_acc: 0.8556\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8885 - val_loss: 0.3137 - val_acc: 0.8593\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8885 - val_loss: 0.3127 - val_acc: 0.8604\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8883 - val_loss: 0.3146 - val_acc: 0.8591\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8879 - val_loss: 0.3082 - val_acc: 0.8629\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8891 - val_loss: 0.3349 - val_acc: 0.8564\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8884 - val_loss: 0.3123 - val_acc: 0.8622\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8884 - val_loss: 0.3345 - val_acc: 0.8534\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8882 - val_loss: 0.3257 - val_acc: 0.8522\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8886 - val_loss: 0.3083 - val_acc: 0.8595\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8886 - val_loss: 0.3215 - val_acc: 0.8576\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8895 - val_loss: 0.3168 - val_acc: 0.8540\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8888 - val_loss: 0.3127 - val_acc: 0.8598\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8891 - val_loss: 0.3226 - val_acc: 0.8602\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8886 - val_loss: 0.3028 - val_acc: 0.8677\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8881 - val_loss: 0.3102 - val_acc: 0.8651\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8891 - val_loss: 0.3239 - val_acc: 0.8598\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8888 - val_loss: 0.3162 - val_acc: 0.8609\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2878 - acc: 0.8891 - val_loss: 0.3335 - val_acc: 0.8534\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8892 - val_loss: 0.3008 - val_acc: 0.8642\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8875 - val_loss: 0.3228 - val_acc: 0.8560\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8884 - val_loss: 0.2984 - val_acc: 0.8633\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8880 - val_loss: 0.3211 - val_acc: 0.8635\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8888 - val_loss: 0.3006 - val_acc: 0.8651\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8890 - val_loss: 0.3226 - val_acc: 0.8529\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8885 - val_loss: 0.3083 - val_acc: 0.8587\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8892 - val_loss: 0.3244 - val_acc: 0.8540\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8882 - val_loss: 0.3059 - val_acc: 0.8611\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8887 - val_loss: 0.3000 - val_acc: 0.8600\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2881 - acc: 0.8894 - val_loss: 0.3334 - val_acc: 0.8564\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8890 - val_loss: 0.3167 - val_acc: 0.8536\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8884 - val_loss: 0.3224 - val_acc: 0.8582\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8888 - val_loss: 0.3179 - val_acc: 0.8584\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8892 - val_loss: 0.3125 - val_acc: 0.8584\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8888 - val_loss: 0.3114 - val_acc: 0.8571\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8888 - val_loss: 0.3128 - val_acc: 0.8556\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8879 - val_loss: 0.3430 - val_acc: 0.8449\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8889 - val_loss: 0.3392 - val_acc: 0.8527\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8888 - val_loss: 0.3161 - val_acc: 0.8560\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8888 - val_loss: 0.3164 - val_acc: 0.8564\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8888 - val_loss: 0.3254 - val_acc: 0.8540\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8886 - val_loss: 0.3171 - val_acc: 0.8591\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8892 - val_loss: 0.3355 - val_acc: 0.8514\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8886 - val_loss: 0.3026 - val_acc: 0.8600\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8880 - val_loss: 0.3009 - val_acc: 0.8607\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8887 - val_loss: 0.3151 - val_acc: 0.8602\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8893 - val_loss: 0.3326 - val_acc: 0.8547\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8883 - val_loss: 0.3019 - val_acc: 0.8644\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8882 - val_loss: 0.3227 - val_acc: 0.8580\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8887 - val_loss: 0.3086 - val_acc: 0.8609\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8892 - val_loss: 0.3298 - val_acc: 0.8553\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8886 - val_loss: 0.3355 - val_acc: 0.8503\n",
            "acc: 85.03%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 26us/step - loss: 0.3581 - acc: 0.8483 - val_loss: 0.3692 - val_acc: 0.8257\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3207 - acc: 0.8679 - val_loss: 0.3760 - val_acc: 0.8230\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3136 - acc: 0.8702 - val_loss: 0.3624 - val_acc: 0.8368\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3092 - acc: 0.8731 - val_loss: 0.3309 - val_acc: 0.8480\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3058 - acc: 0.8751 - val_loss: 0.3481 - val_acc: 0.8368\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3040 - acc: 0.8766 - val_loss: 0.3847 - val_acc: 0.8312\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.3018 - acc: 0.8782 - val_loss: 0.3731 - val_acc: 0.8277\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2989 - acc: 0.8793 - val_loss: 0.3588 - val_acc: 0.8370\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2981 - acc: 0.8800 - val_loss: 0.3500 - val_acc: 0.8401\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2965 - acc: 0.8800 - val_loss: 0.3371 - val_acc: 0.8472\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2949 - acc: 0.8807 - val_loss: 0.3491 - val_acc: 0.8379\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2952 - acc: 0.8808 - val_loss: 0.3569 - val_acc: 0.8376\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2948 - acc: 0.8815 - val_loss: 0.3270 - val_acc: 0.8516\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2944 - acc: 0.8814 - val_loss: 0.3575 - val_acc: 0.8361\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2933 - acc: 0.8811 - val_loss: 0.3677 - val_acc: 0.8328\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8813 - val_loss: 0.3147 - val_acc: 0.8569\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2935 - acc: 0.8814 - val_loss: 0.3462 - val_acc: 0.8410\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8815 - val_loss: 0.3403 - val_acc: 0.8418\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2935 - acc: 0.8816 - val_loss: 0.3551 - val_acc: 0.8383\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2931 - acc: 0.8816 - val_loss: 0.3717 - val_acc: 0.8306\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2934 - acc: 0.8815 - val_loss: 0.3342 - val_acc: 0.8454\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2931 - acc: 0.8829 - val_loss: 0.3543 - val_acc: 0.8345\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2919 - acc: 0.8822 - val_loss: 0.3568 - val_acc: 0.8350\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2916 - acc: 0.8826 - val_loss: 0.3319 - val_acc: 0.8441\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2912 - acc: 0.8830 - val_loss: 0.3381 - val_acc: 0.8441\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8829 - val_loss: 0.3656 - val_acc: 0.8350\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2912 - acc: 0.8824 - val_loss: 0.3371 - val_acc: 0.8485\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8827 - val_loss: 0.3607 - val_acc: 0.8257\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8830 - val_loss: 0.3458 - val_acc: 0.8394\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8829 - val_loss: 0.3627 - val_acc: 0.8317\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8828 - val_loss: 0.3292 - val_acc: 0.8436\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8826 - val_loss: 0.3454 - val_acc: 0.8376\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8833 - val_loss: 0.3652 - val_acc: 0.8345\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8830 - val_loss: 0.3294 - val_acc: 0.8445\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8835 - val_loss: 0.3555 - val_acc: 0.8383\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8823 - val_loss: 0.3425 - val_acc: 0.8374\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8833 - val_loss: 0.3310 - val_acc: 0.8461\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2891 - acc: 0.8827 - val_loss: 0.3445 - val_acc: 0.8441\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2899 - acc: 0.8824 - val_loss: 0.3237 - val_acc: 0.8514\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2894 - acc: 0.8831 - val_loss: 0.3327 - val_acc: 0.8454\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8838 - val_loss: 0.3602 - val_acc: 0.8363\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8829 - val_loss: 0.3502 - val_acc: 0.8381\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2888 - acc: 0.8833 - val_loss: 0.3314 - val_acc: 0.8494\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8833 - val_loss: 0.3295 - val_acc: 0.8445\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8825 - val_loss: 0.3468 - val_acc: 0.8443\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8828 - val_loss: 0.3455 - val_acc: 0.8436\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8836 - val_loss: 0.3481 - val_acc: 0.8421\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8835 - val_loss: 0.3399 - val_acc: 0.8443\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8835 - val_loss: 0.3498 - val_acc: 0.8394\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8836 - val_loss: 0.3579 - val_acc: 0.8399\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2882 - acc: 0.8833 - val_loss: 0.3230 - val_acc: 0.8496\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2887 - acc: 0.8829 - val_loss: 0.3405 - val_acc: 0.8441\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8839 - val_loss: 0.3516 - val_acc: 0.8392\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8834 - val_loss: 0.3259 - val_acc: 0.8498\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8835 - val_loss: 0.3557 - val_acc: 0.8365\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8841 - val_loss: 0.3506 - val_acc: 0.8383\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8843 - val_loss: 0.3330 - val_acc: 0.8483\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8830 - val_loss: 0.3272 - val_acc: 0.8463\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8828 - val_loss: 0.3330 - val_acc: 0.8445\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8835 - val_loss: 0.3533 - val_acc: 0.8403\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2884 - acc: 0.8836 - val_loss: 0.3433 - val_acc: 0.8445\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8834 - val_loss: 0.3467 - val_acc: 0.8436\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8835 - val_loss: 0.3394 - val_acc: 0.8443\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8842 - val_loss: 0.3519 - val_acc: 0.8370\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8842 - val_loss: 0.3428 - val_acc: 0.8434\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8844 - val_loss: 0.3421 - val_acc: 0.8412\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8839 - val_loss: 0.3420 - val_acc: 0.8456\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8831 - val_loss: 0.3455 - val_acc: 0.8410\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8840 - val_loss: 0.3437 - val_acc: 0.8403\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8838 - val_loss: 0.3332 - val_acc: 0.8480\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8834 - val_loss: 0.3616 - val_acc: 0.8352\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8832 - val_loss: 0.3334 - val_acc: 0.8414\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8836 - val_loss: 0.3381 - val_acc: 0.8461\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8838 - val_loss: 0.3673 - val_acc: 0.8334\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8842 - val_loss: 0.3476 - val_acc: 0.8416\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8844 - val_loss: 0.3279 - val_acc: 0.8474\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2876 - acc: 0.8839 - val_loss: 0.3368 - val_acc: 0.8394\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2877 - acc: 0.8844 - val_loss: 0.3550 - val_acc: 0.8361\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8841 - val_loss: 0.3610 - val_acc: 0.8376\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8835 - val_loss: 0.3141 - val_acc: 0.8505\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8841 - val_loss: 0.3305 - val_acc: 0.8496\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8843 - val_loss: 0.3539 - val_acc: 0.8394\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8839 - val_loss: 0.3247 - val_acc: 0.8507\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8843 - val_loss: 0.3330 - val_acc: 0.8447\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8837 - val_loss: 0.3259 - val_acc: 0.8476\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8841 - val_loss: 0.3665 - val_acc: 0.8317\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2874 - acc: 0.8843 - val_loss: 0.3369 - val_acc: 0.8454\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8841 - val_loss: 0.3355 - val_acc: 0.8485\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2878 - acc: 0.8840 - val_loss: 0.3832 - val_acc: 0.8286\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8843 - val_loss: 0.3618 - val_acc: 0.8345\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8845 - val_loss: 0.3288 - val_acc: 0.8434\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8842 - val_loss: 0.3442 - val_acc: 0.8427\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8841 - val_loss: 0.3287 - val_acc: 0.8478\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8845 - val_loss: 0.3630 - val_acc: 0.8368\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2879 - acc: 0.8839 - val_loss: 0.3625 - val_acc: 0.8326\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8845 - val_loss: 0.3384 - val_acc: 0.8432\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8849 - val_loss: 0.3399 - val_acc: 0.8441\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8849 - val_loss: 0.3386 - val_acc: 0.8487\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8846 - val_loss: 0.3655 - val_acc: 0.8257\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2877 - acc: 0.8843 - val_loss: 0.3513 - val_acc: 0.8383\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8839 - val_loss: 0.3585 - val_acc: 0.8359\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8844 - val_loss: 0.3250 - val_acc: 0.8505\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8842 - val_loss: 0.3413 - val_acc: 0.8441\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8846 - val_loss: 0.3562 - val_acc: 0.8385\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8841 - val_loss: 0.3550 - val_acc: 0.8352\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8845 - val_loss: 0.3498 - val_acc: 0.8368\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2875 - acc: 0.8850 - val_loss: 0.3405 - val_acc: 0.8427\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2862 - acc: 0.8846 - val_loss: 0.3457 - val_acc: 0.8441\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8844 - val_loss: 0.3506 - val_acc: 0.8427\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8845 - val_loss: 0.3347 - val_acc: 0.8427\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8845 - val_loss: 0.3631 - val_acc: 0.8388\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8846 - val_loss: 0.3532 - val_acc: 0.8385\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8839 - val_loss: 0.3373 - val_acc: 0.8445\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8854 - val_loss: 0.3614 - val_acc: 0.8376\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8835 - val_loss: 0.3340 - val_acc: 0.8505\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8839 - val_loss: 0.3344 - val_acc: 0.8516\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8853 - val_loss: 0.3478 - val_acc: 0.8421\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8846 - val_loss: 0.3396 - val_acc: 0.8494\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8843 - val_loss: 0.3525 - val_acc: 0.8423\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8840 - val_loss: 0.3506 - val_acc: 0.8416\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8846 - val_loss: 0.3334 - val_acc: 0.8511\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2868 - acc: 0.8850 - val_loss: 0.3411 - val_acc: 0.8414\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8846 - val_loss: 0.3317 - val_acc: 0.8478\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8842 - val_loss: 0.3545 - val_acc: 0.8392\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8844 - val_loss: 0.3596 - val_acc: 0.8390\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8848 - val_loss: 0.3332 - val_acc: 0.8511\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2868 - acc: 0.8847 - val_loss: 0.3514 - val_acc: 0.8361\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8849 - val_loss: 0.3302 - val_acc: 0.8511\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8848 - val_loss: 0.3354 - val_acc: 0.8443\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8845 - val_loss: 0.3582 - val_acc: 0.8403\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2862 - acc: 0.8848 - val_loss: 0.3447 - val_acc: 0.8399\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8849 - val_loss: 0.3416 - val_acc: 0.8441\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8849 - val_loss: 0.3474 - val_acc: 0.8416\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8854 - val_loss: 0.3460 - val_acc: 0.8350\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8847 - val_loss: 0.3331 - val_acc: 0.8461\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2864 - acc: 0.8858 - val_loss: 0.3268 - val_acc: 0.8522\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8850 - val_loss: 0.3576 - val_acc: 0.8359\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8850 - val_loss: 0.3343 - val_acc: 0.8443\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2863 - acc: 0.8853 - val_loss: 0.3454 - val_acc: 0.8416\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8846 - val_loss: 0.3279 - val_acc: 0.8443\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8845 - val_loss: 0.3433 - val_acc: 0.8427\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2868 - acc: 0.8850 - val_loss: 0.3612 - val_acc: 0.8361\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2870 - acc: 0.8847 - val_loss: 0.3572 - val_acc: 0.8306\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8846 - val_loss: 0.3596 - val_acc: 0.8407\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2862 - acc: 0.8858 - val_loss: 0.3370 - val_acc: 0.8454\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3586 - val_acc: 0.8376\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8844 - val_loss: 0.3488 - val_acc: 0.8401\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2865 - acc: 0.8853 - val_loss: 0.3528 - val_acc: 0.8368\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8851 - val_loss: 0.3463 - val_acc: 0.8412\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8845 - val_loss: 0.3567 - val_acc: 0.8396\n",
            "acc: 83.96%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 28us/step - loss: 0.3519 - acc: 0.8503 - val_loss: 0.3400 - val_acc: 0.8330\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3158 - acc: 0.8706 - val_loss: 0.3611 - val_acc: 0.8292\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3036 - acc: 0.8750 - val_loss: 0.3334 - val_acc: 0.8414\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2992 - acc: 0.8772 - val_loss: 0.3417 - val_acc: 0.8352\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2974 - acc: 0.8780 - val_loss: 0.3474 - val_acc: 0.8328\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2959 - acc: 0.8789 - val_loss: 0.3754 - val_acc: 0.8180\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2950 - acc: 0.8796 - val_loss: 0.3525 - val_acc: 0.8288\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2947 - acc: 0.8795 - val_loss: 0.3220 - val_acc: 0.8472\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8819 - val_loss: 0.3337 - val_acc: 0.8425\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2930 - acc: 0.8813 - val_loss: 0.3477 - val_acc: 0.8394\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8822 - val_loss: 0.3390 - val_acc: 0.8416\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8824 - val_loss: 0.3247 - val_acc: 0.8469\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2921 - acc: 0.8824 - val_loss: 0.3221 - val_acc: 0.8447\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8824 - val_loss: 0.3483 - val_acc: 0.8396\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2917 - acc: 0.8815 - val_loss: 0.3485 - val_acc: 0.8379\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2915 - acc: 0.8819 - val_loss: 0.3295 - val_acc: 0.8487\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2915 - acc: 0.8825 - val_loss: 0.3536 - val_acc: 0.8337\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8818 - val_loss: 0.3486 - val_acc: 0.8383\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8825 - val_loss: 0.3484 - val_acc: 0.8396\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8837 - val_loss: 0.3606 - val_acc: 0.8427\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8839 - val_loss: 0.3371 - val_acc: 0.8449\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8826 - val_loss: 0.3256 - val_acc: 0.8516\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2900 - acc: 0.8831 - val_loss: 0.3408 - val_acc: 0.8374\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2901 - acc: 0.8824 - val_loss: 0.3231 - val_acc: 0.8558\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2899 - acc: 0.8826 - val_loss: 0.3260 - val_acc: 0.8520\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8838 - val_loss: 0.3649 - val_acc: 0.8312\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8835 - val_loss: 0.3416 - val_acc: 0.8483\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8836 - val_loss: 0.3411 - val_acc: 0.8370\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8834 - val_loss: 0.3286 - val_acc: 0.8480\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8831 - val_loss: 0.3353 - val_acc: 0.8381\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8843 - val_loss: 0.3551 - val_acc: 0.8368\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8841 - val_loss: 0.3213 - val_acc: 0.8487\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2887 - acc: 0.8837 - val_loss: 0.3512 - val_acc: 0.8326\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8844 - val_loss: 0.3551 - val_acc: 0.8306\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8837 - val_loss: 0.3359 - val_acc: 0.8376\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2884 - acc: 0.8842 - val_loss: 0.3443 - val_acc: 0.8354\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8847 - val_loss: 0.3284 - val_acc: 0.8465\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8834 - val_loss: 0.3384 - val_acc: 0.8476\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8840 - val_loss: 0.3586 - val_acc: 0.8321\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8840 - val_loss: 0.3327 - val_acc: 0.8425\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8837 - val_loss: 0.3070 - val_acc: 0.8613\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8844 - val_loss: 0.3305 - val_acc: 0.8516\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8844 - val_loss: 0.3143 - val_acc: 0.8529\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8849 - val_loss: 0.3259 - val_acc: 0.8491\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8846 - val_loss: 0.3560 - val_acc: 0.8363\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8850 - val_loss: 0.3410 - val_acc: 0.8374\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8849 - val_loss: 0.3410 - val_acc: 0.8363\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8846 - val_loss: 0.3285 - val_acc: 0.8465\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8850 - val_loss: 0.3328 - val_acc: 0.8423\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8853 - val_loss: 0.3364 - val_acc: 0.8425\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8853 - val_loss: 0.3360 - val_acc: 0.8447\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8858 - val_loss: 0.3201 - val_acc: 0.8536\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8856 - val_loss: 0.3324 - val_acc: 0.8480\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8851 - val_loss: 0.3311 - val_acc: 0.8445\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8855 - val_loss: 0.3286 - val_acc: 0.8463\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2872 - acc: 0.8854 - val_loss: 0.3210 - val_acc: 0.8503\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8844 - val_loss: 0.3198 - val_acc: 0.8494\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8851 - val_loss: 0.3242 - val_acc: 0.8463\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8864 - val_loss: 0.3221 - val_acc: 0.8438\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2869 - acc: 0.8852 - val_loss: 0.3370 - val_acc: 0.8427\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8856 - val_loss: 0.3427 - val_acc: 0.8381\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2866 - acc: 0.8857 - val_loss: 0.3199 - val_acc: 0.8538\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8853 - val_loss: 0.3181 - val_acc: 0.8514\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8852 - val_loss: 0.3367 - val_acc: 0.8357\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8853 - val_loss: 0.3077 - val_acc: 0.8507\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8847 - val_loss: 0.3092 - val_acc: 0.8573\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2862 - acc: 0.8860 - val_loss: 0.3399 - val_acc: 0.8430\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8862 - val_loss: 0.3321 - val_acc: 0.8456\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8849 - val_loss: 0.3576 - val_acc: 0.8339\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8857 - val_loss: 0.3460 - val_acc: 0.8357\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8859 - val_loss: 0.3314 - val_acc: 0.8438\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8857 - val_loss: 0.3243 - val_acc: 0.8500\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8859 - val_loss: 0.3127 - val_acc: 0.8461\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8853 - val_loss: 0.3348 - val_acc: 0.8438\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8856 - val_loss: 0.3349 - val_acc: 0.8436\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8855 - val_loss: 0.3458 - val_acc: 0.8394\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8857 - val_loss: 0.3289 - val_acc: 0.8436\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2862 - acc: 0.8862 - val_loss: 0.3289 - val_acc: 0.8436\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8857 - val_loss: 0.3346 - val_acc: 0.8412\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2858 - acc: 0.8865 - val_loss: 0.3469 - val_acc: 0.8352\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8858 - val_loss: 0.3389 - val_acc: 0.8363\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3369 - val_acc: 0.8441\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8862 - val_loss: 0.3225 - val_acc: 0.8461\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8852 - val_loss: 0.3354 - val_acc: 0.8478\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2858 - acc: 0.8865 - val_loss: 0.3269 - val_acc: 0.8474\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8860 - val_loss: 0.3197 - val_acc: 0.8551\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8869 - val_loss: 0.3336 - val_acc: 0.8480\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8853 - val_loss: 0.3257 - val_acc: 0.8405\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3393 - val_acc: 0.8416\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8869 - val_loss: 0.3422 - val_acc: 0.8399\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8858 - val_loss: 0.3338 - val_acc: 0.8465\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8871 - val_loss: 0.3522 - val_acc: 0.8323\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8858 - val_loss: 0.3109 - val_acc: 0.8536\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8865 - val_loss: 0.3237 - val_acc: 0.8478\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2859 - acc: 0.8859 - val_loss: 0.3455 - val_acc: 0.8412\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2862 - acc: 0.8862 - val_loss: 0.3439 - val_acc: 0.8427\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8855 - val_loss: 0.3293 - val_acc: 0.8441\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8860 - val_loss: 0.3509 - val_acc: 0.8399\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8860 - val_loss: 0.3325 - val_acc: 0.8396\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2863 - acc: 0.8872 - val_loss: 0.3476 - val_acc: 0.8372\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8864 - val_loss: 0.3506 - val_acc: 0.8363\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2857 - acc: 0.8861 - val_loss: 0.3011 - val_acc: 0.8589\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3433 - val_acc: 0.8357\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2859 - acc: 0.8863 - val_loss: 0.3550 - val_acc: 0.8319\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2855 - acc: 0.8868 - val_loss: 0.3443 - val_acc: 0.8372\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2858 - acc: 0.8867 - val_loss: 0.3409 - val_acc: 0.8394\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8861 - val_loss: 0.3376 - val_acc: 0.8427\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8869 - val_loss: 0.3319 - val_acc: 0.8443\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8857 - val_loss: 0.3330 - val_acc: 0.8452\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8870 - val_loss: 0.3277 - val_acc: 0.8474\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8866 - val_loss: 0.3272 - val_acc: 0.8410\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8856 - val_loss: 0.3105 - val_acc: 0.8553\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2865 - acc: 0.8859 - val_loss: 0.3400 - val_acc: 0.8412\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2857 - acc: 0.8867 - val_loss: 0.3217 - val_acc: 0.8474\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8862 - val_loss: 0.3458 - val_acc: 0.8312\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8859 - val_loss: 0.3429 - val_acc: 0.8423\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8860 - val_loss: 0.3338 - val_acc: 0.8427\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2852 - acc: 0.8869 - val_loss: 0.3286 - val_acc: 0.8438\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2860 - acc: 0.8861 - val_loss: 0.3392 - val_acc: 0.8343\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8860 - val_loss: 0.3306 - val_acc: 0.8425\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8859 - val_loss: 0.3331 - val_acc: 0.8359\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8855 - val_loss: 0.3266 - val_acc: 0.8463\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2860 - acc: 0.8868 - val_loss: 0.3183 - val_acc: 0.8498\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2859 - acc: 0.8868 - val_loss: 0.3360 - val_acc: 0.8403\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2862 - acc: 0.8866 - val_loss: 0.3412 - val_acc: 0.8401\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8867 - val_loss: 0.3256 - val_acc: 0.8500\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8867 - val_loss: 0.3320 - val_acc: 0.8407\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8869 - val_loss: 0.3250 - val_acc: 0.8472\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8862 - val_loss: 0.3550 - val_acc: 0.8343\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2854 - acc: 0.8865 - val_loss: 0.3136 - val_acc: 0.8483\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8867 - val_loss: 0.3344 - val_acc: 0.8423\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8864 - val_loss: 0.3383 - val_acc: 0.8454\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2854 - acc: 0.8868 - val_loss: 0.3342 - val_acc: 0.8434\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8868 - val_loss: 0.3259 - val_acc: 0.8483\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2859 - acc: 0.8860 - val_loss: 0.3416 - val_acc: 0.8407\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2855 - acc: 0.8862 - val_loss: 0.3326 - val_acc: 0.8445\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2858 - acc: 0.8868 - val_loss: 0.3555 - val_acc: 0.8306\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8864 - val_loss: 0.3414 - val_acc: 0.8385\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2855 - acc: 0.8870 - val_loss: 0.3241 - val_acc: 0.8509\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8863 - val_loss: 0.3553 - val_acc: 0.8383\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2854 - acc: 0.8859 - val_loss: 0.3224 - val_acc: 0.8456\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2856 - acc: 0.8865 - val_loss: 0.3411 - val_acc: 0.8425\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2859 - acc: 0.8859 - val_loss: 0.3347 - val_acc: 0.8452\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2860 - acc: 0.8868 - val_loss: 0.3244 - val_acc: 0.8474\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8857 - val_loss: 0.3368 - val_acc: 0.8399\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8859 - val_loss: 0.3361 - val_acc: 0.8438\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8861 - val_loss: 0.3371 - val_acc: 0.8396\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2862 - acc: 0.8867 - val_loss: 0.3238 - val_acc: 0.8445\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8863 - val_loss: 0.3136 - val_acc: 0.8531\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2854 - acc: 0.8866 - val_loss: 0.3361 - val_acc: 0.8394\n",
            "acc: 83.94%\n",
            "84.24% (+/- 0.54%)\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   460    68        yes\n",
            "   644    3348        no\n",
            "Accuracy: 0.8423834907434031\n",
            "Sensitivity: 0.8704859141614671\n",
            "Specificity: 0.8386603877561245\n",
            "Precision: 0.41684019918515164\n",
            "f_score: 0.5637320925676502\n",
            "AUC: 0.8545699298699752\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWtdWxPr-nQh",
        "colab_type": "code",
        "outputId": "193a51cf-e60d-4744-9233-e863994edc08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "fpr_svm_p,tpr_svm_p ,thresholds_svm_p,auc_svm_p = percentage_split_SVM_SMOTE(0.25,X,Y)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1053    269        yes\n",
            "   1279    8702        no\n",
            "Accuracy: 0.8630452092364859\n",
            "Sensitivity: 0.7965204236006052\n",
            "Specificity: 0.8718565274020639\n",
            "Precision: 0.45154373927958835\n",
            "f_score: 0.5763546798029556\n",
            "AUC: 0.8341884755013346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj6yBcko-5Bd",
        "colab_type": "code",
        "outputId": "b60b26a0-480a-4ae2-e1a8-f8bd49ffbb87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "fpr_svm_c,tpr_svm_c ,thresholds_svm_c,auc_svm_c=crossvalidate_SVM_Smote(10,X,Y)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8668730650154799\n",
            "Acc: 0.8599867285998672\n",
            "Acc: 0.8637469586374696\n",
            "Acc: 0.8644105286441053\n",
            "Acc: 0.8646317186463172\n",
            "Acc: 0.8734793187347932\n",
            "Acc: 0.8577748285777483\n",
            "Acc: 0.8706038487060385\n",
            "Acc: 0.8613138686131386\n",
            "Acc: 0.865074098650741\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   419    109        yes\n",
            "   501    3490        no\n",
            "Accuracy: 0.8647895423680079\n",
            "Sensitivity: 0.7923993193420307\n",
            "Specificity: 0.8743800410801063\n",
            "Precision: 0.45524657831848797\n",
            "f_score: 0.5782683684028975\n",
            "AUC: 0.8333881845149961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rUh7Wppn4Jm",
        "colab_type": "code",
        "outputId": "f504cc07-1033-4ef0-f7c8-afb854acf63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "fpr_lr_p,tpr_lr_p ,thresholds_lr_p,auc_lr_p = percentage_split_LogisticsRegression_SMOTE(0.25,X,Y)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1086    236        yes\n",
            "   1526    8455        no\n",
            "Accuracy: 0.8441121826063877\n",
            "Sensitivity: 0.8214826021180031\n",
            "Specificity: 0.8471095080653241\n",
            "Precision: 0.41577335375191427\n",
            "f_score: 0.5521098118962888\n",
            "AUC: 0.8342960550916636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHb61PKYqJ4x",
        "colab_type": "code",
        "outputId": "8527ba23-6b0c-45cf-fcfd-f19fa1b676f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "fpr_lr_c,tpr_lr_c ,thresholds_lr_c,auc_lr_c = crossvalidate_LogisticsRegression_SMOTE(10,X,Y)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8456435205661211\n",
            "Acc: 0.8391948683919487\n",
            "Acc: 0.8464941384649414\n",
            "Acc: 0.8489272284892723\n",
            "Acc: 0.8411855784118558\n",
            "Acc: 0.8577748285777483\n",
            "Acc: 0.8422915284229153\n",
            "Acc: 0.8553417385534173\n",
            "Acc: 0.8374253483742535\n",
            "Acc: 0.8425127184251272\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   429    99        yes\n",
            "   598    3394        no\n",
            "Accuracy: 0.8456791488796974\n",
            "Sensitivity: 0.8116846284741918\n",
            "Specificity: 0.8501828565703121\n",
            "Precision: 0.41785088573097134\n",
            "f_score: 0.5516931182933881\n",
            "AUC: 0.8309320876956869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--bMKOQUyGkc",
        "colab_type": "code",
        "outputId": "abc38883-c353-485b-acaf-4c7c3a380b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "tpr_dt_p, fpr_dt_p, threshold_dt_p, auc_dt_p = DecisionTree_percentage_SMOTE(0.25,X, Y)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1032    290         yes\n",
            "   1665    8316         no\n",
            "\n",
            "Accuracy Score: 0.8270370698044767\n",
            "Sensitivity: 0.7806354009077155\n",
            "Specificity: 0.8331830477908025\n",
            "\n",
            "F1 Score: 0.5135605872107489\n",
            "Precision: 0.38264738598442716\n",
            "AUC: 0.8069092243492592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ3fZs48yHDJ",
        "colab_type": "code",
        "outputId": "2aed2571-40d1-4e61-9b5c-0edb0950dc3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "tpr_dt_c, fpr_dt_c, threshold_dt_c, auc_dt_c = DecisionTree_crossvalidation_SMOTE(10,X, Y)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8341441839893853\n",
            "Acc: 0.8259234682592347\n",
            "Acc: 0.8332227383322274\n",
            "Acc: 0.8440610484406105\n",
            "Acc: 0.8305684583056846\n",
            "Acc: 0.820836098208361\n",
            "Acc: 0.82304799823048\n",
            "Acc: 0.8259234682592347\n",
            "Acc: 0.8312320283123202\n",
            "Acc: 0.8367617783676178\n",
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   413    115        yes\n",
            "   650    3341        no\n",
            "Accuracy: 0.8305722058791003\n",
            "Sensitivity: 0.781811306485158\n",
            "Specificity: 0.8370322128149893\n",
            "Precision: 0.38859129781035623\n",
            "f_score: 0.5191462649089768\n",
            "AUC: 0.8094210535177652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dxBFS0-eK2e",
        "colab_type": "code",
        "outputId": "19feac3a-a3b5-47db-f7eb-94cde0865fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "tpr_knn_p, fpr_knn_p, threshold_knn_p, auc_knn_p = KNN_percentage_SMOTE(0.25,X, Y)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   926    396         yes\n",
            "   1586    8395         no\n",
            "\n",
            "Accuracy Score: 0.8246483234539502\n",
            "Sensitivity: 0.7004538577912254\n",
            "Specificity: 0.8410980863640918\n",
            "\n",
            "F1 Score: 0.48304642670839854\n",
            "Precision: 0.36863057324840764\n",
            "AUC: 0.7707759720776586\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBkTiNWTeQ4b",
        "colab_type": "code",
        "outputId": "7f88d37e-bcca-42b8-9fa6-a87426816008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "tpr_knn_c, fpr_knn_c, threshold_knn_c, auc_knn_c = KNN_crossvalidation_SMOTE(10,X, Y)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8226448474126493\n",
            "Acc: 0.8168546781685467\n",
            "Acc: 0.8270294182702942\n",
            "Acc: 0.818624198186242\n",
            "Acc: 0.8263658482636584\n",
            "Acc: 0.8345498783454988\n",
            "Acc: 0.8241539482415395\n",
            "Acc: 0.8310108383101084\n",
            "Acc: 0.8245963282459633\n",
            "Acc: 0.8283565582835656\n",
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   376    152        yes\n",
            "   636    3355        no\n",
            "Accuracy: 0.8254185928203315\n",
            "Sensitivity: 0.7118547929665343\n",
            "Specificity: 0.8404639046139973\n",
            "Precision: 0.3715216104203671\n",
            "f_score: 0.4882318615055437\n",
            "AUC: 0.7761602623648213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcqlLVmmi560",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ec53d8dc-443b-4385-b2c3-4d4e18ee7979"
      },
      "source": [
        "tpr_rf_p, fpr_rf_p, threshold_rf_p, auc_rf_p = RandomForest_percentage_SMOTE(0.25,X, Y)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1035    287         yes\n",
            "   1332    8649         no\n",
            "\n",
            "Accuracy Score: 0.856763691055472\n",
            "Sensitivity: 0.7829046898638427\n",
            "Specificity: 0.866546438232642\n",
            "\n",
            "F1 Score: 0.5611276768772026\n",
            "Precision: 0.4372623574144487\n",
            "AUC: 0.8247255640482424\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx8NFQ2Xi6Ym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "345aec06-23fa-4378-f126-6a146bc6170e"
      },
      "source": [
        "tpr_rf_c, fpr_rf_c, threshold_rf_c, auc_rf_c = RandomForest_crossvalidation_SMOTE(10,X, Y)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.848297213622291\n",
            "\n",
            "Accuracy: 0.851802698518027\n",
            "\n",
            "Accuracy: 0.851802698518027\n",
            "\n",
            "Accuracy: 0.8548993585489936\n",
            "\n",
            "Accuracy: 0.8537934085379341\n",
            "\n",
            "Accuracy: 0.8683919486839194\n",
            "\n",
            "Accuracy: 0.847378898473789\n",
            "\n",
            "Accuracy: 0.8599867285998672\n",
            "\n",
            "Accuracy: 0.8513603185136032\n",
            "\n",
            "Accuracy: 0.8469365184693651\n",
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   410    118        yes\n",
            "   544    3447        no\n",
            "Accuracy: 0.8534648647453054\n",
            "Sensitivity: 0.77670637171488\n",
            "Specificity: 0.8636340864686138\n",
            "Precision: 0.43006700167504186\n",
            "f_score: 0.5536015093322553\n",
            "AUC: 0.8201713759944733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpkeIgvckaAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "7f2d2fd1-dadc-4d55-f6ed-cfff19b91613"
      },
      "source": [
        "fpr_bayes_p,tpr_bayes_p ,thresholds_bayes_p,auc_bayes_p=percentage_split_Bayes(0.25,X,Y)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   680    642        yes\n",
            "   1051    8930        no\n",
            "Accuracy: 0.8502167566132885\n",
            "Sensitivity: 0.5143721633888049\n",
            "Specificity: 0.8946999298667468\n",
            "Precision: 0.3928365106874639\n",
            "f_score: 0.44546347854569274\n",
            "AUC: 0.7045360466277759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2GtHXnkaUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "db698aec-498d-4c72-ae72-2200f56b8dea"
      },
      "source": [
        "fpr_bayes_c,tpr_bayes_c ,thresholds_bayes_c,auc_bayes_c=crossvalidate_Bayes(10,X,Y)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.851172047766475\n",
            "Acc: 0.8445034284450342\n",
            "Acc: 0.8577748285777483\n",
            "Acc: 0.8445034284450342\n",
            "Acc: 0.8533510285335103\n",
            "Acc: 0.8597655385976554\n",
            "Acc: 0.8522450785224508\n",
            "Acc: 0.8546781685467817\n",
            "Acc: 0.8537934085379341\n",
            "Acc: 0.8456093784560937\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   268    260        yes\n",
            "   410    3581        no\n",
            "Accuracy: 0.8517396208887218\n",
            "Sensitivity: 0.5084136887880506\n",
            "Specificity: 0.8972245879464956\n",
            "Precision: 0.39590694935217896\n",
            "f_score: 0.4451618243522886\n",
            "AUC: 0.702816801768454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg5pbP0HmKRO",
        "colab_type": "code",
        "outputId": "c41326dd-3058-45db-b982-e0d3c095d7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "plt.plot(fpr_nn_p, tpr_nn_p, label='NN (area = {:.3f})'.format(auc_nn_p))\n",
        "plt.plot(fpr_svm_p, tpr_svm_p, label='SVM (area = {:.3f})'.format(auc_svm_p))\n",
        "plt.plot(fpr_lr_p, tpr_lr_p, label='LR (area = {:.3f})'.format(auc_lr_p))\n",
        "plt.plot(fpr_dt_p, tpr_dt_p, label='DT (area = {:.3f})'.format(auc_dt_p))\n",
        "plt.plot(fpr_knn_p, tpr_knn_p, label='KNN (area = {:.3f})'.format(auc_knn_p))\n",
        "plt.plot(fpr_rf_p, tpr_rf_p, label='RF (area = {:.3f})'.format(auc_rf_p))\n",
        "plt.plot(fpr_bayes_p, tpr_bayes_p, label='NB (area = {:.3f})'.format(auc_bayes_p))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIGhz/gdzdBisYY27xwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECSxi7ytl1RT",
        "colab_type": "code",
        "outputId": "566a48a1-9401-4371-f5f2-23108f00d30a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "plt.plot(fpr_nn_c, tpr_nn_c, label='NN (area = {:.3f})'.format(auc_nn_c))\n",
        "plt.plot(fpr_svm_c, tpr_svm_c, label='SVM (area = {:.3f})'.format(auc_svm_c))\n",
        "plt.plot(fpr_lr_c, tpr_lr_c, label='LR (area = {:.3f})'.format(auc_lr_c))\n",
        "plt.plot(fpr_dt_c, tpr_dt_c, label='DT (area = {:.3f})'.format(auc_dt_c))\n",
        "plt.plot(fpr_knn_c, tpr_knn_c, label='KNN (area = {:.3f})'.format(auc_knn_c))\n",
        "plt.plot(fpr_rf_c, tpr_rf_c, label='RF (area = {:.3f})'.format(auc_rf_c))\n",
        "plt.plot(fpr_bayes_c, tpr_bayes_c, label='NB (area = {:.3f})'.format(auc_bayes_c))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hyleu8Sh01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clf = svm.SVC(kernel=\"linear\")\n",
        "# clf.fit(x_train, y_train)\n",
        "# predict_svm = clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFxB6kc4M4Tf",
        "colab_type": "code",
        "outputId": "1e829502-2d3b-4007-f3a5-6e2432bac28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Confusion_matrix(y_test,predict_svm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "   0     1   << Classified as   \n",
            "   29    101\n",
            "   35    966\n",
            "Accuracy: 0.8797524314765695\n",
            "Sensitivity: 0.2230769230769231\n",
            "Specificity: 0.965034965034965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5bGwv2lS7Me",
        "colab_type": "code",
        "outputId": "23fcc982-b7fe-4819-bd36-ca1c30873b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# crossvalidate_SVM(10,X,Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8741721854304636\n",
            "Acc: 0.8761061946902655\n",
            "Acc: 0.8849557522123894\n",
            "Acc: 0.8938053097345132\n",
            "Acc: 0.8915929203539823\n",
            "Acc: 0.8738938053097345\n",
            "Acc: 0.8761061946902655\n",
            "Acc: 0.8938053097345132\n",
            "Acc: 0.8938053097345132\n",
            "Acc: 0.8938053097345132\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   0     1   << Classified as   \n",
            "   15    37\n",
            "   14    385\n",
            "Accuracy: 0.8815207780725022\n",
            "Sensitivity: 0.28982725527831094\n",
            "Specificity: 0.9627500000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrryyGXfy-Uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}