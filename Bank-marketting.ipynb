{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tv3CZbnS6Ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MvD3L-4NM5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "url='https://raw.githubusercontent.com/scarface961/Bank-Marketting/master/bank-full.csv'\n",
        "data = pd.read_csv(url,delimiter=';')\n",
        "#data1 = pd.read_csv(\"D:/Data Mining and Concepts Learning/Project/bank/bank.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rawbYjyYNM5w",
        "colab_type": "code",
        "outputId": "c2df4c1f-cc37-45cb-8153-5a9144f199cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>balance</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>day</th>\n",
              "      <th>month</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>management</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>2143</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>technician</td>\n",
              "      <td>single</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>29</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1506</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>unknown</td>\n",
              "      <td>single</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>unknown</td>\n",
              "      <td>5</td>\n",
              "      <td>may</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>technician</td>\n",
              "      <td>married</td>\n",
              "      <td>tertiary</td>\n",
              "      <td>no</td>\n",
              "      <td>825</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>retired</td>\n",
              "      <td>divorced</td>\n",
              "      <td>primary</td>\n",
              "      <td>no</td>\n",
              "      <td>1729</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>retired</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>5715</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>success</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>668</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>married</td>\n",
              "      <td>secondary</td>\n",
              "      <td>no</td>\n",
              "      <td>2971</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>17</td>\n",
              "      <td>nov</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>other</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age           job   marital  education  ... pdays  previous poutcome    y\n",
              "0       58    management   married   tertiary  ...    -1         0  unknown   no\n",
              "1       44    technician    single  secondary  ...    -1         0  unknown   no\n",
              "2       33  entrepreneur   married  secondary  ...    -1         0  unknown   no\n",
              "3       47   blue-collar   married    unknown  ...    -1         0  unknown   no\n",
              "4       33       unknown    single    unknown  ...    -1         0  unknown   no\n",
              "...    ...           ...       ...        ...  ...   ...       ...      ...  ...\n",
              "45206   51    technician   married   tertiary  ...    -1         0  unknown  yes\n",
              "45207   71       retired  divorced    primary  ...    -1         0  unknown  yes\n",
              "45208   72       retired   married  secondary  ...   184         3  success  yes\n",
              "45209   57   blue-collar   married  secondary  ...    -1         0  unknown   no\n",
              "45210   37  entrepreneur   married  secondary  ...   188        11    other   no\n",
              "\n",
              "[45211 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8YIuEdhNM50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.get_dummies(data, columns=[\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"])\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "data['y'] = labelencoder.fit_transform(data['y'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ5b4PBZOR-2",
        "colab_type": "code",
        "outputId": "9c141750-9159-4304-c94b-0a377b9e3ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>balance</th>\n",
              "      <th>day</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>y</th>\n",
              "      <th>job_admin.</th>\n",
              "      <th>job_blue-collar</th>\n",
              "      <th>job_entrepreneur</th>\n",
              "      <th>job_housemaid</th>\n",
              "      <th>job_management</th>\n",
              "      <th>job_retired</th>\n",
              "      <th>job_self-employed</th>\n",
              "      <th>job_services</th>\n",
              "      <th>job_student</th>\n",
              "      <th>job_technician</th>\n",
              "      <th>job_unemployed</th>\n",
              "      <th>job_unknown</th>\n",
              "      <th>marital_divorced</th>\n",
              "      <th>marital_married</th>\n",
              "      <th>marital_single</th>\n",
              "      <th>education_primary</th>\n",
              "      <th>education_secondary</th>\n",
              "      <th>education_tertiary</th>\n",
              "      <th>education_unknown</th>\n",
              "      <th>default_no</th>\n",
              "      <th>default_yes</th>\n",
              "      <th>housing_no</th>\n",
              "      <th>housing_yes</th>\n",
              "      <th>loan_no</th>\n",
              "      <th>loan_yes</th>\n",
              "      <th>contact_cellular</th>\n",
              "      <th>contact_telephone</th>\n",
              "      <th>contact_unknown</th>\n",
              "      <th>month_apr</th>\n",
              "      <th>month_aug</th>\n",
              "      <th>month_dec</th>\n",
              "      <th>month_feb</th>\n",
              "      <th>month_jan</th>\n",
              "      <th>month_jul</th>\n",
              "      <th>month_jun</th>\n",
              "      <th>month_mar</th>\n",
              "      <th>month_may</th>\n",
              "      <th>month_nov</th>\n",
              "      <th>month_oct</th>\n",
              "      <th>month_sep</th>\n",
              "      <th>poutcome_failure</th>\n",
              "      <th>poutcome_other</th>\n",
              "      <th>poutcome_success</th>\n",
              "      <th>poutcome_unknown</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>58</td>\n",
              "      <td>2143</td>\n",
              "      <td>5</td>\n",
              "      <td>261</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>29</td>\n",
              "      <td>5</td>\n",
              "      <td>151</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>47</td>\n",
              "      <td>1506</td>\n",
              "      <td>5</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>198</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45206</th>\n",
              "      <td>51</td>\n",
              "      <td>825</td>\n",
              "      <td>17</td>\n",
              "      <td>977</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45207</th>\n",
              "      <td>71</td>\n",
              "      <td>1729</td>\n",
              "      <td>17</td>\n",
              "      <td>456</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45208</th>\n",
              "      <td>72</td>\n",
              "      <td>5715</td>\n",
              "      <td>17</td>\n",
              "      <td>1127</td>\n",
              "      <td>5</td>\n",
              "      <td>184</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45209</th>\n",
              "      <td>57</td>\n",
              "      <td>668</td>\n",
              "      <td>17</td>\n",
              "      <td>508</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45210</th>\n",
              "      <td>37</td>\n",
              "      <td>2971</td>\n",
              "      <td>17</td>\n",
              "      <td>361</td>\n",
              "      <td>2</td>\n",
              "      <td>188</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45211 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       age  balance  day  ...  poutcome_other  poutcome_success  poutcome_unknown\n",
              "0       58     2143    5  ...               0                 0                 1\n",
              "1       44       29    5  ...               0                 0                 1\n",
              "2       33        2    5  ...               0                 0                 1\n",
              "3       47     1506    5  ...               0                 0                 1\n",
              "4       33        1    5  ...               0                 0                 1\n",
              "...    ...      ...  ...  ...             ...               ...               ...\n",
              "45206   51      825   17  ...               0                 0                 1\n",
              "45207   71     1729   17  ...               0                 0                 1\n",
              "45208   72     5715   17  ...               0                 1                 0\n",
              "45209   57      668   17  ...               0                 0                 1\n",
              "45210   37     2971   17  ...               1                 0                 0\n",
              "\n",
              "[45211 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGHBijTch-aY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch (history):\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBPmtJk8SF8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dPCA(X_train,X_test):\n",
        "    pca = PCA(n_components= 6)\n",
        "    X_train = pca.fit_transform(X_train)\n",
        "    X_test = pca.transform(X_test)\n",
        "    return X_train,X_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_EhqT1zSIij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dSMOTE(X_train,y_train):\n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6iEGAXxTBkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_NN(percentage_split,X,Y):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "\n",
        "  model= create_NN_model()\n",
        "  history = model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs=150, batch_size=100)\n",
        "  pred = model.predict_classes(x_test)\n",
        "  tn,fp,fn,tp = confusion_matrix(y_test,pred).ravel()\n",
        "  print(\"Percentage Split\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  #epoch(history)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl_uq9SNT28D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_NN(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    model= create_NN_model()\n",
        "    history = model.fit(X[train], Y[train],validation_data=(X[test],Y[test]),epochs=150, batch_size=100)\n",
        "    pred = model.predict_classes(X[test])\n",
        "    #evaluate the model\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],pred).ravel()\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "\n",
        "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "  print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "    \n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoqaNrinSPWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_NN_PCA(percentage_split,X,Y):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "  x_train,x_test = dPCA(x_train,x_test)\n",
        "  model= create_NN_model()\n",
        "  history = model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs=150, batch_size=100)\n",
        "  pred = model.predict_classes(x_test)\n",
        "  tn,fp,fn,tp = confusion_matrix(y_test,pred).ravel()\n",
        "  print(\"Percentage Split\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  #epoch(history)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el2mOiriSbsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_NN_PCA(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    x_train = X[train]\n",
        "    x_test = X[test]\n",
        "    x_train,x_test= dPCA(x_train,x_test)\n",
        "    model= create_NN_model()\n",
        "    history = model.fit(x_train, Y[train],validation_data=(x_test,Y[test]),epochs=150, batch_size=100)\n",
        "    pred = model.predict_classes(x_test)\n",
        "    #evaluate the model\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],pred).ravel()\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "\n",
        "    scores = model.evaluate(x_test, Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "  print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "    \n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrYfoqf9T2pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_NN_SMOTE(percentage_split,X,Y):\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "  x_train,y_train = dSMOTE(x_train,y_train)\n",
        "  model= create_NN_model()\n",
        "  history = model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs=150, batch_size=100)\n",
        "  pred = model.predict_classes(x_test)\n",
        "  fpr_nn,tpr_nn ,thresholds_nn  = roc_curve(y_test,pred)\n",
        "  auc_nn = auc(fpr_nn, tpr_nn)\n",
        "  tn,fp,fn,tp = confusion_matrix(y_test,pred).ravel()\n",
        "\n",
        "  print(\"Percentage Split\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  print(\"AUC: \"+repr(auc_nn))\n",
        "  return  fpr_nn,tpr_nn ,thresholds_nn,auc_nn\n",
        "  #epoch(history)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAlaWp0OT_v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_NN_SMOTE(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  fpr_nn = 0\n",
        "  tpr_nn = 0\n",
        "  thresholds_nn = 0\n",
        "  auc_nn=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    # x_train = X[train]\n",
        "    # x_test = X[test]\n",
        "    x_train,y_train= dSMOTE(X[train],Y[train])\n",
        "    model= create_NN_model()\n",
        "    history = model.fit(x_train, y_train,validation_data=(X[test],Y[test]),epochs=150, batch_size=100)\n",
        "    pred = model.predict_classes(X[test])\n",
        "    #evaluate the model\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],pred).ravel()\n",
        "    fpr_nn1,tpr_nn1 ,thresholds_nn1  = roc_curve(Y[test],pred)\n",
        "    auc_nn1 = auc(fpr_nn1, tpr_nn1)\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "    fpr_nn = fpr_nn + fpr_nn1\n",
        "    tpr_nn =  tpr_nn + tpr_nn1\n",
        "    thresholds_nn =thresholds_nn + thresholds_nn1 \n",
        "    auc_nn=auc_nn + auc_nn1\n",
        "\n",
        "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "  print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
        "    \n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  fpr_nn = fpr_nn/folds\n",
        "  tpr_nn = tpr_nn/folds\n",
        "  thresholds_nn = thresholds_nn/folds\n",
        "  auc_nn = auc_nn/folds \n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  print(\"AUC: \"+repr(auc_nn))\n",
        "  return  fpr_nn,tpr_nn ,thresholds_nn,auc_nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQdpyIbnR_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_SVM(percentage_split,X,Y):\n",
        "\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "  clf = svm.SVC(kernel=\"linear\")\n",
        "  clf.fit(x_train, y_train)\n",
        "  predict_svm = clf.predict(x_test)\n",
        "  Confusion_matrix(y_test,predict_svm)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-c5Sxtkn2jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_SVM(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    clf = svm.SVC(kernel=\"linear\")\n",
        "    clf.fit(X[train], Y[train])\n",
        "    predict_svm = clf.predict(X[test])\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],predict_svm).ravel()\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "\n",
        "\n",
        "    print(\"Acc: \"+repr(accuracy_score(Y[test],predict_svm)))\n",
        "\n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrMxb9MP8m2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_SVM_SMOTE(percentage_split,X,Y):\n",
        "\n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "    x_train,y_train = dSMOTE(x_train,y_train)\n",
        "    clf = svm.SVC(kernel=\"rbf\")\n",
        "    clf.fit(x_train, y_train)\n",
        "    predict_svm = clf.predict(x_test)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,predict_svm).ravel()\n",
        "    fpr_svm,tpr_svm ,thresholds_svm  = roc_curve(y_test,predict_svm)\n",
        "    auc_svm = auc(fpr_svm, tpr_svm)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,predict_svm).ravel()\n",
        "\n",
        "    print(\"Percentage Split\")\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC: \"+repr(auc_svm))\n",
        "    return  fpr_svm,tpr_svm ,thresholds_svm,auc_svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRuayk-h9UKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_SVM_Smote(folds,X,Y):\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "  cvscores = []\n",
        "  tn=0\n",
        "  fp=0\n",
        "  fn=0\n",
        "  tp=0\n",
        "  fpr_svm = 0\n",
        "  tpr_svm = 0\n",
        "  thresholds_svm = 0\n",
        "  auc_svm=0\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    x_train = X[train]\n",
        "    y_train = Y[train]\n",
        "    x_train,y_train= dSMOTE(x_train,y_train)\n",
        "    clf = svm.SVC(kernel=\"rbf\")\n",
        "    clf.fit(x_train, y_train)\n",
        "    predict_svm = clf.predict(X[test])\n",
        "    tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],predict_svm).ravel()\n",
        "    fpr_svm1,tpr_svm1 ,thresholds_svm1  = roc_curve(Y[test],predict_svm)\n",
        "    auc_svm1 = auc(fpr_svm1, tpr_svm1)\n",
        "    tn = tn+tn1\n",
        "    fp = fp+fp1\n",
        "    fn = fn+fn1\n",
        "    tp = tp+tp1\n",
        "    fpr_svm = fpr_svm + fpr_svm1\n",
        "    tpr_svm =  tpr_svm + tpr_svm1\n",
        "    thresholds_svm =thresholds_svm + thresholds_svm1 \n",
        "    auc_svm=auc_svm + auc_svm1\n",
        "\n",
        "\n",
        "    print(\"Acc: \"+repr(accuracy_score(Y[test],predict_svm)))\n",
        "\n",
        "  tn=tn/folds\n",
        "  fp=fp/folds\n",
        "  fn=fn/folds\n",
        "  tp=tp/folds\n",
        "  fpr_svm = fpr_svm/folds\n",
        "  tpr_svm = tpr_svm/folds\n",
        "  thresholds_svm = thresholds_svm/folds\n",
        "  auc_svm = auc_svm/folds \n",
        "  print(\"Cross-validation\")\n",
        "  Confusion_matrix(tn,fp,fn,tp)\n",
        "  print(\"AUC: \"+repr(auc_svm))\n",
        "  return  fpr_svm,tpr_svm ,thresholds_svm,auc_svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew1GchCKnwCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_LogisticsRegression_SMOTE(percentage_split,X,Y):\n",
        "    \n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "    x_train,y_train = dSMOTE(x_train,y_train)\n",
        "    clf = LogisticRegression(penalty = 'l2')\n",
        "    clf.fit(x_train,y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    fpr_lr,tpr_lr ,thresholds_lr  = roc_curve(y_test,y_pred)\n",
        "    auc_lr = auc(fpr_lr, tpr_lr)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,y_pred).ravel()\n",
        "\n",
        "    print(\"Percentage Split\")\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC: \"+repr(auc_lr))\n",
        "    return  fpr_lr,tpr_lr ,thresholds_lr,auc_lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk_j9uV2oXbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_LogisticsRegression_SMOTE(folds,X,Y):\n",
        "      kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "      cvscores = []\n",
        "      tn=0\n",
        "      fp=0\n",
        "      fn=0\n",
        "      tp=0\n",
        "      fpr_lr = 0\n",
        "      tpr_lr = 0\n",
        "      thresholds_lr = 0\n",
        "      auc_lr=0\n",
        "      for train, test in kfold.split(X, Y):\n",
        "        \n",
        "        x_train = X[train]\n",
        "        y_train = Y[train]\n",
        "        x_train,y_train= dSMOTE(x_train,y_train)\n",
        "        clf = LogisticRegression(penalty = 'l2')\n",
        "        clf.fit(x_train,y_train)\n",
        "        y_pred = clf.predict(X[test])\n",
        "\n",
        "        tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],y_pred).ravel()\n",
        "        fpr_lr1,tpr_lr1 ,thresholds_lr1  = roc_curve(Y[test],y_pred)\n",
        "        auc_lr1 = auc(fpr_lr1, tpr_lr1)\n",
        "        tn = tn+tn1\n",
        "        fp = fp+fp1\n",
        "        fn = fn+fn1\n",
        "        tp = tp+tp1\n",
        "        fpr_lr = fpr_lr + fpr_lr1\n",
        "        tpr_lr =  tpr_lr + tpr_lr1\n",
        "        thresholds_lr =thresholds_lr + thresholds_lr1 \n",
        "        auc_lr=auc_lr + auc_lr1\n",
        "\n",
        "        print(\"Acc: \"+repr(accuracy_score(Y[test],y_pred)))\n",
        "\n",
        "      tn=tn/folds\n",
        "      fp=fp/folds\n",
        "      fn=fn/folds\n",
        "      tp=tp/folds\n",
        "      fpr_lr = fpr_lr/folds\n",
        "      tpr_lr = tpr_lr/folds\n",
        "      thresholds_lr = thresholds_lr/folds\n",
        "      auc_lr = auc_lr/folds \n",
        "      print(\"Cross-validation\")\n",
        "      Confusion_matrix(tn,fp,fn,tp)\n",
        "      print(\"AUC: \"+repr(auc_lr))\n",
        "      return  fpr_lr,tpr_lr ,thresholds_lr,auc_lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7SU1-Gmxdp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DecisionTree_percentage_SMOTE(percentage,X, y):\n",
        "\n",
        "    ##### Train & Test Split #####\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=percentage, random_state=0)\n",
        "    \n",
        "    ####### Handling Imbalanced Dataset ###########\n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    \n",
        "    clf = DecisionTreeClassifier(criterion= 'gini', max_depth=7, splitter='best', random_state=0)\n",
        "    \n",
        "    ######## WITH SMOTE ##########\n",
        "#     print(\"ONLY SMOTE\")\n",
        "#     print(\"Criterion = 'gini', max_depth=7, splitter='best'\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"\")\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    predict = clf.predict(X_test)\n",
        "    print(\"\")\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(\"   yes     no   <<-- Classified as   \")\n",
        "    print(\"  \",tp,\"  \",fn,\"        yes\")\n",
        "    print(\"  \",fp,\"  \",tn,\"        no\")\n",
        "    print(\"\")\n",
        "    Accuracy_SMOTE = accuracy_score(y_test,predict)\n",
        "    print(\"Accuracy Score:\",Accuracy_SMOTE)\n",
        "    Sensitivity_SMOTE = tp/(tp+fn)\n",
        "    print(\"Sensitivity:\", Sensitivity_SMOTE)\n",
        "    print(\"Specificity:\", tn/(tn+fp))\n",
        "    print(\"\")\n",
        "    print(\"F1 Score:\",f1_score(y_test, predict))\n",
        "    #print(\"Log Loss:\",log_loss(y_test, predict, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
        "    print(\"Precision:\",precision_score(y_test, predict))\n",
        "    #print(\"Recall:\",recall_score(y_test, predict))\n",
        "    auc= roc_auc_score(y_test, predict)\n",
        "    print(\"AUC:\",roc_auc_score(y_test, predict))\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predict)\n",
        "#     print(\"True Positive Rate:\",tpr.mean())\n",
        "#     print(\"False Positive Rate:\",fpr.mean())\n",
        "#     print(\"Threshold:\",thresholds.mean())\n",
        "#     print(\"CLASSIFICATION REPORT\")\n",
        "#     print(classification_report(y_test, predict))\n",
        "#     print(\"\")\n",
        "    return tpr, fpr, thresholds, auc\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL_W_SHWx4RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########## K-Folds CV on Raw Data WITH SMOTE##########\n",
        "def DecisionTree_crossvalidation_SMOTE(folds,X, y):\n",
        "#     print(\"WITH ONLY SMOTE\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"-------------------------------------\")\n",
        "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "    tn=0\n",
        "    fp=0\n",
        "    fn=0\n",
        "    tp=0\n",
        "    fpr_dt = 0\n",
        "    tpr_dt = 0\n",
        "    thresholds_dt = 0\n",
        "    auc_dt=0\n",
        "    clf = DecisionTreeClassifier(criterion= 'gini', max_depth=7, splitter='best', random_state=0)\n",
        "    \n",
        "    for train, test in kfold.split(X, y):\n",
        "      X_train = X[train]\n",
        "      y_train = y[train]\n",
        "      X_train,y_train = dSMOTE(X_train,y_train)\n",
        "      clf.fit(X_train, y_train)\n",
        "      predict = clf.predict(X[test])\n",
        "      fpr_dt1,tpr_dt1 ,thresholds_dt1 = metrics.roc_curve(y[test],predict)\n",
        "      auc_dt1 = roc_auc_score(y[test], predict)\n",
        "      tn1, fp1, fn1, tp1 = confusion_matrix(y[test],predict).ravel()\n",
        "      tn = tn+tn1\n",
        "      fp = fp+fp1\n",
        "      fn = fn+fn1\n",
        "      tp = tp+tp1\n",
        "      fpr_dt = fpr_dt + fpr_dt1\n",
        "      tpr_dt =  tpr_dt + tpr_dt1\n",
        "      thresholds_dt = thresholds_dt + thresholds_dt1\n",
        "      auc_dt=auc_dt + auc_dt1\n",
        "      print(\"Acc: \"+repr(accuracy_score(y[test],predict)))\n",
        "    print(\"\")\n",
        "    tn=tn/folds\n",
        "    fp=fp/folds\n",
        "    fn=fn/folds\n",
        "    tp=tp/folds\n",
        "    fpr_dt = fpr_dt/folds\n",
        "    tpr_dt = tpr_dt/folds\n",
        "    thresholds_dt = thresholds_dt/folds\n",
        "    auc_dt = auc_dt/folds\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC:\",auc_dt)\n",
        "    return tpr_dt, fpr_dt, thresholds_dt, auc_dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeCq5Q7Kdy_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KNN_percentage_SMOTE(percent,X, y):\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=percent, random_state=0)\n",
        "    \n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    \n",
        "    clf = KNeighborsClassifier(algorithm=\"kd_tree\", metric=\"euclidean\", n_neighbors=7)\n",
        "    \n",
        "    ##### Training#####\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    \n",
        "    ##### Testing and accuracy #####\n",
        "    predict = clf.predict(X_test)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(\"   yes     no   <<-- Classified as   \")\n",
        "    print(\"  \",tp,\"  \",fn,\"        yes\")\n",
        "    print(\"  \",fp,\"  \",tn,\"        no\")\n",
        "    print(\"\")\n",
        "    Accuracy_SMOTE = accuracy_score(y_test,predict)\n",
        "    print(\"Accuracy Score:\",Accuracy_SMOTE)\n",
        "    Sensitivity_SMOTE = tp/(tp+fn)\n",
        "    print(\"Sensitivity:\", Sensitivity_SMOTE)\n",
        "    print(\"Specificity:\", tn/(tn+fp))\n",
        "    print(\"\")\n",
        "    print(\"F1 Score:\",f1_score(y_test, predict))\n",
        "    print(\"Precision:\",precision_score(y_test, predict))\n",
        "    #print(\"Recall:\",recall_score(y_test, predict))\n",
        "    auc = roc_auc_score(y_test, predict)\n",
        "    print(\"AUC:\",auc)\n",
        "    print(\"\")\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predict)\n",
        "#     print(\"True Positive Rate:\",tpr)\n",
        "#     print(\"False Positive Rate:\",fpr)\n",
        "#     print(\"Threshold:\",thresholds)\n",
        "    print(\"\")\n",
        "    return tpr, fpr, thresholds, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWxd-4vCd4x7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KNN_crossvalidation_SMOTE(folds,X, y):\n",
        "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "    tn=0\n",
        "    fp=0\n",
        "    fn=0\n",
        "    tp=0\n",
        "    fpr_knn = 0\n",
        "    tpr_knn = 0\n",
        "    thresholds_knn = 0\n",
        "    auc_knn=0\n",
        "    clf = KNeighborsClassifier(algorithm=\"kd_tree\", metric=\"euclidean\", n_neighbors=7)\n",
        "    \n",
        "    for train, test in kfold.split(X, y):\n",
        "      X_train = X[train]\n",
        "      y_train = y[train]\n",
        "      X_train,y_train = dSMOTE(X_train,y_train)\n",
        "      clf.fit(X_train, y_train)\n",
        "      predict = clf.predict(X[test])\n",
        "      fpr_knn1,tpr_knn1 ,thresholds_knn1 = metrics.roc_curve(y[test],predict)\n",
        "      auc_knn1 = roc_auc_score(y[test], predict)\n",
        "      tn1, fp1, fn1, tp1 = confusion_matrix(y[test],predict).ravel()\n",
        "      tn = tn+tn1\n",
        "      fp = fp+fp1\n",
        "      fn = fn+fn1\n",
        "      tp = tp+tp1\n",
        "      fpr_knn = fpr_knn + fpr_knn1\n",
        "      tpr_knn =  tpr_knn + tpr_knn1\n",
        "      thresholds_knn = thresholds_knn + thresholds_knn1\n",
        "      auc_knn=auc_knn + auc_knn1\n",
        "      print(\"Acc: \"+repr(accuracy_score(y[test],predict)))\n",
        "    print(\"\")\n",
        "    tn=tn/folds\n",
        "    fp=fp/folds\n",
        "    fn=fn/folds\n",
        "    tp=tp/folds\n",
        "    fpr_knn = fpr_knn/folds\n",
        "    tpr_knn = tpr_knn/folds\n",
        "    thresholds_knn = thresholds_knn/folds\n",
        "    auc_knn = auc_knn/folds\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC:\",auc_knn)\n",
        "    return tpr_knn, fpr_knn, thresholds_knn, auc_knn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEaL6rF-iR1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RandomForest_percentage_SMOTE(percent,X, y):\n",
        "    ##### Train & Test Split #####\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=percent, random_state=0)\n",
        "    \n",
        "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "    \n",
        "    ####### Handling Imbalanced Dataset ###########\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    smt = SMOTE()\n",
        "    X_train, y_train = smt.fit_sample(X_train, y_train)\n",
        "    \n",
        "    \n",
        "    ##### Training#####\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    \n",
        "    ####### Testing and accuracy #####\n",
        "    from sklearn.metrics import accuracy_score\n",
        "#     print(\"Feature Importances:\", clf.feature_importances_)\n",
        "#     print(\"\")\n",
        "    predict = clf.predict(X_test)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()\n",
        "    print(\"Confusion Matrix\")\n",
        "    print(\"   yes     no   <<-- Classified as   \")\n",
        "    print(\"  \",tp,\"  \",fn,\"        yes\")\n",
        "    print(\"  \",fp,\"  \",tn,\"        no\")\n",
        "    print(\"\")\n",
        "    Accuracy_SMOTE = accuracy_score(y_test,predict)\n",
        "    print(\"Accuracy Score:\",Accuracy_SMOTE)\n",
        "    Sensitivity_SMOTE = tp/(tp+fn)\n",
        "    print(\"Sensitivity:\", Sensitivity_SMOTE)\n",
        "    print(\"Specificity:\", tn/(tn+fp))\n",
        "    print(\"\")\n",
        "    print(\"F1 Score:\",f1_score(y_test, predict))\n",
        "    #print(\"Log Loss:\",log_loss(y_test, predict, eps=1e-15, normalize=True, sample_weight=None, labels=None))\n",
        "    print(\"Precision:\",precision_score(y_test, predict))\n",
        "    #print(\"Recall:\",recall_score(y_test, predict))\n",
        "    auc = roc_auc_score(y_test, predict)\n",
        "    print(\"AUC:\",auc)\n",
        "    print(\"\")\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predict)\n",
        "#     print(\"True Positive Rate:\",tpr)\n",
        "#     print(\"False Positive Rate:\",fpr)\n",
        "#     print(\"Threshold:\",thresholds)\n",
        "    return tpr, fpr, thresholds, auc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7ySeVXeiSSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RandomForest_crossvalidation_SMOTE(folds,X, y):\n",
        "    ########## K-Folds CV with RFECV WITH SMOTE##########\n",
        "#     print(\"WITH ONLY SMOTE\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"-------------------------------------\")\n",
        "#     print(\"\")\n",
        "    clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\n",
        "    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "    tn=0\n",
        "    fp=0\n",
        "    fn=0\n",
        "    tp=0\n",
        "    fpr_rf = 0\n",
        "    tpr_rf = 0\n",
        "    thresholds_rf = 0\n",
        "    auc_rf=0\n",
        "    for train, test in kfold.split(X, y):\n",
        "      X_train = X[train]\n",
        "      y_train = y[train]\n",
        "      X_train,y_train = dSMOTE(X_train,y_train)\n",
        "      clf.fit(X_train, y_train)\n",
        "      #print(\"Feature Importances:\", clf.feature_importances_)\n",
        "      #print(\"\")\n",
        "      predict = clf.predict(X[test])\n",
        "      fpr_rf1,tpr_rf1 ,thresholds_rf1 = metrics.roc_curve(y[test],predict)\n",
        "      auc_rf1 = roc_auc_score(y[test], predict)\n",
        "      tn1, fp1, fn1, tp1 = confusion_matrix(y[test],predict).ravel()\n",
        "      tn = tn+tn1\n",
        "      fp = fp+fp1\n",
        "      fn = fn+fn1\n",
        "      tp = tp+tp1\n",
        "      fpr_rf = fpr_rf + fpr_rf1\n",
        "      tpr_rf =  tpr_rf + tpr_rf1\n",
        "      thresholds_rf = thresholds_rf + thresholds_rf1\n",
        "      auc_rf=auc_rf + auc_rf1\n",
        "      print(\"Accuracy: \"+repr(accuracy_score(y[test],predict)))\n",
        "      print(\"\")\n",
        "    tn=tn/folds\n",
        "    fp=fp/folds\n",
        "    fn=fn/folds\n",
        "    tp=tp/folds\n",
        "    fpr_rf = fpr_rf/folds\n",
        "    tpr_rf = tpr_rf/folds\n",
        "    thresholds_rf = thresholds_rf/folds\n",
        "    auc_rf = auc_rf/folds\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC:\",auc_rf)\n",
        "    return tpr_rf,fpr_rf,thresholds_rf,auc_rf\n",
        "\n",
        "\n",
        "\n",
        "# print(tpr_ps)\n",
        "# print(fpr_ps)\n",
        "# print(threshold_ps)\n",
        "# print(auc_ps)\n",
        "# print(\"\")\n",
        "# print(tpr_cv)\n",
        "# print(fpr_cv)\n",
        "# print(threshold_cv)\n",
        "# print(auc_cv)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui5oFfOLj5Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def percentage_split_Bayes(percentage_split,X,Y):\n",
        "    \n",
        "    x_train,x_test,y_train,y_test = train_test_split(X,Y,stratify=Y,test_size = percentage_split,random_state=0)\n",
        "   \n",
        "    nb = GaussianNB()\n",
        "    nb.fit(x_train,y_train)\n",
        "    y_pred = nb.predict(x_test)\n",
        "    fpr_bayes,tpr_bayes ,thresholds_bayes  = roc_curve(y_test,y_pred)\n",
        "    auc_bayes = auc(fpr_bayes, tpr_bayes)\n",
        "    tn,fp,fn,tp = confusion_matrix(y_test,y_pred).ravel()\n",
        "\n",
        "    print(\"Percentage Split\")\n",
        "    Confusion_matrix(tn,fp,fn,tp)\n",
        "    print(\"AUC: \"+repr(auc_bayes))\n",
        "    return  fpr_bayes,tpr_bayes ,thresholds_bayes,auc_bayes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4i3fcD5j5gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossvalidate_Bayes(folds,X,Y):\n",
        "      kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=0)\n",
        "      cvscores = []\n",
        "      tn=0\n",
        "      fp=0\n",
        "      fn=0\n",
        "      tp=0\n",
        "      fpr_bayes = 0\n",
        "      tpr_bayes = 0\n",
        "      thresholds_bayes = 0\n",
        "      auc_bayes=0\n",
        "      for train, test in kfold.split(X, Y):\n",
        "        nb = GaussianNB()\n",
        "        nb.fit( X[train],Y[train])\n",
        "        y_pred = nb.predict(X[test])\n",
        "\n",
        "        tn1, fp1, fn1, tp1 = confusion_matrix(Y[test],y_pred).ravel()\n",
        "        fpr_bayes1,tpr_bayes1 ,thresholds_bayes1  = roc_curve(Y[test],y_pred)\n",
        "        auc_bayes1 = auc(fpr_bayes1, tpr_bayes1)\n",
        "        tn = tn+tn1\n",
        "        fp = fp+fp1\n",
        "        fn = fn+fn1\n",
        "        tp = tp+tp1\n",
        "        fpr_bayes = fpr_bayes + fpr_bayes1\n",
        "        tpr_bayes =  tpr_bayes + tpr_bayes1\n",
        "        thresholds_bayes =thresholds_bayes + thresholds_bayes1 \n",
        "        auc_bayes=auc_bayes + auc_bayes1\n",
        "\n",
        "\n",
        "        print(\"Acc: \"+repr(accuracy_score(Y[test],y_pred)))\n",
        "\n",
        "      tn=tn/folds\n",
        "      fp=fp/folds\n",
        "      fn=fn/folds\n",
        "      tp=tp/folds\n",
        "      fpr_bayes = fpr_bayes/folds\n",
        "      tpr_bayes = tpr_bayes/folds\n",
        "      thresholds_bayes = thresholds_bayes/folds\n",
        "      auc_bayes = auc_bayes/folds \n",
        "      print(\"Cross-validation\")\n",
        "      Confusion_matrix(tn,fp,fn,tp)\n",
        "      print(\"AUC: \"+repr(auc_bayes))\n",
        "      return  fpr_bayes,tpr_bayes ,thresholds_bayes,auc_bayes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk3RE06HSEnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Confusion_matrix(tn, fp, fn, tp):\n",
        "  #tn, fp, fn, tp = confusion_matrix(y_test,pred).ravel()\n",
        "  print(\"Confusion Matrix\")\n",
        "  print(\"   yes     no   <<-- Classified as   \")\n",
        "  # print(\"   %.2f    %.2f  yes\" %(tp,fn))\n",
        "  # print(\"   %.2f    %.2f  no\" %(fp,tn))\n",
        "  print(\"   \"+repr(int(tp))+\"    \"+repr(int(fn))+\"        yes\")\n",
        "  print(\"   \"+repr(int(fp))+\"    \"+repr(int(tn))+\"        no\")\n",
        "  accuracy = float((tp+tn)/(tp+tn+fp+fn))\n",
        "  specificity = float(tn / (tn+fp))\n",
        "  sensitivity = float(tp / (tp+fn))\n",
        "  \n",
        "  if (tp+fp)>=0:\n",
        "    precision = float(tp / (tp+fp))\n",
        "    f_score = (2*precision*sensitivity)/(precision+sensitivity) \n",
        "  else:\n",
        "    precision = 0\n",
        "    f_score = \"?\"\n",
        " \n",
        "  #print(classification_report(y_test,predictions))\n",
        "  print(\"Accuracy: \"+repr(accuracy))\n",
        "  print(\"Sensitivity: \"+repr(sensitivity))\n",
        "  print(\"Specificity: \"+repr(specificity))\n",
        "  print(\"Precision: \"+repr(precision))\n",
        "  print(\"f_score: \"+repr(f_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQPYa0HyNM54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_NN_model():\n",
        "  #Keras Multi Layer\n",
        "  model = Sequential()\n",
        "  model.add(Dense(5,input_dim=51,activation='tanh'))\n",
        "  model.add(Dense(5, activation='tanh'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  #keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "  #sgd = optimizers.SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  a = optimizers.adam(lr=0.007)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=a, metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HklmHbO4NM52",
        "colab_type": "code",
        "outputId": "96bb3342-db57-480b-cc0f-d29b142ca398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Splitting the data labels and dataset\n",
        "Y = data.y.values\n",
        "X= data.drop('y',axis = 1).values\n",
        "# X = data.iloc[:,0:53]\n",
        "# Y = data.iloc[:,52:53]\n",
        "# print(X)\n",
        "# #Normalize\n",
        "# from sklearn import preprocessing\n",
        "# X = preprocessing.scale(X)\n",
        "# print(Y)\n",
        "from sklearn import preprocessing\n",
        "X = preprocessing.scale(X)\n",
        "X\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.60696496,  0.25641925, -1.29847633, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       [ 0.28852927, -0.43789469, -1.29847633, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       [-0.74738448, -0.44676247, -1.29847633, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       ...,\n",
              "       [ 2.92540065,  1.42959305,  0.14341818, ..., -0.20597248,\n",
              "         5.37784754, -2.11631591],\n",
              "       [ 1.51279098, -0.22802402,  0.14341818, ..., -0.20597248,\n",
              "        -0.185948  ,  0.47251925],\n",
              "       [-0.37068857,  0.52836436,  0.14341818, ...,  4.85501757,\n",
              "        -0.185948  , -2.11631591]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7cqkSwYnNEO",
        "colab_type": "code",
        "outputId": "2971d55a-2807-4221-a012-d98a37acccd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "percentage_split_NN(0.25,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 33908 samples, validate on 11303 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "33908/33908 [==============================] - 1s 36us/step - loss: 0.2777 - acc: 0.8843 - val_loss: 0.2274 - val_acc: 0.9012\n",
            "Epoch 2/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.2240 - acc: 0.9014 - val_loss: 0.2220 - val_acc: 0.9005\n",
            "Epoch 3/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2178 - acc: 0.9060 - val_loss: 0.2171 - val_acc: 0.9028\n",
            "Epoch 4/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2151 - acc: 0.9074 - val_loss: 0.2148 - val_acc: 0.9022\n",
            "Epoch 5/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2132 - acc: 0.9063 - val_loss: 0.2175 - val_acc: 0.9013\n",
            "Epoch 6/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2117 - acc: 0.9073 - val_loss: 0.2170 - val_acc: 0.9023\n",
            "Epoch 7/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2111 - acc: 0.9075 - val_loss: 0.2170 - val_acc: 0.9022\n",
            "Epoch 8/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2098 - acc: 0.9078 - val_loss: 0.2174 - val_acc: 0.9019\n",
            "Epoch 9/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2095 - acc: 0.9073 - val_loss: 0.2150 - val_acc: 0.9053\n",
            "Epoch 10/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2086 - acc: 0.9086 - val_loss: 0.2157 - val_acc: 0.9018\n",
            "Epoch 11/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2078 - acc: 0.9076 - val_loss: 0.2173 - val_acc: 0.9024\n",
            "Epoch 12/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2081 - acc: 0.9069 - val_loss: 0.2166 - val_acc: 0.9027\n",
            "Epoch 13/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2076 - acc: 0.9074 - val_loss: 0.2131 - val_acc: 0.9022\n",
            "Epoch 14/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2072 - acc: 0.9077 - val_loss: 0.2119 - val_acc: 0.9047\n",
            "Epoch 15/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2069 - acc: 0.9080 - val_loss: 0.2133 - val_acc: 0.9024\n",
            "Epoch 16/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2068 - acc: 0.9076 - val_loss: 0.2131 - val_acc: 0.9027\n",
            "Epoch 17/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2062 - acc: 0.9077 - val_loss: 0.2117 - val_acc: 0.9022\n",
            "Epoch 18/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2055 - acc: 0.9082 - val_loss: 0.2126 - val_acc: 0.9031\n",
            "Epoch 19/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2058 - acc: 0.9088 - val_loss: 0.2125 - val_acc: 0.9037\n",
            "Epoch 20/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2051 - acc: 0.9079 - val_loss: 0.2118 - val_acc: 0.9022\n",
            "Epoch 21/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2052 - acc: 0.9072 - val_loss: 0.2127 - val_acc: 0.9014\n",
            "Epoch 22/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2042 - acc: 0.9069 - val_loss: 0.2132 - val_acc: 0.9028\n",
            "Epoch 23/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2041 - acc: 0.9079 - val_loss: 0.2133 - val_acc: 0.9038\n",
            "Epoch 24/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2041 - acc: 0.9086 - val_loss: 0.2138 - val_acc: 0.9020\n",
            "Epoch 25/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2037 - acc: 0.9077 - val_loss: 0.2153 - val_acc: 0.9044\n",
            "Epoch 26/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2047 - acc: 0.9065 - val_loss: 0.2138 - val_acc: 0.9037\n",
            "Epoch 27/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.2037 - acc: 0.9076 - val_loss: 0.2129 - val_acc: 0.9030\n",
            "Epoch 28/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9082 - val_loss: 0.2138 - val_acc: 0.9032\n",
            "Epoch 29/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9078 - val_loss: 0.2129 - val_acc: 0.9035\n",
            "Epoch 30/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9074 - val_loss: 0.2138 - val_acc: 0.9038\n",
            "Epoch 31/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2033 - acc: 0.9081 - val_loss: 0.2133 - val_acc: 0.9032\n",
            "Epoch 32/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2035 - acc: 0.9078 - val_loss: 0.2139 - val_acc: 0.9014\n",
            "Epoch 33/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2030 - acc: 0.9083 - val_loss: 0.2125 - val_acc: 0.9037\n",
            "Epoch 34/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2029 - acc: 0.9080 - val_loss: 0.2123 - val_acc: 0.9038\n",
            "Epoch 35/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2025 - acc: 0.9082 - val_loss: 0.2140 - val_acc: 0.9035\n",
            "Epoch 36/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2027 - acc: 0.9073 - val_loss: 0.2128 - val_acc: 0.9045\n",
            "Epoch 37/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2027 - acc: 0.9083 - val_loss: 0.2115 - val_acc: 0.9049\n",
            "Epoch 38/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2027 - acc: 0.9084 - val_loss: 0.2126 - val_acc: 0.9045\n",
            "Epoch 39/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2024 - acc: 0.9077 - val_loss: 0.2135 - val_acc: 0.9043\n",
            "Epoch 40/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2024 - acc: 0.9078 - val_loss: 0.2121 - val_acc: 0.9054\n",
            "Epoch 41/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2019 - acc: 0.9081 - val_loss: 0.2117 - val_acc: 0.9039\n",
            "Epoch 42/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2023 - acc: 0.9077 - val_loss: 0.2106 - val_acc: 0.9054\n",
            "Epoch 43/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2019 - acc: 0.9082 - val_loss: 0.2118 - val_acc: 0.9042\n",
            "Epoch 44/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2018 - acc: 0.9075 - val_loss: 0.2122 - val_acc: 0.9049\n",
            "Epoch 45/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2013 - acc: 0.9087 - val_loss: 0.2144 - val_acc: 0.9033\n",
            "Epoch 46/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2010 - acc: 0.9080 - val_loss: 0.2128 - val_acc: 0.9037\n",
            "Epoch 47/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2015 - acc: 0.9082 - val_loss: 0.2117 - val_acc: 0.9026\n",
            "Epoch 48/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.2016 - acc: 0.9078 - val_loss: 0.2126 - val_acc: 0.9015\n",
            "Epoch 49/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.2009 - acc: 0.9080 - val_loss: 0.2121 - val_acc: 0.9042\n",
            "Epoch 50/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2007 - acc: 0.9078 - val_loss: 0.2123 - val_acc: 0.9020\n",
            "Epoch 51/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2010 - acc: 0.9076 - val_loss: 0.2118 - val_acc: 0.9021\n",
            "Epoch 52/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2008 - acc: 0.9088 - val_loss: 0.2125 - val_acc: 0.9045\n",
            "Epoch 53/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2010 - acc: 0.9076 - val_loss: 0.2114 - val_acc: 0.9061\n",
            "Epoch 54/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2006 - acc: 0.9080 - val_loss: 0.2114 - val_acc: 0.9045\n",
            "Epoch 55/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2004 - acc: 0.9081 - val_loss: 0.2118 - val_acc: 0.9024\n",
            "Epoch 56/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2008 - acc: 0.9086 - val_loss: 0.2123 - val_acc: 0.9031\n",
            "Epoch 57/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.2001 - acc: 0.9085 - val_loss: 0.2118 - val_acc: 0.9026\n",
            "Epoch 58/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2007 - acc: 0.9082 - val_loss: 0.2125 - val_acc: 0.9020\n",
            "Epoch 59/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2009 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.9022\n",
            "Epoch 60/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2006 - acc: 0.9080 - val_loss: 0.2121 - val_acc: 0.9037\n",
            "Epoch 61/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1999 - acc: 0.9083 - val_loss: 0.2116 - val_acc: 0.9050\n",
            "Epoch 62/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1998 - acc: 0.9082 - val_loss: 0.2127 - val_acc: 0.9052\n",
            "Epoch 63/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.2002 - acc: 0.9073 - val_loss: 0.2118 - val_acc: 0.9033\n",
            "Epoch 64/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1998 - acc: 0.9076 - val_loss: 0.2137 - val_acc: 0.9017\n",
            "Epoch 65/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.1999 - acc: 0.9079 - val_loss: 0.2110 - val_acc: 0.9037\n",
            "Epoch 66/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1994 - acc: 0.9092 - val_loss: 0.2113 - val_acc: 0.9025\n",
            "Epoch 67/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1997 - acc: 0.9074 - val_loss: 0.2130 - val_acc: 0.9041\n",
            "Epoch 68/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1995 - acc: 0.9081 - val_loss: 0.2119 - val_acc: 0.9048\n",
            "Epoch 69/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1992 - acc: 0.9075 - val_loss: 0.2100 - val_acc: 0.9045\n",
            "Epoch 70/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1992 - acc: 0.9080 - val_loss: 0.2110 - val_acc: 0.9031\n",
            "Epoch 71/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1995 - acc: 0.9076 - val_loss: 0.2104 - val_acc: 0.9026\n",
            "Epoch 72/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9080 - val_loss: 0.2097 - val_acc: 0.9043\n",
            "Epoch 73/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1989 - acc: 0.9080 - val_loss: 0.2125 - val_acc: 0.9012\n",
            "Epoch 74/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1993 - acc: 0.9095 - val_loss: 0.2109 - val_acc: 0.9025\n",
            "Epoch 75/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1995 - acc: 0.9080 - val_loss: 0.2107 - val_acc: 0.9039\n",
            "Epoch 76/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9079 - val_loss: 0.2118 - val_acc: 0.9016\n",
            "Epoch 77/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9078 - val_loss: 0.2127 - val_acc: 0.9014\n",
            "Epoch 78/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1991 - acc: 0.9098 - val_loss: 0.2110 - val_acc: 0.9057\n",
            "Epoch 79/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1986 - acc: 0.9083 - val_loss: 0.2125 - val_acc: 0.9034\n",
            "Epoch 80/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1988 - acc: 0.9087 - val_loss: 0.2104 - val_acc: 0.9040\n",
            "Epoch 81/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.2112 - val_acc: 0.9050\n",
            "Epoch 82/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.2111 - val_acc: 0.9050\n",
            "Epoch 83/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1989 - acc: 0.9084 - val_loss: 0.2111 - val_acc: 0.9036\n",
            "Epoch 84/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1988 - acc: 0.9071 - val_loss: 0.2120 - val_acc: 0.9014\n",
            "Epoch 85/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1986 - acc: 0.9076 - val_loss: 0.2114 - val_acc: 0.9035\n",
            "Epoch 86/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1989 - acc: 0.9079 - val_loss: 0.2117 - val_acc: 0.9028\n",
            "Epoch 87/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1983 - acc: 0.9077 - val_loss: 0.2114 - val_acc: 0.9034\n",
            "Epoch 88/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1988 - acc: 0.9082 - val_loss: 0.2137 - val_acc: 0.9027\n",
            "Epoch 89/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1981 - acc: 0.9086 - val_loss: 0.2123 - val_acc: 0.9037\n",
            "Epoch 90/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1983 - acc: 0.9086 - val_loss: 0.2104 - val_acc: 0.9051\n",
            "Epoch 91/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1985 - acc: 0.9084 - val_loss: 0.2130 - val_acc: 0.9014\n",
            "Epoch 92/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9072 - val_loss: 0.2115 - val_acc: 0.9034\n",
            "Epoch 93/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1984 - acc: 0.9078 - val_loss: 0.2113 - val_acc: 0.9026\n",
            "Epoch 94/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9086 - val_loss: 0.2121 - val_acc: 0.9020\n",
            "Epoch 95/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1985 - acc: 0.9074 - val_loss: 0.2118 - val_acc: 0.9039\n",
            "Epoch 96/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9081 - val_loss: 0.2103 - val_acc: 0.9058\n",
            "Epoch 97/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1981 - acc: 0.9085 - val_loss: 0.2117 - val_acc: 0.9023\n",
            "Epoch 98/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9078 - val_loss: 0.2136 - val_acc: 0.9006\n",
            "Epoch 99/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9080 - val_loss: 0.2110 - val_acc: 0.9037\n",
            "Epoch 100/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9090 - val_loss: 0.2120 - val_acc: 0.9013\n",
            "Epoch 101/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9085 - val_loss: 0.2115 - val_acc: 0.9024\n",
            "Epoch 102/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2120 - val_acc: 0.9033\n",
            "Epoch 103/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9085 - val_loss: 0.2121 - val_acc: 0.9037\n",
            "Epoch 104/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9078 - val_loss: 0.2124 - val_acc: 0.9031\n",
            "Epoch 105/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9082 - val_loss: 0.2127 - val_acc: 0.9033\n",
            "Epoch 106/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9081 - val_loss: 0.2106 - val_acc: 0.9036\n",
            "Epoch 107/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9079 - val_loss: 0.2112 - val_acc: 0.9032\n",
            "Epoch 108/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1984 - acc: 0.9073 - val_loss: 0.2131 - val_acc: 0.9044\n",
            "Epoch 109/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1980 - acc: 0.9082 - val_loss: 0.2149 - val_acc: 0.9006\n",
            "Epoch 110/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9085 - val_loss: 0.2118 - val_acc: 0.9037\n",
            "Epoch 111/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9087 - val_loss: 0.2122 - val_acc: 0.9044\n",
            "Epoch 112/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1981 - acc: 0.9083 - val_loss: 0.2123 - val_acc: 0.9040\n",
            "Epoch 113/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9085 - val_loss: 0.2120 - val_acc: 0.9009\n",
            "Epoch 114/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9092 - val_loss: 0.2118 - val_acc: 0.9021\n",
            "Epoch 115/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9087 - val_loss: 0.2176 - val_acc: 0.8994\n",
            "Epoch 116/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2137 - val_acc: 0.9002\n",
            "Epoch 117/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9072 - val_loss: 0.2135 - val_acc: 0.9025\n",
            "Epoch 118/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2136 - val_acc: 0.9024\n",
            "Epoch 119/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9079 - val_loss: 0.2114 - val_acc: 0.9029\n",
            "Epoch 120/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9092 - val_loss: 0.2112 - val_acc: 0.9047\n",
            "Epoch 121/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1980 - acc: 0.9079 - val_loss: 0.2138 - val_acc: 0.9006\n",
            "Epoch 122/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1979 - acc: 0.9073 - val_loss: 0.2132 - val_acc: 0.9035\n",
            "Epoch 123/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9083 - val_loss: 0.2142 - val_acc: 0.9023\n",
            "Epoch 124/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2125 - val_acc: 0.9021\n",
            "Epoch 125/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9082 - val_loss: 0.2111 - val_acc: 0.9034\n",
            "Epoch 126/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9077 - val_loss: 0.2134 - val_acc: 0.9018\n",
            "Epoch 127/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9087 - val_loss: 0.2131 - val_acc: 0.9006\n",
            "Epoch 128/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.1977 - acc: 0.9083 - val_loss: 0.2124 - val_acc: 0.9025\n",
            "Epoch 129/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9079 - val_loss: 0.2117 - val_acc: 0.9014\n",
            "Epoch 130/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1978 - acc: 0.9083 - val_loss: 0.2135 - val_acc: 0.9010\n",
            "Epoch 131/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.2134 - val_acc: 0.9026\n",
            "Epoch 132/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9088 - val_loss: 0.2119 - val_acc: 0.9021\n",
            "Epoch 133/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9087 - val_loss: 0.2132 - val_acc: 0.9034\n",
            "Epoch 134/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9074 - val_loss: 0.2124 - val_acc: 0.9039\n",
            "Epoch 135/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1974 - acc: 0.9075 - val_loss: 0.2142 - val_acc: 0.9023\n",
            "Epoch 136/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9086 - val_loss: 0.2117 - val_acc: 0.9021\n",
            "Epoch 137/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.2122 - val_acc: 0.9039\n",
            "Epoch 138/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9089 - val_loss: 0.2125 - val_acc: 0.9021\n",
            "Epoch 139/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9082 - val_loss: 0.2142 - val_acc: 0.9016\n",
            "Epoch 140/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1971 - acc: 0.9081 - val_loss: 0.2137 - val_acc: 0.9009\n",
            "Epoch 141/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9075 - val_loss: 0.2145 - val_acc: 0.9014\n",
            "Epoch 142/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1970 - acc: 0.9089 - val_loss: 0.2124 - val_acc: 0.9028\n",
            "Epoch 143/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1972 - acc: 0.9075 - val_loss: 0.2122 - val_acc: 0.9037\n",
            "Epoch 144/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1975 - acc: 0.9079 - val_loss: 0.2138 - val_acc: 0.9005\n",
            "Epoch 145/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1971 - acc: 0.9085 - val_loss: 0.2127 - val_acc: 0.9022\n",
            "Epoch 146/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1976 - acc: 0.9082 - val_loss: 0.2125 - val_acc: 0.9018\n",
            "Epoch 147/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1977 - acc: 0.9085 - val_loss: 0.2123 - val_acc: 0.9013\n",
            "Epoch 148/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2147 - val_acc: 0.9035\n",
            "Epoch 149/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1971 - acc: 0.9086 - val_loss: 0.2132 - val_acc: 0.9021\n",
            "Epoch 150/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.1972 - acc: 0.9093 - val_loss: 0.2124 - val_acc: 0.9037\n",
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   638    684        yes\n",
            "   405    9576        no\n",
            "Accuracy: 0.9036538971954349\n",
            "Sensitivity: 0.4826021180030257\n",
            "Specificity: 0.9594229035166817\n",
            "Precision: 0.6116970278044104\n",
            "f_score: 0.5395348837209303\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuAPwy_7m9NN",
        "colab_type": "code",
        "outputId": "d4eac071-3e08-41a3-85e8-d1ba7be7c17d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "crossvalidate_NN(10,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40689 samples, validate on 4522 samples\n",
            "Epoch 1/150\n",
            "40689/40689 [==============================] - 6s 141us/step - loss: 0.2573 - acc: 0.8895 - val_loss: 0.2255 - val_acc: 0.9027\n",
            "Epoch 2/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2171 - acc: 0.9012 - val_loss: 0.2155 - val_acc: 0.9045\n",
            "Epoch 3/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2118 - acc: 0.9027 - val_loss: 0.2112 - val_acc: 0.9034\n",
            "Epoch 4/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2097 - acc: 0.9035 - val_loss: 0.2111 - val_acc: 0.9067\n",
            "Epoch 5/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2078 - acc: 0.9039 - val_loss: 0.2120 - val_acc: 0.9054\n",
            "Epoch 6/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2067 - acc: 0.9057 - val_loss: 0.2126 - val_acc: 0.9076\n",
            "Epoch 7/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2065 - acc: 0.9049 - val_loss: 0.2096 - val_acc: 0.9056\n",
            "Epoch 8/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2056 - acc: 0.9048 - val_loss: 0.2079 - val_acc: 0.9093\n",
            "Epoch 9/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2045 - acc: 0.9049 - val_loss: 0.2068 - val_acc: 0.9096\n",
            "Epoch 10/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2040 - acc: 0.9062 - val_loss: 0.2061 - val_acc: 0.9109\n",
            "Epoch 11/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2035 - acc: 0.9061 - val_loss: 0.2073 - val_acc: 0.9104\n",
            "Epoch 12/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2034 - acc: 0.9070 - val_loss: 0.2069 - val_acc: 0.9115\n",
            "Epoch 13/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2025 - acc: 0.9074 - val_loss: 0.2055 - val_acc: 0.9104\n",
            "Epoch 14/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2020 - acc: 0.9071 - val_loss: 0.2068 - val_acc: 0.9093\n",
            "Epoch 15/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2020 - acc: 0.9071 - val_loss: 0.2051 - val_acc: 0.9133\n",
            "Epoch 16/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2022 - acc: 0.9076 - val_loss: 0.2049 - val_acc: 0.9120\n",
            "Epoch 17/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2012 - acc: 0.9072 - val_loss: 0.2074 - val_acc: 0.9109\n",
            "Epoch 18/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2008 - acc: 0.9076 - val_loss: 0.2097 - val_acc: 0.9131\n",
            "Epoch 19/150\n",
            "40689/40689 [==============================] - 1s 23us/step - loss: 0.2008 - acc: 0.9071 - val_loss: 0.2047 - val_acc: 0.9111\n",
            "Epoch 20/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2009 - acc: 0.9085 - val_loss: 0.2074 - val_acc: 0.9111\n",
            "Epoch 21/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2004 - acc: 0.9080 - val_loss: 0.2052 - val_acc: 0.9129\n",
            "Epoch 22/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2005 - acc: 0.9079 - val_loss: 0.2092 - val_acc: 0.9102\n",
            "Epoch 23/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.2002 - acc: 0.9083 - val_loss: 0.2057 - val_acc: 0.9113\n",
            "Epoch 24/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2001 - acc: 0.9084 - val_loss: 0.2053 - val_acc: 0.9100\n",
            "Epoch 25/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.2000 - acc: 0.9078 - val_loss: 0.2044 - val_acc: 0.9122\n",
            "Epoch 26/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1991 - acc: 0.9083 - val_loss: 0.2092 - val_acc: 0.9102\n",
            "Epoch 27/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1994 - acc: 0.9086 - val_loss: 0.2052 - val_acc: 0.9089\n",
            "Epoch 28/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1991 - acc: 0.9088 - val_loss: 0.2046 - val_acc: 0.9098\n",
            "Epoch 29/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1993 - acc: 0.9093 - val_loss: 0.2066 - val_acc: 0.9118\n",
            "Epoch 30/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1987 - acc: 0.9092 - val_loss: 0.2042 - val_acc: 0.9102\n",
            "Epoch 31/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1995 - acc: 0.9086 - val_loss: 0.2044 - val_acc: 0.9115\n",
            "Epoch 32/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9089 - val_loss: 0.2078 - val_acc: 0.9089\n",
            "Epoch 33/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1989 - acc: 0.9084 - val_loss: 0.2045 - val_acc: 0.9113\n",
            "Epoch 34/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1983 - acc: 0.9092 - val_loss: 0.2050 - val_acc: 0.9073\n",
            "Epoch 35/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1983 - acc: 0.9085 - val_loss: 0.2047 - val_acc: 0.9115\n",
            "Epoch 36/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1980 - acc: 0.9092 - val_loss: 0.2060 - val_acc: 0.9071\n",
            "Epoch 37/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1981 - acc: 0.9097 - val_loss: 0.2040 - val_acc: 0.9111\n",
            "Epoch 38/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1974 - acc: 0.9093 - val_loss: 0.2045 - val_acc: 0.9089\n",
            "Epoch 39/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1978 - acc: 0.9096 - val_loss: 0.2043 - val_acc: 0.9111\n",
            "Epoch 40/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1983 - acc: 0.9092 - val_loss: 0.2052 - val_acc: 0.9122\n",
            "Epoch 41/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1976 - acc: 0.9103 - val_loss: 0.2039 - val_acc: 0.9091\n",
            "Epoch 42/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1975 - acc: 0.9089 - val_loss: 0.2034 - val_acc: 0.9087\n",
            "Epoch 43/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1978 - acc: 0.9087 - val_loss: 0.2045 - val_acc: 0.9100\n",
            "Epoch 44/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1973 - acc: 0.9093 - val_loss: 0.2040 - val_acc: 0.9100\n",
            "Epoch 45/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1968 - acc: 0.9097 - val_loss: 0.2053 - val_acc: 0.9102\n",
            "Epoch 46/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.2024 - val_acc: 0.9107\n",
            "Epoch 47/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9094 - val_loss: 0.2031 - val_acc: 0.9111\n",
            "Epoch 48/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1970 - acc: 0.9094 - val_loss: 0.2024 - val_acc: 0.9102\n",
            "Epoch 49/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1970 - acc: 0.9099 - val_loss: 0.2037 - val_acc: 0.9089\n",
            "Epoch 50/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9098 - val_loss: 0.2037 - val_acc: 0.9084\n",
            "Epoch 51/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1970 - acc: 0.9095 - val_loss: 0.2032 - val_acc: 0.9118\n",
            "Epoch 52/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1970 - acc: 0.9099 - val_loss: 0.2040 - val_acc: 0.9065\n",
            "Epoch 53/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1966 - acc: 0.9097 - val_loss: 0.2030 - val_acc: 0.9091\n",
            "Epoch 54/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1968 - acc: 0.9101 - val_loss: 0.2042 - val_acc: 0.9113\n",
            "Epoch 55/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2028 - val_acc: 0.9109\n",
            "Epoch 56/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9095 - val_loss: 0.2031 - val_acc: 0.9109\n",
            "Epoch 57/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9103 - val_loss: 0.2042 - val_acc: 0.9120\n",
            "Epoch 58/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2031 - val_acc: 0.9109\n",
            "Epoch 59/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1967 - acc: 0.9095 - val_loss: 0.2035 - val_acc: 0.9098\n",
            "Epoch 60/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9092 - val_loss: 0.2037 - val_acc: 0.9109\n",
            "Epoch 61/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9107 - val_loss: 0.2038 - val_acc: 0.9111\n",
            "Epoch 62/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1965 - acc: 0.9100 - val_loss: 0.2035 - val_acc: 0.9107\n",
            "Epoch 63/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9096 - val_loss: 0.2028 - val_acc: 0.9111\n",
            "Epoch 64/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1958 - acc: 0.9101 - val_loss: 0.2041 - val_acc: 0.9076\n",
            "Epoch 65/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1965 - acc: 0.9109 - val_loss: 0.2046 - val_acc: 0.9102\n",
            "Epoch 66/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9101 - val_loss: 0.2033 - val_acc: 0.9107\n",
            "Epoch 67/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9099 - val_loss: 0.2027 - val_acc: 0.9082\n",
            "Epoch 68/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9102 - val_loss: 0.2050 - val_acc: 0.9124\n",
            "Epoch 69/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9103 - val_loss: 0.2030 - val_acc: 0.9118\n",
            "Epoch 70/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1965 - acc: 0.9097 - val_loss: 0.2030 - val_acc: 0.9118\n",
            "Epoch 71/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1959 - acc: 0.9101 - val_loss: 0.2034 - val_acc: 0.9115\n",
            "Epoch 72/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9094 - val_loss: 0.2038 - val_acc: 0.9151\n",
            "Epoch 73/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9094 - val_loss: 0.2032 - val_acc: 0.9113\n",
            "Epoch 74/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9097 - val_loss: 0.2060 - val_acc: 0.9111\n",
            "Epoch 75/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9100 - val_loss: 0.2021 - val_acc: 0.9100\n",
            "Epoch 76/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9100 - val_loss: 0.2042 - val_acc: 0.9091\n",
            "Epoch 77/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1963 - acc: 0.9099 - val_loss: 0.2039 - val_acc: 0.9109\n",
            "Epoch 78/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9111 - val_loss: 0.2028 - val_acc: 0.9100\n",
            "Epoch 79/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9107 - val_loss: 0.2035 - val_acc: 0.9102\n",
            "Epoch 80/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9100 - val_loss: 0.2048 - val_acc: 0.9102\n",
            "Epoch 81/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1964 - acc: 0.9099 - val_loss: 0.2035 - val_acc: 0.9091\n",
            "Epoch 82/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9101 - val_loss: 0.2035 - val_acc: 0.9102\n",
            "Epoch 83/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9102 - val_loss: 0.2042 - val_acc: 0.9109\n",
            "Epoch 84/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9100 - val_loss: 0.2021 - val_acc: 0.9102\n",
            "Epoch 85/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1964 - acc: 0.9092 - val_loss: 0.2033 - val_acc: 0.9093\n",
            "Epoch 86/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9103 - val_loss: 0.2031 - val_acc: 0.9122\n",
            "Epoch 87/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1958 - acc: 0.9109 - val_loss: 0.2036 - val_acc: 0.9087\n",
            "Epoch 88/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9100 - val_loss: 0.2048 - val_acc: 0.9138\n",
            "Epoch 89/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1958 - acc: 0.9101 - val_loss: 0.2036 - val_acc: 0.9107\n",
            "Epoch 90/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9103 - val_loss: 0.2027 - val_acc: 0.9113\n",
            "Epoch 91/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9101 - val_loss: 0.2043 - val_acc: 0.9109\n",
            "Epoch 92/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9104 - val_loss: 0.2077 - val_acc: 0.9109\n",
            "Epoch 93/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9103 - val_loss: 0.2036 - val_acc: 0.9109\n",
            "Epoch 94/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9108 - val_loss: 0.2043 - val_acc: 0.9093\n",
            "Epoch 95/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9104 - val_loss: 0.2035 - val_acc: 0.9131\n",
            "Epoch 96/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9107 - val_loss: 0.2018 - val_acc: 0.9087\n",
            "Epoch 97/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9102 - val_loss: 0.2064 - val_acc: 0.9104\n",
            "Epoch 98/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9099 - val_loss: 0.2042 - val_acc: 0.9096\n",
            "Epoch 99/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9097 - val_loss: 0.2050 - val_acc: 0.9113\n",
            "Epoch 100/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9107 - val_loss: 0.2033 - val_acc: 0.9109\n",
            "Epoch 101/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9093 - val_loss: 0.2055 - val_acc: 0.9124\n",
            "Epoch 102/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9103 - val_loss: 0.2018 - val_acc: 0.9107\n",
            "Epoch 103/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9115 - val_loss: 0.2033 - val_acc: 0.9140\n",
            "Epoch 104/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9104 - val_loss: 0.2028 - val_acc: 0.9122\n",
            "Epoch 105/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9104 - val_loss: 0.2032 - val_acc: 0.9100\n",
            "Epoch 106/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9096 - val_loss: 0.2039 - val_acc: 0.9093\n",
            "Epoch 107/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9099 - val_loss: 0.2032 - val_acc: 0.9118\n",
            "Epoch 108/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9105 - val_loss: 0.2032 - val_acc: 0.9118\n",
            "Epoch 109/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9110 - val_loss: 0.2025 - val_acc: 0.9118\n",
            "Epoch 110/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9104 - val_loss: 0.2021 - val_acc: 0.9100\n",
            "Epoch 111/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9107 - val_loss: 0.2043 - val_acc: 0.9115\n",
            "Epoch 112/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1953 - acc: 0.9103 - val_loss: 0.2028 - val_acc: 0.9126\n",
            "Epoch 113/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9103 - val_loss: 0.2041 - val_acc: 0.9133\n",
            "Epoch 114/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9106 - val_loss: 0.2030 - val_acc: 0.9100\n",
            "Epoch 115/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9097 - val_loss: 0.2029 - val_acc: 0.9096\n",
            "Epoch 116/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9105 - val_loss: 0.2031 - val_acc: 0.9120\n",
            "Epoch 117/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9109 - val_loss: 0.2022 - val_acc: 0.9115\n",
            "Epoch 118/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9103 - val_loss: 0.2020 - val_acc: 0.9113\n",
            "Epoch 119/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9105 - val_loss: 0.2036 - val_acc: 0.9122\n",
            "Epoch 120/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9104 - val_loss: 0.2041 - val_acc: 0.9131\n",
            "Epoch 121/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9103 - val_loss: 0.2032 - val_acc: 0.9098\n",
            "Epoch 122/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9102 - val_loss: 0.2031 - val_acc: 0.9113\n",
            "Epoch 123/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9107 - val_loss: 0.2051 - val_acc: 0.9098\n",
            "Epoch 124/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9113 - val_loss: 0.2037 - val_acc: 0.9133\n",
            "Epoch 125/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1953 - acc: 0.9103 - val_loss: 0.2020 - val_acc: 0.9118\n",
            "Epoch 126/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1952 - acc: 0.9104 - val_loss: 0.2033 - val_acc: 0.9120\n",
            "Epoch 127/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9106 - val_loss: 0.2049 - val_acc: 0.9113\n",
            "Epoch 128/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9108 - val_loss: 0.2032 - val_acc: 0.9120\n",
            "Epoch 129/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9111 - val_loss: 0.2035 - val_acc: 0.9113\n",
            "Epoch 130/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1950 - acc: 0.9106 - val_loss: 0.2027 - val_acc: 0.9120\n",
            "Epoch 131/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9104 - val_loss: 0.2014 - val_acc: 0.9089\n",
            "Epoch 132/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9109 - val_loss: 0.2043 - val_acc: 0.9129\n",
            "Epoch 133/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9099 - val_loss: 0.2037 - val_acc: 0.9109\n",
            "Epoch 134/150\n",
            "40689/40689 [==============================] - 1s 23us/step - loss: 0.1951 - acc: 0.9102 - val_loss: 0.2049 - val_acc: 0.9084\n",
            "Epoch 135/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1950 - acc: 0.9110 - val_loss: 0.2055 - val_acc: 0.9129\n",
            "Epoch 136/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1951 - acc: 0.9103 - val_loss: 0.2033 - val_acc: 0.9087\n",
            "Epoch 137/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1952 - acc: 0.9111 - val_loss: 0.2041 - val_acc: 0.9109\n",
            "Epoch 138/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9097 - val_loss: 0.2034 - val_acc: 0.9111\n",
            "Epoch 139/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9105 - val_loss: 0.2030 - val_acc: 0.9113\n",
            "Epoch 140/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1955 - acc: 0.9101 - val_loss: 0.2040 - val_acc: 0.9118\n",
            "Epoch 141/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1951 - acc: 0.9108 - val_loss: 0.2049 - val_acc: 0.9113\n",
            "Epoch 142/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9103 - val_loss: 0.2035 - val_acc: 0.9120\n",
            "Epoch 143/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9108 - val_loss: 0.2038 - val_acc: 0.9124\n",
            "Epoch 144/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1953 - acc: 0.9106 - val_loss: 0.2035 - val_acc: 0.9089\n",
            "Epoch 145/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1954 - acc: 0.9105 - val_loss: 0.2072 - val_acc: 0.9118\n",
            "Epoch 146/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1951 - acc: 0.9110 - val_loss: 0.2039 - val_acc: 0.9124\n",
            "Epoch 147/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1952 - acc: 0.9104 - val_loss: 0.2042 - val_acc: 0.9102\n",
            "Epoch 148/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1952 - acc: 0.9109 - val_loss: 0.2046 - val_acc: 0.9129\n",
            "Epoch 149/150\n",
            "40689/40689 [==============================] - 1s 22us/step - loss: 0.1948 - acc: 0.9108 - val_loss: 0.2031 - val_acc: 0.9109\n",
            "Epoch 150/150\n",
            "40689/40689 [==============================] - 1s 21us/step - loss: 0.1953 - acc: 0.9103 - val_loss: 0.2061 - val_acc: 0.9096\n",
            "acc: 90.96%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 141us/step - loss: 0.2628 - acc: 0.8849 - val_loss: 0.2353 - val_acc: 0.8967\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2178 - acc: 0.9011 - val_loss: 0.2269 - val_acc: 0.8994\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2126 - acc: 0.9030 - val_loss: 0.2269 - val_acc: 0.8958\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2098 - acc: 0.9035 - val_loss: 0.2244 - val_acc: 0.8974\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2078 - acc: 0.9045 - val_loss: 0.2200 - val_acc: 0.8987\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2061 - acc: 0.9049 - val_loss: 0.2262 - val_acc: 0.9002\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2061 - acc: 0.9055 - val_loss: 0.2168 - val_acc: 0.8994\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2049 - acc: 0.9050 - val_loss: 0.2209 - val_acc: 0.9011\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2041 - acc: 0.9057 - val_loss: 0.2193 - val_acc: 0.9007\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2031 - acc: 0.9063 - val_loss: 0.2221 - val_acc: 0.8983\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2027 - acc: 0.9067 - val_loss: 0.2174 - val_acc: 0.9002\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2021 - acc: 0.9075 - val_loss: 0.2182 - val_acc: 0.8980\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2019 - acc: 0.9077 - val_loss: 0.2184 - val_acc: 0.8985\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2015 - acc: 0.9074 - val_loss: 0.2189 - val_acc: 0.8994\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2011 - acc: 0.9078 - val_loss: 0.2214 - val_acc: 0.8978\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2006 - acc: 0.9085 - val_loss: 0.2234 - val_acc: 0.8967\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2005 - acc: 0.9081 - val_loss: 0.2191 - val_acc: 0.9018\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2001 - acc: 0.9080 - val_loss: 0.2192 - val_acc: 0.8998\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2006 - acc: 0.9080 - val_loss: 0.2180 - val_acc: 0.8996\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2001 - acc: 0.9088 - val_loss: 0.2168 - val_acc: 0.8991\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1995 - acc: 0.9078 - val_loss: 0.2151 - val_acc: 0.9011\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2000 - acc: 0.9075 - val_loss: 0.2150 - val_acc: 0.9013\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2000 - acc: 0.9085 - val_loss: 0.2144 - val_acc: 0.8983\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1996 - acc: 0.9087 - val_loss: 0.2162 - val_acc: 0.9000\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9080 - val_loss: 0.2177 - val_acc: 0.8991\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1996 - acc: 0.9079 - val_loss: 0.2158 - val_acc: 0.8996\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1995 - acc: 0.9081 - val_loss: 0.2170 - val_acc: 0.9005\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1990 - acc: 0.9078 - val_loss: 0.2177 - val_acc: 0.8998\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1994 - acc: 0.9075 - val_loss: 0.2181 - val_acc: 0.8983\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1987 - acc: 0.9084 - val_loss: 0.2159 - val_acc: 0.8998\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9078 - val_loss: 0.2160 - val_acc: 0.8989\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9077 - val_loss: 0.2206 - val_acc: 0.9013\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9084 - val_loss: 0.2154 - val_acc: 0.9011\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1988 - acc: 0.9071 - val_loss: 0.2136 - val_acc: 0.9020\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.2178 - val_acc: 0.8996\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1988 - acc: 0.9083 - val_loss: 0.2174 - val_acc: 0.9005\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.2160 - val_acc: 0.9033\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1984 - acc: 0.9078 - val_loss: 0.2173 - val_acc: 0.9007\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1985 - acc: 0.9080 - val_loss: 0.2152 - val_acc: 0.9027\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1984 - acc: 0.9087 - val_loss: 0.2194 - val_acc: 0.8980\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1985 - acc: 0.9075 - val_loss: 0.2180 - val_acc: 0.9007\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1983 - acc: 0.9079 - val_loss: 0.2183 - val_acc: 0.8983\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.2187 - val_acc: 0.9002\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1982 - acc: 0.9086 - val_loss: 0.2182 - val_acc: 0.9011\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1981 - acc: 0.9075 - val_loss: 0.2158 - val_acc: 0.9011\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1977 - acc: 0.9073 - val_loss: 0.2152 - val_acc: 0.9020\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.2144 - val_acc: 0.9038\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9081 - val_loss: 0.2204 - val_acc: 0.9029\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9085 - val_loss: 0.2169 - val_acc: 0.9009\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1978 - acc: 0.9081 - val_loss: 0.2165 - val_acc: 0.8998\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9081 - val_loss: 0.2165 - val_acc: 0.9007\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1978 - acc: 0.9083 - val_loss: 0.2183 - val_acc: 0.9000\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9085 - val_loss: 0.2178 - val_acc: 0.9018\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9085 - val_loss: 0.2161 - val_acc: 0.9007\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9089 - val_loss: 0.2177 - val_acc: 0.9002\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9083 - val_loss: 0.2150 - val_acc: 0.9011\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9078 - val_loss: 0.2196 - val_acc: 0.8965\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1977 - acc: 0.9085 - val_loss: 0.2149 - val_acc: 0.9018\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1975 - acc: 0.9084 - val_loss: 0.2163 - val_acc: 0.9013\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1977 - acc: 0.9080 - val_loss: 0.2166 - val_acc: 0.9020\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2187 - val_acc: 0.9044\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9091 - val_loss: 0.2146 - val_acc: 0.9044\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1973 - acc: 0.9082 - val_loss: 0.2139 - val_acc: 0.9031\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1973 - acc: 0.9087 - val_loss: 0.2161 - val_acc: 0.8985\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9083 - val_loss: 0.2177 - val_acc: 0.9000\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9090 - val_loss: 0.2144 - val_acc: 0.9027\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1972 - acc: 0.9089 - val_loss: 0.2136 - val_acc: 0.9022\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9089 - val_loss: 0.2167 - val_acc: 0.9000\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9092 - val_loss: 0.2144 - val_acc: 0.9013\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9088 - val_loss: 0.2139 - val_acc: 0.9020\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9083 - val_loss: 0.2134 - val_acc: 0.9007\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1971 - acc: 0.9082 - val_loss: 0.2146 - val_acc: 0.8998\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9084 - val_loss: 0.2146 - val_acc: 0.8994\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1973 - acc: 0.9090 - val_loss: 0.2150 - val_acc: 0.8987\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1972 - acc: 0.9084 - val_loss: 0.2184 - val_acc: 0.9000\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1969 - acc: 0.9090 - val_loss: 0.2201 - val_acc: 0.8998\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1968 - acc: 0.9083 - val_loss: 0.2161 - val_acc: 0.9022\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9086 - val_loss: 0.2168 - val_acc: 0.9033\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9086 - val_loss: 0.2153 - val_acc: 0.8980\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9092 - val_loss: 0.2176 - val_acc: 0.9029\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9090 - val_loss: 0.2145 - val_acc: 0.9033\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9087 - val_loss: 0.2188 - val_acc: 0.9013\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9088 - val_loss: 0.2148 - val_acc: 0.9025\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9094 - val_loss: 0.2160 - val_acc: 0.8998\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9084 - val_loss: 0.2139 - val_acc: 0.9022\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1967 - acc: 0.9079 - val_loss: 0.2173 - val_acc: 0.9005\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9090 - val_loss: 0.2160 - val_acc: 0.9007\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9087 - val_loss: 0.2145 - val_acc: 0.8987\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9094 - val_loss: 0.2173 - val_acc: 0.9016\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9086 - val_loss: 0.2162 - val_acc: 0.9031\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1963 - acc: 0.9090 - val_loss: 0.2140 - val_acc: 0.9009\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9097 - val_loss: 0.2146 - val_acc: 0.9036\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9094 - val_loss: 0.2169 - val_acc: 0.9027\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9092 - val_loss: 0.2148 - val_acc: 0.9005\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9086 - val_loss: 0.2151 - val_acc: 0.9025\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9086 - val_loss: 0.2143 - val_acc: 0.9022\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9084 - val_loss: 0.2155 - val_acc: 0.9031\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9088 - val_loss: 0.2129 - val_acc: 0.9018\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9085 - val_loss: 0.2177 - val_acc: 0.9009\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9090 - val_loss: 0.2152 - val_acc: 0.8989\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9095 - val_loss: 0.2187 - val_acc: 0.9038\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9080 - val_loss: 0.2159 - val_acc: 0.9016\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9088 - val_loss: 0.2169 - val_acc: 0.9000\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9094 - val_loss: 0.2157 - val_acc: 0.9018\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9089 - val_loss: 0.2160 - val_acc: 0.9025\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9087 - val_loss: 0.2164 - val_acc: 0.9036\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9093 - val_loss: 0.2155 - val_acc: 0.9007\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9089 - val_loss: 0.2139 - val_acc: 0.9009\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9087 - val_loss: 0.2158 - val_acc: 0.9025\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9096 - val_loss: 0.2164 - val_acc: 0.9007\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1966 - acc: 0.9083 - val_loss: 0.2150 - val_acc: 0.9031\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1961 - acc: 0.9089 - val_loss: 0.2144 - val_acc: 0.9020\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9086 - val_loss: 0.2152 - val_acc: 0.8978\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9091 - val_loss: 0.2138 - val_acc: 0.9002\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9089 - val_loss: 0.2168 - val_acc: 0.8985\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9100 - val_loss: 0.2168 - val_acc: 0.9025\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9084 - val_loss: 0.2181 - val_acc: 0.9009\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9092 - val_loss: 0.2152 - val_acc: 0.9036\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1963 - acc: 0.9097 - val_loss: 0.2177 - val_acc: 0.8998\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1962 - acc: 0.9090 - val_loss: 0.2151 - val_acc: 0.9020\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9093 - val_loss: 0.2165 - val_acc: 0.9016\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9083 - val_loss: 0.2173 - val_acc: 0.8989\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9093 - val_loss: 0.2143 - val_acc: 0.8998\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9093 - val_loss: 0.2156 - val_acc: 0.9020\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9098 - val_loss: 0.2175 - val_acc: 0.9002\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9088 - val_loss: 0.2153 - val_acc: 0.8976\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9091 - val_loss: 0.2168 - val_acc: 0.9020\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1965 - acc: 0.9089 - val_loss: 0.2197 - val_acc: 0.8996\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1961 - acc: 0.9093 - val_loss: 0.2148 - val_acc: 0.9018\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9090 - val_loss: 0.2140 - val_acc: 0.9022\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9092 - val_loss: 0.2149 - val_acc: 0.9018\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9092 - val_loss: 0.2169 - val_acc: 0.8985\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9086 - val_loss: 0.2176 - val_acc: 0.9020\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9094 - val_loss: 0.2147 - val_acc: 0.9013\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9094 - val_loss: 0.2178 - val_acc: 0.9029\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9088 - val_loss: 0.2133 - val_acc: 0.9007\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1964 - acc: 0.9087 - val_loss: 0.2150 - val_acc: 0.9020\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1960 - acc: 0.9082 - val_loss: 0.2168 - val_acc: 0.9036\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1958 - acc: 0.9090 - val_loss: 0.2153 - val_acc: 0.9009\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9088 - val_loss: 0.2173 - val_acc: 0.9016\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9095 - val_loss: 0.2151 - val_acc: 0.8998\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9098 - val_loss: 0.2145 - val_acc: 0.8991\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1957 - acc: 0.9085 - val_loss: 0.2126 - val_acc: 0.9007\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1959 - acc: 0.9088 - val_loss: 0.2177 - val_acc: 0.8983\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1957 - acc: 0.9095 - val_loss: 0.2149 - val_acc: 0.9016\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1956 - acc: 0.9096 - val_loss: 0.2168 - val_acc: 0.9029\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9085 - val_loss: 0.2139 - val_acc: 0.9005\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.1960 - acc: 0.9090 - val_loss: 0.2153 - val_acc: 0.9025\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1955 - acc: 0.9093 - val_loss: 0.2175 - val_acc: 0.9011\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1958 - acc: 0.9093 - val_loss: 0.2179 - val_acc: 0.9018\n",
            "acc: 90.18%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 143us/step - loss: 0.2483 - acc: 0.8971 - val_loss: 0.2098 - val_acc: 0.9020\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2182 - acc: 0.9032 - val_loss: 0.2092 - val_acc: 0.9044\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2135 - acc: 0.9049 - val_loss: 0.2068 - val_acc: 0.9031\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2107 - acc: 0.9055 - val_loss: 0.2040 - val_acc: 0.9040\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.2086 - acc: 0.9056 - val_loss: 0.2051 - val_acc: 0.9033\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2068 - acc: 0.9069 - val_loss: 0.2044 - val_acc: 0.9067\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2060 - acc: 0.9060 - val_loss: 0.2037 - val_acc: 0.9060\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2053 - acc: 0.9058 - val_loss: 0.2053 - val_acc: 0.9064\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2053 - acc: 0.9068 - val_loss: 0.2002 - val_acc: 0.9078\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2039 - acc: 0.9072 - val_loss: 0.2017 - val_acc: 0.9016\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2040 - acc: 0.9075 - val_loss: 0.1982 - val_acc: 0.9047\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2030 - acc: 0.9075 - val_loss: 0.1949 - val_acc: 0.9067\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2025 - acc: 0.9079 - val_loss: 0.1983 - val_acc: 0.9058\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2018 - acc: 0.9091 - val_loss: 0.1973 - val_acc: 0.9086\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2013 - acc: 0.9092 - val_loss: 0.1953 - val_acc: 0.9093\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2008 - acc: 0.9088 - val_loss: 0.1967 - val_acc: 0.9100\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2003 - acc: 0.9091 - val_loss: 0.1957 - val_acc: 0.9089\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.2005 - acc: 0.9098 - val_loss: 0.1955 - val_acc: 0.9102\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9098 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 2s 37us/step - loss: 0.2000 - acc: 0.9099 - val_loss: 0.1946 - val_acc: 0.9104\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 32us/step - loss: 0.1997 - acc: 0.9094 - val_loss: 0.1952 - val_acc: 0.9084\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1998 - acc: 0.9099 - val_loss: 0.1942 - val_acc: 0.9082\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9100 - val_loss: 0.1976 - val_acc: 0.9084\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9101 - val_loss: 0.1955 - val_acc: 0.9071\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1991 - acc: 0.9096 - val_loss: 0.1951 - val_acc: 0.9080\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9094 - val_loss: 0.1974 - val_acc: 0.9093\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9100 - val_loss: 0.1968 - val_acc: 0.9080\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9100 - val_loss: 0.1933 - val_acc: 0.9100\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9102 - val_loss: 0.1965 - val_acc: 0.9078\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9094 - val_loss: 0.1964 - val_acc: 0.9089\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9100 - val_loss: 0.1939 - val_acc: 0.9106\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9102 - val_loss: 0.1952 - val_acc: 0.9111\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9099 - val_loss: 0.1952 - val_acc: 0.9086\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9106 - val_loss: 0.1966 - val_acc: 0.9073\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9106 - val_loss: 0.1953 - val_acc: 0.9089\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9101 - val_loss: 0.1945 - val_acc: 0.9093\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9097 - val_loss: 0.1952 - val_acc: 0.9093\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1986 - acc: 0.9090 - val_loss: 0.1955 - val_acc: 0.9117\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9095 - val_loss: 0.1939 - val_acc: 0.9098\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9102 - val_loss: 0.1944 - val_acc: 0.9089\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9097 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9101 - val_loss: 0.1962 - val_acc: 0.9089\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9098 - val_loss: 0.1930 - val_acc: 0.9100\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9102 - val_loss: 0.1936 - val_acc: 0.9078\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9098 - val_loss: 0.1942 - val_acc: 0.9080\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9099 - val_loss: 0.1963 - val_acc: 0.9086\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9095 - val_loss: 0.1958 - val_acc: 0.9044\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9110 - val_loss: 0.1947 - val_acc: 0.9098\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1978 - acc: 0.9100 - val_loss: 0.1939 - val_acc: 0.9095\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9110 - val_loss: 0.1965 - val_acc: 0.9089\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9104 - val_loss: 0.1957 - val_acc: 0.9080\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9100 - val_loss: 0.1924 - val_acc: 0.9100\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9114 - val_loss: 0.1945 - val_acc: 0.9117\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9104 - val_loss: 0.1950 - val_acc: 0.9109\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9102 - val_loss: 0.1930 - val_acc: 0.9126\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9109 - val_loss: 0.1938 - val_acc: 0.9093\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9101 - val_loss: 0.1946 - val_acc: 0.9100\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9103 - val_loss: 0.1958 - val_acc: 0.9111\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9095 - val_loss: 0.1942 - val_acc: 0.9093\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9103 - val_loss: 0.1951 - val_acc: 0.9069\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9105 - val_loss: 0.1946 - val_acc: 0.9091\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9101 - val_loss: 0.1941 - val_acc: 0.9111\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9107 - val_loss: 0.1954 - val_acc: 0.9115\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9103 - val_loss: 0.1935 - val_acc: 0.9095\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9095 - val_loss: 0.1939 - val_acc: 0.9104\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9111 - val_loss: 0.1932 - val_acc: 0.9095\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9101 - val_loss: 0.1966 - val_acc: 0.9098\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9105 - val_loss: 0.1941 - val_acc: 0.9069\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9112 - val_loss: 0.1976 - val_acc: 0.9060\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9105 - val_loss: 0.1941 - val_acc: 0.9111\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9105 - val_loss: 0.1940 - val_acc: 0.9091\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9112 - val_loss: 0.1946 - val_acc: 0.9086\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9101 - val_loss: 0.1943 - val_acc: 0.9082\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9108 - val_loss: 0.1937 - val_acc: 0.9113\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9101 - val_loss: 0.1935 - val_acc: 0.9102\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9107 - val_loss: 0.1937 - val_acc: 0.9093\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9107 - val_loss: 0.1951 - val_acc: 0.9106\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9104 - val_loss: 0.1950 - val_acc: 0.9089\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9098 - val_loss: 0.1955 - val_acc: 0.9115\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9109 - val_loss: 0.1946 - val_acc: 0.9102\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9113 - val_loss: 0.1946 - val_acc: 0.9109\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.1944 - val_acc: 0.9067\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9101 - val_loss: 0.1954 - val_acc: 0.9075\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9100 - val_loss: 0.1961 - val_acc: 0.9124\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9102 - val_loss: 0.1950 - val_acc: 0.9093\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9106 - val_loss: 0.1945 - val_acc: 0.9102\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9103 - val_loss: 0.1945 - val_acc: 0.9084\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9108 - val_loss: 0.1932 - val_acc: 0.9098\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9102 - val_loss: 0.1937 - val_acc: 0.9117\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9106 - val_loss: 0.1943 - val_acc: 0.9086\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9108 - val_loss: 0.1941 - val_acc: 0.9082\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9113 - val_loss: 0.1949 - val_acc: 0.9084\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9106 - val_loss: 0.1940 - val_acc: 0.9089\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9101 - val_loss: 0.1960 - val_acc: 0.9091\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9119 - val_loss: 0.1928 - val_acc: 0.9095\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9110 - val_loss: 0.1937 - val_acc: 0.9091\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9105 - val_loss: 0.1946 - val_acc: 0.9084\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9108 - val_loss: 0.1943 - val_acc: 0.9098\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9105 - val_loss: 0.1952 - val_acc: 0.9111\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9107 - val_loss: 0.1945 - val_acc: 0.9086\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9109 - val_loss: 0.1946 - val_acc: 0.9117\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9111 - val_loss: 0.1942 - val_acc: 0.9104\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9102 - val_loss: 0.1937 - val_acc: 0.9120\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9104 - val_loss: 0.1941 - val_acc: 0.9091\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9108 - val_loss: 0.1944 - val_acc: 0.9093\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9108 - val_loss: 0.1944 - val_acc: 0.9102\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9106 - val_loss: 0.1939 - val_acc: 0.9111\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9104 - val_loss: 0.1932 - val_acc: 0.9093\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9107 - val_loss: 0.1944 - val_acc: 0.9115\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9110 - val_loss: 0.1945 - val_acc: 0.9078\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9104 - val_loss: 0.1945 - val_acc: 0.9089\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9106 - val_loss: 0.1930 - val_acc: 0.9102\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9112 - val_loss: 0.1969 - val_acc: 0.9111\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9110 - val_loss: 0.1941 - val_acc: 0.9095\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1958 - acc: 0.9107 - val_loss: 0.1932 - val_acc: 0.9084\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9111 - val_loss: 0.1942 - val_acc: 0.9100\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1955 - val_acc: 0.9104\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9100 - val_loss: 0.1961 - val_acc: 0.9104\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9105 - val_loss: 0.1964 - val_acc: 0.9102\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9107 - val_loss: 0.1929 - val_acc: 0.9098\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9106 - val_loss: 0.1949 - val_acc: 0.9104\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9107 - val_loss: 0.1939 - val_acc: 0.9106\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9108 - val_loss: 0.1942 - val_acc: 0.9106\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9112 - val_loss: 0.1931 - val_acc: 0.9104\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9114 - val_loss: 0.1981 - val_acc: 0.9120\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9115 - val_loss: 0.1962 - val_acc: 0.9091\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1959 - acc: 0.9109 - val_loss: 0.1938 - val_acc: 0.9084\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9114 - val_loss: 0.1961 - val_acc: 0.9078\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1960 - acc: 0.9111 - val_loss: 0.1957 - val_acc: 0.9106\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9110 - val_loss: 0.1938 - val_acc: 0.9084\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1963 - acc: 0.9111 - val_loss: 0.1956 - val_acc: 0.9082\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1965 - acc: 0.9102 - val_loss: 0.1943 - val_acc: 0.9089\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9106 - val_loss: 0.1934 - val_acc: 0.9091\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1962 - acc: 0.9108 - val_loss: 0.1948 - val_acc: 0.9111\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9110 - val_loss: 0.1939 - val_acc: 0.9082\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9110 - val_loss: 0.1924 - val_acc: 0.9080\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9112 - val_loss: 0.1955 - val_acc: 0.9100\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1958 - acc: 0.9107 - val_loss: 0.1938 - val_acc: 0.9069\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9105 - val_loss: 0.1919 - val_acc: 0.9106\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9112 - val_loss: 0.1953 - val_acc: 0.9073\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1941 - val_acc: 0.9086\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1947 - val_acc: 0.9109\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9109 - val_loss: 0.1953 - val_acc: 0.9098\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1960 - acc: 0.9111 - val_loss: 0.1935 - val_acc: 0.9064\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1961 - acc: 0.9111 - val_loss: 0.1925 - val_acc: 0.9093\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1961 - acc: 0.9112 - val_loss: 0.1929 - val_acc: 0.9089\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9107 - val_loss: 0.1930 - val_acc: 0.9084\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9109 - val_loss: 0.1933 - val_acc: 0.9069\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.1950 - val_acc: 0.9078\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9111 - val_loss: 0.1946 - val_acc: 0.9098\n",
            "acc: 90.98%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 149us/step - loss: 0.2972 - acc: 0.8691 - val_loss: 0.2143 - val_acc: 0.9073\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2205 - acc: 0.9016 - val_loss: 0.2110 - val_acc: 0.9058\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2153 - acc: 0.9040 - val_loss: 0.2056 - val_acc: 0.9124\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2124 - acc: 0.9040 - val_loss: 0.2082 - val_acc: 0.9084\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2107 - acc: 0.9048 - val_loss: 0.2039 - val_acc: 0.9100\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2090 - acc: 0.9069 - val_loss: 0.2037 - val_acc: 0.9137\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2084 - acc: 0.9063 - val_loss: 0.2020 - val_acc: 0.9122\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2082 - acc: 0.9067 - val_loss: 0.2012 - val_acc: 0.9115\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2073 - acc: 0.9068 - val_loss: 0.2001 - val_acc: 0.9109\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2066 - acc: 0.9072 - val_loss: 0.2016 - val_acc: 0.9086\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2064 - acc: 0.9068 - val_loss: 0.1981 - val_acc: 0.9120\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2059 - acc: 0.9071 - val_loss: 0.2020 - val_acc: 0.9086\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2059 - acc: 0.9066 - val_loss: 0.2007 - val_acc: 0.9111\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2052 - acc: 0.9070 - val_loss: 0.1981 - val_acc: 0.9120\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2054 - acc: 0.9072 - val_loss: 0.1971 - val_acc: 0.9122\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2048 - acc: 0.9067 - val_loss: 0.1963 - val_acc: 0.9117\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2044 - acc: 0.9066 - val_loss: 0.1992 - val_acc: 0.9117\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2039 - acc: 0.9069 - val_loss: 0.1980 - val_acc: 0.9117\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2037 - acc: 0.9080 - val_loss: 0.1944 - val_acc: 0.9104\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2036 - acc: 0.9068 - val_loss: 0.1980 - val_acc: 0.9098\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2032 - acc: 0.9072 - val_loss: 0.1973 - val_acc: 0.9131\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2030 - acc: 0.9072 - val_loss: 0.1969 - val_acc: 0.9106\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9074 - val_loss: 0.1942 - val_acc: 0.9106\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2029 - acc: 0.9068 - val_loss: 0.1976 - val_acc: 0.9111\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2020 - acc: 0.9079 - val_loss: 0.1961 - val_acc: 0.9120\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2025 - acc: 0.9073 - val_loss: 0.1997 - val_acc: 0.9111\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2023 - acc: 0.9069 - val_loss: 0.1951 - val_acc: 0.9102\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2016 - acc: 0.9072 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2016 - acc: 0.9077 - val_loss: 0.1966 - val_acc: 0.9084\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2011 - acc: 0.9081 - val_loss: 0.1991 - val_acc: 0.9098\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2013 - acc: 0.9069 - val_loss: 0.1958 - val_acc: 0.9115\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9070 - val_loss: 0.1973 - val_acc: 0.9078\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9071 - val_loss: 0.1967 - val_acc: 0.9102\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2009 - acc: 0.9071 - val_loss: 0.1955 - val_acc: 0.9115\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2005 - acc: 0.9084 - val_loss: 0.1961 - val_acc: 0.9111\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2005 - acc: 0.9081 - val_loss: 0.1958 - val_acc: 0.9102\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2006 - acc: 0.9084 - val_loss: 0.1966 - val_acc: 0.9089\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9074 - val_loss: 0.1945 - val_acc: 0.9098\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9081 - val_loss: 0.1974 - val_acc: 0.9082\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9072 - val_loss: 0.1951 - val_acc: 0.9095\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9079 - val_loss: 0.1960 - val_acc: 0.9093\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2002 - acc: 0.9075 - val_loss: 0.1959 - val_acc: 0.9113\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9083 - val_loss: 0.1962 - val_acc: 0.9111\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9079 - val_loss: 0.1969 - val_acc: 0.9091\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2004 - acc: 0.9072 - val_loss: 0.1976 - val_acc: 0.9109\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9079 - val_loss: 0.1978 - val_acc: 0.9078\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1998 - acc: 0.9084 - val_loss: 0.1963 - val_acc: 0.9075\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1999 - acc: 0.9077 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1997 - acc: 0.9080 - val_loss: 0.1970 - val_acc: 0.9086\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9078 - val_loss: 0.1976 - val_acc: 0.9073\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2000 - acc: 0.9077 - val_loss: 0.1988 - val_acc: 0.9058\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1998 - acc: 0.9078 - val_loss: 0.1979 - val_acc: 0.9089\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1996 - acc: 0.9080 - val_loss: 0.1959 - val_acc: 0.9086\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9089 - val_loss: 0.1965 - val_acc: 0.9093\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9078 - val_loss: 0.1989 - val_acc: 0.9078\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1999 - acc: 0.9083 - val_loss: 0.2021 - val_acc: 0.9091\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9080 - val_loss: 0.1994 - val_acc: 0.9036\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9082 - val_loss: 0.1972 - val_acc: 0.9078\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9073 - val_loss: 0.1985 - val_acc: 0.9084\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1996 - acc: 0.9087 - val_loss: 0.1982 - val_acc: 0.9098\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9074 - val_loss: 0.1967 - val_acc: 0.9095\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9088 - val_loss: 0.1973 - val_acc: 0.9109\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9085 - val_loss: 0.1954 - val_acc: 0.9098\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1994 - acc: 0.9085 - val_loss: 0.1958 - val_acc: 0.9069\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9082 - val_loss: 0.1979 - val_acc: 0.9071\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9095 - val_loss: 0.1986 - val_acc: 0.9053\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9085 - val_loss: 0.1974 - val_acc: 0.9122\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9095\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9083 - val_loss: 0.1977 - val_acc: 0.9069\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9080 - val_loss: 0.1959 - val_acc: 0.9080\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9085 - val_loss: 0.1975 - val_acc: 0.9106\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9088 - val_loss: 0.1967 - val_acc: 0.9086\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9077 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9077 - val_loss: 0.1985 - val_acc: 0.9098\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9081 - val_loss: 0.1964 - val_acc: 0.9100\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9082 - val_loss: 0.1966 - val_acc: 0.9115\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9086 - val_loss: 0.1960 - val_acc: 0.9106\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9089 - val_loss: 0.1949 - val_acc: 0.9086\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1996 - acc: 0.9075 - val_loss: 0.1972 - val_acc: 0.9102\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9091 - val_loss: 0.1957 - val_acc: 0.9124\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9086 - val_loss: 0.1969 - val_acc: 0.9115\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9102\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9087 - val_loss: 0.1974 - val_acc: 0.9084\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.1963 - val_acc: 0.9104\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9084 - val_loss: 0.1957 - val_acc: 0.9095\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1973 - val_acc: 0.9106\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9088 - val_loss: 0.1971 - val_acc: 0.9082\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9079 - val_loss: 0.1985 - val_acc: 0.9106\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9081 - val_loss: 0.1998 - val_acc: 0.9111\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9084 - val_loss: 0.1960 - val_acc: 0.9073\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1991 - acc: 0.9088 - val_loss: 0.1966 - val_acc: 0.9102\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9084 - val_loss: 0.1977 - val_acc: 0.9078\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9084 - val_loss: 0.1962 - val_acc: 0.9075\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.1969 - val_acc: 0.9104\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9082 - val_loss: 0.1960 - val_acc: 0.9086\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9087 - val_loss: 0.1957 - val_acc: 0.9102\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9080 - val_loss: 0.1978 - val_acc: 0.9075\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1988 - val_acc: 0.9126\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9082 - val_loss: 0.1957 - val_acc: 0.9084\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9088 - val_loss: 0.1958 - val_acc: 0.9095\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9088 - val_loss: 0.1958 - val_acc: 0.9102\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.1966 - val_acc: 0.9100\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9086 - val_loss: 0.1961 - val_acc: 0.9111\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1969 - val_acc: 0.9111\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9087 - val_loss: 0.1974 - val_acc: 0.9091\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9087 - val_loss: 0.1942 - val_acc: 0.9100\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9090 - val_loss: 0.1955 - val_acc: 0.9086\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9080 - val_loss: 0.1969 - val_acc: 0.9069\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9092 - val_loss: 0.1951 - val_acc: 0.9069\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9087 - val_loss: 0.1957 - val_acc: 0.9080\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9090 - val_loss: 0.1974 - val_acc: 0.9106\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9089 - val_loss: 0.1968 - val_acc: 0.9093\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9087 - val_loss: 0.1961 - val_acc: 0.9093\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9086 - val_loss: 0.1970 - val_acc: 0.9091\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9088 - val_loss: 0.1962 - val_acc: 0.9129\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9091 - val_loss: 0.1962 - val_acc: 0.9124\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9089 - val_loss: 0.1956 - val_acc: 0.9120\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9086 - val_loss: 0.1943 - val_acc: 0.9095\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9083 - val_loss: 0.1949 - val_acc: 0.9080\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9089 - val_loss: 0.1965 - val_acc: 0.9122\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9079 - val_loss: 0.2027 - val_acc: 0.9080\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.1971 - val_acc: 0.9120\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9089 - val_loss: 0.1967 - val_acc: 0.9098\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9088 - val_loss: 0.1961 - val_acc: 0.9073\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9086 - val_loss: 0.1957 - val_acc: 0.9122\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9086 - val_loss: 0.1950 - val_acc: 0.9084\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9082 - val_loss: 0.1946 - val_acc: 0.9091\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9089 - val_loss: 0.1954 - val_acc: 0.9098\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9095 - val_loss: 0.1965 - val_acc: 0.9102\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9086 - val_loss: 0.1953 - val_acc: 0.9098\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9089 - val_loss: 0.1947 - val_acc: 0.9104\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.1954 - val_acc: 0.9102\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.1967 - val_acc: 0.9102\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9088 - val_loss: 0.1948 - val_acc: 0.9106\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1986 - acc: 0.9090 - val_loss: 0.1974 - val_acc: 0.9100\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9089 - val_loss: 0.1962 - val_acc: 0.9102\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9087 - val_loss: 0.1962 - val_acc: 0.9106\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9088 - val_loss: 0.1968 - val_acc: 0.9113\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9085 - val_loss: 0.1966 - val_acc: 0.9093\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9088 - val_loss: 0.1962 - val_acc: 0.9106\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9085 - val_loss: 0.1959 - val_acc: 0.9115\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9093 - val_loss: 0.1969 - val_acc: 0.9120\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9085 - val_loss: 0.1978 - val_acc: 0.9093\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9091 - val_loss: 0.1960 - val_acc: 0.9113\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9088 - val_loss: 0.1943 - val_acc: 0.9104\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1988 - acc: 0.9082 - val_loss: 0.1953 - val_acc: 0.9111\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9085 - val_loss: 0.1962 - val_acc: 0.9102\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1987 - acc: 0.9095 - val_loss: 0.1973 - val_acc: 0.9089\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1983 - acc: 0.9083 - val_loss: 0.1970 - val_acc: 0.9089\n",
            "acc: 90.89%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 150us/step - loss: 0.2520 - acc: 0.8925 - val_loss: 0.2233 - val_acc: 0.9027\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2176 - acc: 0.9017 - val_loss: 0.2174 - val_acc: 0.9058\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2140 - acc: 0.9033 - val_loss: 0.2175 - val_acc: 0.9047\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2116 - acc: 0.9048 - val_loss: 0.2162 - val_acc: 0.9049\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2100 - acc: 0.9060 - val_loss: 0.2148 - val_acc: 0.9086\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2080 - acc: 0.9039 - val_loss: 0.2130 - val_acc: 0.9084\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2069 - acc: 0.9049 - val_loss: 0.2120 - val_acc: 0.9067\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2059 - acc: 0.9053 - val_loss: 0.2111 - val_acc: 0.9084\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2055 - acc: 0.9058 - val_loss: 0.2115 - val_acc: 0.9078\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9065 - val_loss: 0.2126 - val_acc: 0.9067\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2035 - acc: 0.9067 - val_loss: 0.2104 - val_acc: 0.9064\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2031 - acc: 0.9066 - val_loss: 0.2096 - val_acc: 0.9067\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9069 - val_loss: 0.2094 - val_acc: 0.9084\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2025 - acc: 0.9068 - val_loss: 0.2094 - val_acc: 0.9056\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2024 - acc: 0.9070 - val_loss: 0.2088 - val_acc: 0.9060\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9071 - val_loss: 0.2086 - val_acc: 0.9075\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2018 - acc: 0.9074 - val_loss: 0.2104 - val_acc: 0.9089\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2011 - acc: 0.9083 - val_loss: 0.2093 - val_acc: 0.9086\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9087 - val_loss: 0.2079 - val_acc: 0.9104\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9083 - val_loss: 0.2079 - val_acc: 0.9073\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9072 - val_loss: 0.2102 - val_acc: 0.9049\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9082 - val_loss: 0.2095 - val_acc: 0.9064\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1999 - acc: 0.9084 - val_loss: 0.2075 - val_acc: 0.9086\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2005 - acc: 0.9078 - val_loss: 0.2071 - val_acc: 0.9078\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9078 - val_loss: 0.2074 - val_acc: 0.9080\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1997 - acc: 0.9087 - val_loss: 0.2067 - val_acc: 0.9124\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9093 - val_loss: 0.2071 - val_acc: 0.9058\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9083 - val_loss: 0.2087 - val_acc: 0.9106\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1995 - acc: 0.9086 - val_loss: 0.2062 - val_acc: 0.9104\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2092 - val_acc: 0.9073\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9097 - val_loss: 0.2101 - val_acc: 0.9102\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1993 - acc: 0.9087 - val_loss: 0.2072 - val_acc: 0.9078\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1990 - acc: 0.9082 - val_loss: 0.2071 - val_acc: 0.9080\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 22us/step - loss: 0.1989 - acc: 0.9089 - val_loss: 0.2088 - val_acc: 0.9091\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9092 - val_loss: 0.2073 - val_acc: 0.9093\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1985 - acc: 0.9092 - val_loss: 0.2085 - val_acc: 0.9080\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1992 - acc: 0.9084 - val_loss: 0.2048 - val_acc: 0.9080\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9090 - val_loss: 0.2102 - val_acc: 0.9086\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1989 - acc: 0.9087 - val_loss: 0.2081 - val_acc: 0.9078\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9092 - val_loss: 0.2074 - val_acc: 0.9075\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9088 - val_loss: 0.2062 - val_acc: 0.9080\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9092 - val_loss: 0.2065 - val_acc: 0.9089\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9088 - val_loss: 0.2055 - val_acc: 0.9080\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9087 - val_loss: 0.2064 - val_acc: 0.9093\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9095 - val_loss: 0.2066 - val_acc: 0.9100\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9089 - val_loss: 0.2044 - val_acc: 0.9084\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9085 - val_loss: 0.2059 - val_acc: 0.9073\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9088 - val_loss: 0.2061 - val_acc: 0.9073\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9091 - val_loss: 0.2058 - val_acc: 0.9080\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1982 - acc: 0.9094 - val_loss: 0.2081 - val_acc: 0.9049\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9089 - val_loss: 0.2071 - val_acc: 0.9095\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9096 - val_loss: 0.2069 - val_acc: 0.9080\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1984 - acc: 0.9094 - val_loss: 0.2080 - val_acc: 0.9062\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9094 - val_loss: 0.2077 - val_acc: 0.9117\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9096 - val_loss: 0.2064 - val_acc: 0.9086\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9091 - val_loss: 0.2078 - val_acc: 0.9084\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9086 - val_loss: 0.2062 - val_acc: 0.9084\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1980 - acc: 0.9093 - val_loss: 0.2051 - val_acc: 0.9102\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9097 - val_loss: 0.2062 - val_acc: 0.9100\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1979 - acc: 0.9092 - val_loss: 0.2058 - val_acc: 0.9080\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9092 - val_loss: 0.2071 - val_acc: 0.9093\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1977 - acc: 0.9089 - val_loss: 0.2085 - val_acc: 0.9084\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9092 - val_loss: 0.2069 - val_acc: 0.9078\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1981 - acc: 0.9101 - val_loss: 0.2074 - val_acc: 0.9089\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9102 - val_loss: 0.2086 - val_acc: 0.9095\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9092 - val_loss: 0.2063 - val_acc: 0.9095\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9095 - val_loss: 0.2080 - val_acc: 0.9086\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9092 - val_loss: 0.2072 - val_acc: 0.9086\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9098 - val_loss: 0.2094 - val_acc: 0.9095\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9093 - val_loss: 0.2068 - val_acc: 0.9086\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9097 - val_loss: 0.2073 - val_acc: 0.9091\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9101 - val_loss: 0.2075 - val_acc: 0.9095\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9099 - val_loss: 0.2066 - val_acc: 0.9106\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9100 - val_loss: 0.2067 - val_acc: 0.9095\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9095 - val_loss: 0.2082 - val_acc: 0.9082\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9096 - val_loss: 0.2085 - val_acc: 0.9095\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.2058 - val_acc: 0.9098\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9094 - val_loss: 0.2068 - val_acc: 0.9080\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1975 - acc: 0.9097 - val_loss: 0.2065 - val_acc: 0.9080\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9097 - val_loss: 0.2086 - val_acc: 0.9071\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9096 - val_loss: 0.2088 - val_acc: 0.9100\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9102 - val_loss: 0.2081 - val_acc: 0.9071\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9096 - val_loss: 0.2080 - val_acc: 0.9075\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9099 - val_loss: 0.2060 - val_acc: 0.9078\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9097 - val_loss: 0.2071 - val_acc: 0.9093\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9100 - val_loss: 0.2084 - val_acc: 0.9089\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2084 - val_acc: 0.9067\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9102 - val_loss: 0.2082 - val_acc: 0.9109\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9097 - val_loss: 0.2061 - val_acc: 0.9069\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1976 - acc: 0.9098 - val_loss: 0.2064 - val_acc: 0.9080\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9090 - val_loss: 0.2067 - val_acc: 0.9080\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1972 - acc: 0.9097 - val_loss: 0.2075 - val_acc: 0.9086\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9103 - val_loss: 0.2068 - val_acc: 0.9091\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9102 - val_loss: 0.2059 - val_acc: 0.9067\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9095 - val_loss: 0.2054 - val_acc: 0.9078\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9095 - val_loss: 0.2054 - val_acc: 0.9106\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9104 - val_loss: 0.2058 - val_acc: 0.9093\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1973 - acc: 0.9093 - val_loss: 0.2060 - val_acc: 0.9080\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9098 - val_loss: 0.2061 - val_acc: 0.9080\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9101 - val_loss: 0.2060 - val_acc: 0.9089\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9098 - val_loss: 0.2076 - val_acc: 0.9086\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9098 - val_loss: 0.2058 - val_acc: 0.9082\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9097 - val_loss: 0.2073 - val_acc: 0.9109\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9097 - val_loss: 0.2078 - val_acc: 0.9089\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9093 - val_loss: 0.2082 - val_acc: 0.9082\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9093 - val_loss: 0.2059 - val_acc: 0.9067\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9100 - val_loss: 0.2065 - val_acc: 0.9086\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9093 - val_loss: 0.2067 - val_acc: 0.9091\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9096 - val_loss: 0.2062 - val_acc: 0.9073\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9099 - val_loss: 0.2059 - val_acc: 0.9095\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9099 - val_loss: 0.2066 - val_acc: 0.9064\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9094 - val_loss: 0.2069 - val_acc: 0.9080\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9091 - val_loss: 0.2060 - val_acc: 0.9082\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9095 - val_loss: 0.2062 - val_acc: 0.9091\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9091 - val_loss: 0.2078 - val_acc: 0.9084\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9097 - val_loss: 0.2073 - val_acc: 0.9075\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9092 - val_loss: 0.2058 - val_acc: 0.9082\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9092 - val_loss: 0.2089 - val_acc: 0.9064\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9094 - val_loss: 0.2073 - val_acc: 0.9073\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9089 - val_loss: 0.2064 - val_acc: 0.9080\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9095 - val_loss: 0.2069 - val_acc: 0.9082\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9098 - val_loss: 0.2077 - val_acc: 0.9082\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1971 - acc: 0.9090 - val_loss: 0.2074 - val_acc: 0.9067\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9096 - val_loss: 0.2059 - val_acc: 0.9093\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2067 - val_acc: 0.9082\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9087 - val_loss: 0.2076 - val_acc: 0.9062\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9094 - val_loss: 0.2086 - val_acc: 0.9073\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9094 - val_loss: 0.2074 - val_acc: 0.9098\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9089 - val_loss: 0.2084 - val_acc: 0.9058\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9091 - val_loss: 0.2105 - val_acc: 0.9089\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9093 - val_loss: 0.2085 - val_acc: 0.9089\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9098 - val_loss: 0.2089 - val_acc: 0.9071\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9099 - val_loss: 0.2083 - val_acc: 0.9058\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1963 - acc: 0.9085 - val_loss: 0.2076 - val_acc: 0.9078\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9091 - val_loss: 0.2080 - val_acc: 0.9073\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9090 - val_loss: 0.2066 - val_acc: 0.9069\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1962 - acc: 0.9099 - val_loss: 0.2062 - val_acc: 0.9084\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9093 - val_loss: 0.2078 - val_acc: 0.9062\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1968 - acc: 0.9096 - val_loss: 0.2075 - val_acc: 0.9080\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9089 - val_loss: 0.2071 - val_acc: 0.9091\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1967 - acc: 0.9101 - val_loss: 0.2067 - val_acc: 0.9075\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9094 - val_loss: 0.2078 - val_acc: 0.9071\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9095 - val_loss: 0.2075 - val_acc: 0.9071\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9092 - val_loss: 0.2092 - val_acc: 0.9069\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1966 - acc: 0.9095 - val_loss: 0.2062 - val_acc: 0.9071\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1965 - acc: 0.9095 - val_loss: 0.2075 - val_acc: 0.9056\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1964 - acc: 0.9096 - val_loss: 0.2085 - val_acc: 0.9080\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1969 - acc: 0.9100 - val_loss: 0.2074 - val_acc: 0.9075\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9100 - val_loss: 0.2072 - val_acc: 0.9095\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1964 - acc: 0.9089 - val_loss: 0.2054 - val_acc: 0.9106\n",
            "acc: 91.06%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 6s 151us/step - loss: 0.2520 - acc: 0.8880 - val_loss: 0.2243 - val_acc: 0.8969\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2181 - acc: 0.9026 - val_loss: 0.2165 - val_acc: 0.9058\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2134 - acc: 0.9045 - val_loss: 0.2134 - val_acc: 0.9022\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2114 - acc: 0.9046 - val_loss: 0.2108 - val_acc: 0.9067\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2098 - acc: 0.9059 - val_loss: 0.2111 - val_acc: 0.9038\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2089 - acc: 0.9053 - val_loss: 0.2098 - val_acc: 0.9071\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2074 - acc: 0.9083 - val_loss: 0.2096 - val_acc: 0.9051\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2067 - acc: 0.9071 - val_loss: 0.2059 - val_acc: 0.9042\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2058 - acc: 0.9075 - val_loss: 0.2078 - val_acc: 0.9080\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2054 - acc: 0.9082 - val_loss: 0.2076 - val_acc: 0.9100\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2048 - acc: 0.9076 - val_loss: 0.2086 - val_acc: 0.9064\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2039 - acc: 0.9095 - val_loss: 0.2059 - val_acc: 0.9089\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9089 - val_loss: 0.2069 - val_acc: 0.9062\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9090 - val_loss: 0.2060 - val_acc: 0.9095\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2041 - acc: 0.9093 - val_loss: 0.2088 - val_acc: 0.9071\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2038 - acc: 0.9084 - val_loss: 0.2031 - val_acc: 0.9086\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2033 - acc: 0.9097 - val_loss: 0.2062 - val_acc: 0.9078\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2030 - acc: 0.9095 - val_loss: 0.2085 - val_acc: 0.9082\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2024 - acc: 0.9101 - val_loss: 0.2046 - val_acc: 0.9086\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9098 - val_loss: 0.2070 - val_acc: 0.9080\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2025 - acc: 0.9096 - val_loss: 0.2057 - val_acc: 0.9047\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9100 - val_loss: 0.2078 - val_acc: 0.9060\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2025 - acc: 0.9091 - val_loss: 0.2072 - val_acc: 0.9080\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2020 - acc: 0.9102 - val_loss: 0.2045 - val_acc: 0.9071\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9098 - val_loss: 0.2089 - val_acc: 0.9067\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9091 - val_loss: 0.2062 - val_acc: 0.9075\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2020 - acc: 0.9094 - val_loss: 0.2063 - val_acc: 0.9075\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2022 - acc: 0.9093 - val_loss: 0.2053 - val_acc: 0.9053\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2019 - acc: 0.9092 - val_loss: 0.2063 - val_acc: 0.9047\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2019 - acc: 0.9100 - val_loss: 0.2066 - val_acc: 0.9053\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9104 - val_loss: 0.2069 - val_acc: 0.9058\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9092 - val_loss: 0.2090 - val_acc: 0.9060\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2016 - acc: 0.9090 - val_loss: 0.2069 - val_acc: 0.9049\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2016 - acc: 0.9097 - val_loss: 0.2112 - val_acc: 0.9060\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2012 - acc: 0.9095 - val_loss: 0.2047 - val_acc: 0.9067\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9095 - val_loss: 0.2078 - val_acc: 0.9058\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9095 - val_loss: 0.2061 - val_acc: 0.9082\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9097 - val_loss: 0.2057 - val_acc: 0.9053\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9103 - val_loss: 0.2062 - val_acc: 0.9089\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9094 - val_loss: 0.2060 - val_acc: 0.9069\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9093 - val_loss: 0.2058 - val_acc: 0.9078\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9099 - val_loss: 0.2058 - val_acc: 0.9080\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2013 - acc: 0.9094 - val_loss: 0.2045 - val_acc: 0.9104\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2009 - acc: 0.9094 - val_loss: 0.2101 - val_acc: 0.9044\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2011 - acc: 0.9098 - val_loss: 0.2058 - val_acc: 0.9075\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2009 - acc: 0.9098 - val_loss: 0.2069 - val_acc: 0.9086\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2011 - acc: 0.9094 - val_loss: 0.2056 - val_acc: 0.9069\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9094 - val_loss: 0.2087 - val_acc: 0.9049\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2010 - acc: 0.9102 - val_loss: 0.2064 - val_acc: 0.9047\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9103 - val_loss: 0.2061 - val_acc: 0.9060\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9094 - val_loss: 0.2068 - val_acc: 0.9047\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9094 - val_loss: 0.2096 - val_acc: 0.9047\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2010 - acc: 0.9093 - val_loss: 0.2069 - val_acc: 0.9067\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9096 - val_loss: 0.2057 - val_acc: 0.9071\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9101 - val_loss: 0.2064 - val_acc: 0.9060\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9099 - val_loss: 0.2069 - val_acc: 0.9071\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9097 - val_loss: 0.2076 - val_acc: 0.9049\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9095 - val_loss: 0.2056 - val_acc: 0.9071\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9104 - val_loss: 0.2058 - val_acc: 0.9058\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9098 - val_loss: 0.2075 - val_acc: 0.9062\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9099 - val_loss: 0.2048 - val_acc: 0.9082\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2009 - acc: 0.9094 - val_loss: 0.2049 - val_acc: 0.9093\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9093 - val_loss: 0.2065 - val_acc: 0.9075\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9093 - val_loss: 0.2063 - val_acc: 0.9071\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9097 - val_loss: 0.2078 - val_acc: 0.9062\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2008 - acc: 0.9100 - val_loss: 0.2047 - val_acc: 0.9062\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9100 - val_loss: 0.2059 - val_acc: 0.9073\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2002 - acc: 0.9092 - val_loss: 0.2054 - val_acc: 0.9080\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9098 - val_loss: 0.2052 - val_acc: 0.9095\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9108 - val_loss: 0.2085 - val_acc: 0.9075\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9095 - val_loss: 0.2094 - val_acc: 0.9056\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9099 - val_loss: 0.2056 - val_acc: 0.9069\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.2003 - acc: 0.9101 - val_loss: 0.2052 - val_acc: 0.9075\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9102 - val_loss: 0.2059 - val_acc: 0.9069\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2001 - acc: 0.9097 - val_loss: 0.2065 - val_acc: 0.9080\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2006 - acc: 0.9094 - val_loss: 0.2072 - val_acc: 0.9064\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9104 - val_loss: 0.2086 - val_acc: 0.9086\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9099 - val_loss: 0.2062 - val_acc: 0.9056\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9100 - val_loss: 0.2067 - val_acc: 0.9078\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1998 - acc: 0.9103 - val_loss: 0.2065 - val_acc: 0.9067\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2004 - acc: 0.9102 - val_loss: 0.2052 - val_acc: 0.9082\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9106 - val_loss: 0.2089 - val_acc: 0.9060\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9105 - val_loss: 0.2060 - val_acc: 0.9060\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2003 - acc: 0.9100 - val_loss: 0.2054 - val_acc: 0.9064\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9100 - val_loss: 0.2055 - val_acc: 0.9071\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9100 - val_loss: 0.2077 - val_acc: 0.9071\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9100 - val_loss: 0.2102 - val_acc: 0.9075\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9100 - val_loss: 0.2059 - val_acc: 0.9075\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9105 - val_loss: 0.2049 - val_acc: 0.9071\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9102 - val_loss: 0.2077 - val_acc: 0.9067\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9105 - val_loss: 0.2055 - val_acc: 0.9093\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9099 - val_loss: 0.2038 - val_acc: 0.9086\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9095 - val_loss: 0.2055 - val_acc: 0.9075\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9104 - val_loss: 0.2040 - val_acc: 0.9093\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9101 - val_loss: 0.2055 - val_acc: 0.9060\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9108 - val_loss: 0.2039 - val_acc: 0.9078\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9104 - val_loss: 0.2044 - val_acc: 0.9056\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9103 - val_loss: 0.2063 - val_acc: 0.9067\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9099 - val_loss: 0.2057 - val_acc: 0.9060\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9105 - val_loss: 0.2082 - val_acc: 0.9064\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9094 - val_loss: 0.2066 - val_acc: 0.9051\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9097 - val_loss: 0.2116 - val_acc: 0.9060\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9103 - val_loss: 0.2042 - val_acc: 0.9067\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1996 - acc: 0.9106 - val_loss: 0.2057 - val_acc: 0.9064\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 29us/step - loss: 0.2001 - acc: 0.9100 - val_loss: 0.2058 - val_acc: 0.9060\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 29us/step - loss: 0.2002 - acc: 0.9104 - val_loss: 0.2092 - val_acc: 0.9040\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 28us/step - loss: 0.1996 - acc: 0.9104 - val_loss: 0.2065 - val_acc: 0.9056\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1996 - acc: 0.9105 - val_loss: 0.2076 - val_acc: 0.9064\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1997 - acc: 0.9106 - val_loss: 0.2120 - val_acc: 0.9040\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 32us/step - loss: 0.1997 - acc: 0.9103 - val_loss: 0.2079 - val_acc: 0.9073\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2000 - acc: 0.9102 - val_loss: 0.2067 - val_acc: 0.9056\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9104 - val_loss: 0.2137 - val_acc: 0.9049\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9104 - val_loss: 0.2085 - val_acc: 0.9069\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9108 - val_loss: 0.2076 - val_acc: 0.9049\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9104 - val_loss: 0.2063 - val_acc: 0.9042\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9109 - val_loss: 0.2064 - val_acc: 0.9060\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9102 - val_loss: 0.2053 - val_acc: 0.9069\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1999 - acc: 0.9101 - val_loss: 0.2069 - val_acc: 0.9062\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9108 - val_loss: 0.2062 - val_acc: 0.9078\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9105 - val_loss: 0.2054 - val_acc: 0.9064\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9110 - val_loss: 0.2065 - val_acc: 0.9056\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9093 - val_loss: 0.2073 - val_acc: 0.9067\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9109 - val_loss: 0.2059 - val_acc: 0.9051\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9101 - val_loss: 0.2057 - val_acc: 0.9084\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9108 - val_loss: 0.2062 - val_acc: 0.9071\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9103 - val_loss: 0.2073 - val_acc: 0.9064\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9109 - val_loss: 0.2087 - val_acc: 0.9069\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9108 - val_loss: 0.2060 - val_acc: 0.9058\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9101 - val_loss: 0.2058 - val_acc: 0.9078\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9106 - val_loss: 0.2054 - val_acc: 0.9053\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9103 - val_loss: 0.2057 - val_acc: 0.9053\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9101 - val_loss: 0.2049 - val_acc: 0.9056\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9100 - val_loss: 0.2056 - val_acc: 0.9051\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9111 - val_loss: 0.2078 - val_acc: 0.9058\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9103 - val_loss: 0.2077 - val_acc: 0.9053\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9103 - val_loss: 0.2040 - val_acc: 0.9073\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9106 - val_loss: 0.2054 - val_acc: 0.9071\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9099 - val_loss: 0.2108 - val_acc: 0.9060\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9106 - val_loss: 0.2043 - val_acc: 0.9051\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9100 - val_loss: 0.2047 - val_acc: 0.9078\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9103 - val_loss: 0.2081 - val_acc: 0.9051\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9104 - val_loss: 0.2051 - val_acc: 0.9056\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9098 - val_loss: 0.2082 - val_acc: 0.9056\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9109 - val_loss: 0.2043 - val_acc: 0.9067\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9104 - val_loss: 0.2057 - val_acc: 0.9067\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9105 - val_loss: 0.2053 - val_acc: 0.9056\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9107 - val_loss: 0.2076 - val_acc: 0.9049\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9098 - val_loss: 0.2052 - val_acc: 0.9078\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9099 - val_loss: 0.2061 - val_acc: 0.9093\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9103 - val_loss: 0.2037 - val_acc: 0.9064\n",
            "acc: 90.64%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 161us/step - loss: 0.2583 - acc: 0.8902 - val_loss: 0.2309 - val_acc: 0.8971\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2171 - acc: 0.9036 - val_loss: 0.2280 - val_acc: 0.8938\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2136 - acc: 0.9034 - val_loss: 0.2244 - val_acc: 0.8956\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2105 - acc: 0.9053 - val_loss: 0.2189 - val_acc: 0.8987\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2095 - acc: 0.9064 - val_loss: 0.2186 - val_acc: 0.8987\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2079 - acc: 0.9055 - val_loss: 0.2196 - val_acc: 0.8967\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2074 - acc: 0.9059 - val_loss: 0.2170 - val_acc: 0.8974\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2063 - acc: 0.9073 - val_loss: 0.2165 - val_acc: 0.8983\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2061 - acc: 0.9072 - val_loss: 0.2149 - val_acc: 0.8996\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2053 - acc: 0.9071 - val_loss: 0.2172 - val_acc: 0.8965\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2047 - acc: 0.9070 - val_loss: 0.2177 - val_acc: 0.8969\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2039 - acc: 0.9075 - val_loss: 0.2141 - val_acc: 0.8983\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2039 - acc: 0.9082 - val_loss: 0.2150 - val_acc: 0.8983\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2036 - acc: 0.9076 - val_loss: 0.2132 - val_acc: 0.8994\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9078 - val_loss: 0.2154 - val_acc: 0.8985\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2031 - acc: 0.9073 - val_loss: 0.2144 - val_acc: 0.8960\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2021 - acc: 0.9072 - val_loss: 0.2140 - val_acc: 0.8978\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9074 - val_loss: 0.2127 - val_acc: 0.8978\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2014 - acc: 0.9078 - val_loss: 0.2128 - val_acc: 0.8954\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2015 - acc: 0.9078 - val_loss: 0.2146 - val_acc: 0.8983\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2018 - acc: 0.9068 - val_loss: 0.2128 - val_acc: 0.8996\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2013 - acc: 0.9073 - val_loss: 0.2130 - val_acc: 0.8985\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2010 - acc: 0.9070 - val_loss: 0.2141 - val_acc: 0.8978\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9079 - val_loss: 0.2114 - val_acc: 0.8983\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9081 - val_loss: 0.2126 - val_acc: 0.8956\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2006 - acc: 0.9070 - val_loss: 0.2114 - val_acc: 0.8974\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9080 - val_loss: 0.2130 - val_acc: 0.8954\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9086 - val_loss: 0.2122 - val_acc: 0.8987\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9078 - val_loss: 0.2124 - val_acc: 0.8987\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9082 - val_loss: 0.2106 - val_acc: 0.8991\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9072 - val_loss: 0.2136 - val_acc: 0.8978\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2001 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.8952\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9079 - val_loss: 0.2130 - val_acc: 0.8987\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9080 - val_loss: 0.2135 - val_acc: 0.8989\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9076 - val_loss: 0.2130 - val_acc: 0.8965\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9079 - val_loss: 0.2123 - val_acc: 0.8980\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9084 - val_loss: 0.2121 - val_acc: 0.8954\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9070 - val_loss: 0.2140 - val_acc: 0.8991\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9079 - val_loss: 0.2125 - val_acc: 0.8985\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1996 - acc: 0.9079 - val_loss: 0.2134 - val_acc: 0.8958\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9077 - val_loss: 0.2112 - val_acc: 0.8987\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9073 - val_loss: 0.2137 - val_acc: 0.8952\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9084 - val_loss: 0.2129 - val_acc: 0.8971\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9081 - val_loss: 0.2135 - val_acc: 0.8978\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9081 - val_loss: 0.2142 - val_acc: 0.8960\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9087 - val_loss: 0.2123 - val_acc: 0.8958\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9075 - val_loss: 0.2143 - val_acc: 0.8963\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9085 - val_loss: 0.2132 - val_acc: 0.8989\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9087 - val_loss: 0.2136 - val_acc: 0.8971\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9079 - val_loss: 0.2125 - val_acc: 0.8980\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9094 - val_loss: 0.2138 - val_acc: 0.8994\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9083 - val_loss: 0.2127 - val_acc: 0.8963\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9078 - val_loss: 0.2129 - val_acc: 0.9009\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9085 - val_loss: 0.2115 - val_acc: 0.8983\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9082 - val_loss: 0.2105 - val_acc: 0.8971\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9074 - val_loss: 0.2112 - val_acc: 0.8989\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9078 - val_loss: 0.2117 - val_acc: 0.8983\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9080 - val_loss: 0.2128 - val_acc: 0.8987\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9081 - val_loss: 0.2123 - val_acc: 0.8980\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9084 - val_loss: 0.2136 - val_acc: 0.8963\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9084 - val_loss: 0.2112 - val_acc: 0.8967\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9088 - val_loss: 0.2124 - val_acc: 0.8987\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9087 - val_loss: 0.2129 - val_acc: 0.8967\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.2150 - val_acc: 0.8989\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9083 - val_loss: 0.2124 - val_acc: 0.8974\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9090 - val_loss: 0.2124 - val_acc: 0.8965\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9089 - val_loss: 0.2139 - val_acc: 0.8967\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9095 - val_loss: 0.2122 - val_acc: 0.8994\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9083 - val_loss: 0.2108 - val_acc: 0.8971\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9087 - val_loss: 0.2125 - val_acc: 0.9000\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9080 - val_loss: 0.2126 - val_acc: 0.8960\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9081 - val_loss: 0.2113 - val_acc: 0.8978\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9079 - val_loss: 0.2113 - val_acc: 0.8987\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9083 - val_loss: 0.2116 - val_acc: 0.8963\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9088 - val_loss: 0.2105 - val_acc: 0.8998\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.2127 - val_acc: 0.8956\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9082 - val_loss: 0.2115 - val_acc: 0.8971\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9081 - val_loss: 0.2114 - val_acc: 0.8994\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9079 - val_loss: 0.2131 - val_acc: 0.8963\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9073 - val_loss: 0.2111 - val_acc: 0.8987\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9090 - val_loss: 0.2112 - val_acc: 0.8996\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9091 - val_loss: 0.2111 - val_acc: 0.8996\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9096 - val_loss: 0.2106 - val_acc: 0.9005\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9075 - val_loss: 0.2112 - val_acc: 0.8971\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9074 - val_loss: 0.2138 - val_acc: 0.9000\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9081 - val_loss: 0.2130 - val_acc: 0.8980\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9077 - val_loss: 0.2100 - val_acc: 0.8996\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9085 - val_loss: 0.2111 - val_acc: 0.8976\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9087 - val_loss: 0.2114 - val_acc: 0.8958\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9090 - val_loss: 0.2120 - val_acc: 0.8989\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9081 - val_loss: 0.2122 - val_acc: 0.8965\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9081 - val_loss: 0.2132 - val_acc: 0.8956\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.8991\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9089 - val_loss: 0.2103 - val_acc: 0.8991\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9087 - val_loss: 0.2103 - val_acc: 0.8983\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9088 - val_loss: 0.2122 - val_acc: 0.9002\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9090 - val_loss: 0.2118 - val_acc: 0.8969\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9080 - val_loss: 0.2119 - val_acc: 0.9013\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.2101 - val_acc: 0.8994\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.2111 - val_acc: 0.9002\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2126 - val_acc: 0.8980\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.2117 - val_acc: 0.8985\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9084 - val_loss: 0.2105 - val_acc: 0.8996\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9087 - val_loss: 0.2133 - val_acc: 0.8969\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9090 - val_loss: 0.2098 - val_acc: 0.9016\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9084 - val_loss: 0.2107 - val_acc: 0.8983\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9088 - val_loss: 0.2115 - val_acc: 0.8983\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9082 - val_loss: 0.2114 - val_acc: 0.8974\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.2108 - val_acc: 0.8976\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2119 - val_acc: 0.8965\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9083 - val_loss: 0.2097 - val_acc: 0.9020\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9089 - val_loss: 0.2120 - val_acc: 0.8978\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9082 - val_loss: 0.2116 - val_acc: 0.9007\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9080 - val_loss: 0.2111 - val_acc: 0.8989\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.2142 - val_acc: 0.8987\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1973 - acc: 0.9091 - val_loss: 0.2090 - val_acc: 0.9007\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9084 - val_loss: 0.2108 - val_acc: 0.8960\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9087 - val_loss: 0.2105 - val_acc: 0.8987\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.2113 - val_acc: 0.8996\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9084 - val_loss: 0.2107 - val_acc: 0.8987\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9088 - val_loss: 0.2114 - val_acc: 0.8978\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9085 - val_loss: 0.2113 - val_acc: 0.8991\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9092 - val_loss: 0.2115 - val_acc: 0.8987\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9092 - val_loss: 0.2104 - val_acc: 0.8996\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.2115 - val_acc: 0.8996\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9082 - val_loss: 0.2157 - val_acc: 0.8985\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9089 - val_loss: 0.2088 - val_acc: 0.9002\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9091 - val_loss: 0.2100 - val_acc: 0.8983\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9089 - val_loss: 0.2097 - val_acc: 0.8971\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9091 - val_loss: 0.2121 - val_acc: 0.8971\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9088 - val_loss: 0.2131 - val_acc: 0.8985\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9078 - val_loss: 0.2125 - val_acc: 0.8987\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 23us/step - loss: 0.1970 - acc: 0.9083 - val_loss: 0.2097 - val_acc: 0.8978\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9088 - val_loss: 0.2129 - val_acc: 0.8974\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9088 - val_loss: 0.2111 - val_acc: 0.8987\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9090 - val_loss: 0.2115 - val_acc: 0.8971\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9080 - val_loss: 0.2108 - val_acc: 0.8980\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9094 - val_loss: 0.2098 - val_acc: 0.8976\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9078 - val_loss: 0.2097 - val_acc: 0.8998\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9097 - val_loss: 0.2097 - val_acc: 0.8998\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9081 - val_loss: 0.2114 - val_acc: 0.8969\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1966 - acc: 0.9084 - val_loss: 0.2143 - val_acc: 0.8983\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9094 - val_loss: 0.2125 - val_acc: 0.8985\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9085 - val_loss: 0.2134 - val_acc: 0.8998\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9084 - val_loss: 0.2106 - val_acc: 0.9002\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9092 - val_loss: 0.2112 - val_acc: 0.8998\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9082 - val_loss: 0.2143 - val_acc: 0.8989\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9085 - val_loss: 0.2116 - val_acc: 0.8994\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9086 - val_loss: 0.2106 - val_acc: 0.8994\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1969 - acc: 0.9086 - val_loss: 0.2103 - val_acc: 0.8983\n",
            "acc: 89.83%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 165us/step - loss: 0.2568 - acc: 0.8916 - val_loss: 0.2073 - val_acc: 0.9064\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2155 - acc: 0.9030 - val_loss: 0.2047 - val_acc: 0.9091\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2116 - acc: 0.9039 - val_loss: 0.2069 - val_acc: 0.9047\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2103 - acc: 0.9045 - val_loss: 0.2044 - val_acc: 0.9060\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2085 - acc: 0.9051 - val_loss: 0.2036 - val_acc: 0.9086\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2076 - acc: 0.9050 - val_loss: 0.2029 - val_acc: 0.9062\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2067 - acc: 0.9059 - val_loss: 0.2031 - val_acc: 0.9058\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2065 - acc: 0.9055 - val_loss: 0.2019 - val_acc: 0.9082\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2062 - acc: 0.9062 - val_loss: 0.2028 - val_acc: 0.9089\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2055 - acc: 0.9062 - val_loss: 0.2012 - val_acc: 0.9049\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2053 - acc: 0.9058 - val_loss: 0.2017 - val_acc: 0.9056\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2047 - acc: 0.9067 - val_loss: 0.2011 - val_acc: 0.9064\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2045 - acc: 0.9056 - val_loss: 0.1994 - val_acc: 0.9098\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2043 - acc: 0.9063 - val_loss: 0.1989 - val_acc: 0.9089\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2038 - acc: 0.9067 - val_loss: 0.2021 - val_acc: 0.9064\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2042 - acc: 0.9063 - val_loss: 0.2008 - val_acc: 0.9084\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2031 - acc: 0.9065 - val_loss: 0.1994 - val_acc: 0.9095\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2029 - acc: 0.9067 - val_loss: 0.1984 - val_acc: 0.9086\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2029 - acc: 0.9063 - val_loss: 0.1994 - val_acc: 0.9082\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2029 - acc: 0.9072 - val_loss: 0.1974 - val_acc: 0.9086\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2023 - acc: 0.9076 - val_loss: 0.1963 - val_acc: 0.9069\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2019 - acc: 0.9057 - val_loss: 0.1973 - val_acc: 0.9084\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2019 - acc: 0.9076 - val_loss: 0.1976 - val_acc: 0.9062\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2018 - acc: 0.9063 - val_loss: 0.1966 - val_acc: 0.9084\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2010 - acc: 0.9073 - val_loss: 0.1986 - val_acc: 0.9084\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2011 - acc: 0.9066 - val_loss: 0.1983 - val_acc: 0.9098\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2012 - acc: 0.9065 - val_loss: 0.1985 - val_acc: 0.9095\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2005 - acc: 0.9067 - val_loss: 0.1967 - val_acc: 0.9102\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2007 - acc: 0.9062 - val_loss: 0.1980 - val_acc: 0.9080\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9073 - val_loss: 0.1968 - val_acc: 0.9093\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9070 - val_loss: 0.1970 - val_acc: 0.9053\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9068 - val_loss: 0.1973 - val_acc: 0.9082\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9071 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2000 - acc: 0.9077 - val_loss: 0.1975 - val_acc: 0.9089\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9075 - val_loss: 0.1982 - val_acc: 0.9089\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9075 - val_loss: 0.1973 - val_acc: 0.9109\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9070 - val_loss: 0.1966 - val_acc: 0.9082\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9074 - val_loss: 0.1972 - val_acc: 0.9082\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9077 - val_loss: 0.1982 - val_acc: 0.9069\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9071 - val_loss: 0.1973 - val_acc: 0.9100\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1997 - acc: 0.9075 - val_loss: 0.1960 - val_acc: 0.9084\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9075 - val_loss: 0.1969 - val_acc: 0.9106\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2000 - acc: 0.9067 - val_loss: 0.1974 - val_acc: 0.9095\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9069 - val_loss: 0.1963 - val_acc: 0.9093\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9068 - val_loss: 0.1963 - val_acc: 0.9100\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9070 - val_loss: 0.1955 - val_acc: 0.9098\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9068 - val_loss: 0.1965 - val_acc: 0.9084\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9077 - val_loss: 0.1958 - val_acc: 0.9111\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9074 - val_loss: 0.1951 - val_acc: 0.9086\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9067 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9068 - val_loss: 0.1972 - val_acc: 0.9089\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9070 - val_loss: 0.1969 - val_acc: 0.9084\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9060 - val_loss: 0.1959 - val_acc: 0.9084\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9074 - val_loss: 0.1966 - val_acc: 0.9095\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9073 - val_loss: 0.1975 - val_acc: 0.9064\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9076 - val_loss: 0.1970 - val_acc: 0.9062\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9072 - val_loss: 0.1968 - val_acc: 0.9078\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9066 - val_loss: 0.1969 - val_acc: 0.9093\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9075 - val_loss: 0.1956 - val_acc: 0.9082\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9072 - val_loss: 0.1946 - val_acc: 0.9084\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9071 - val_loss: 0.1974 - val_acc: 0.9082\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9074 - val_loss: 0.1980 - val_acc: 0.9086\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9081 - val_loss: 0.1952 - val_acc: 0.9086\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9086 - val_loss: 0.1956 - val_acc: 0.9084\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9073 - val_loss: 0.1964 - val_acc: 0.9098\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9082 - val_loss: 0.1976 - val_acc: 0.9067\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1989 - acc: 0.9087 - val_loss: 0.1989 - val_acc: 0.9053\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9081 - val_loss: 0.1967 - val_acc: 0.9080\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9074 - val_loss: 0.1963 - val_acc: 0.9091\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9077 - val_loss: 0.1954 - val_acc: 0.9086\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1983 - acc: 0.9087 - val_loss: 0.1963 - val_acc: 0.9073\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1981 - acc: 0.9082 - val_loss: 0.1949 - val_acc: 0.9089\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1986 - acc: 0.9077 - val_loss: 0.1947 - val_acc: 0.9095\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1983 - acc: 0.9075 - val_loss: 0.1986 - val_acc: 0.9062\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1980 - acc: 0.9083 - val_loss: 0.1967 - val_acc: 0.9091\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1983 - acc: 0.9079 - val_loss: 0.1955 - val_acc: 0.9100\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.1975 - val_acc: 0.9082\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1979 - acc: 0.9077 - val_loss: 0.1955 - val_acc: 0.9080\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9075 - val_loss: 0.1959 - val_acc: 0.9089\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1981 - acc: 0.9082 - val_loss: 0.1963 - val_acc: 0.9078\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9077 - val_loss: 0.1974 - val_acc: 0.9073\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9078 - val_loss: 0.1962 - val_acc: 0.9073\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1980 - acc: 0.9076 - val_loss: 0.1983 - val_acc: 0.9084\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1982 - acc: 0.9077 - val_loss: 0.1955 - val_acc: 0.9091\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.1959 - val_acc: 0.9075\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9078 - val_loss: 0.1942 - val_acc: 0.9086\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9082 - val_loss: 0.1953 - val_acc: 0.9080\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9076 - val_loss: 0.1976 - val_acc: 0.9080\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.1973 - val_acc: 0.9064\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9080 - val_loss: 0.1971 - val_acc: 0.9078\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9076 - val_loss: 0.1972 - val_acc: 0.9091\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9078 - val_loss: 0.1948 - val_acc: 0.9082\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9084 - val_loss: 0.1972 - val_acc: 0.9058\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9086 - val_loss: 0.1957 - val_acc: 0.9082\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9081 - val_loss: 0.1970 - val_acc: 0.9086\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9081 - val_loss: 0.1981 - val_acc: 0.9080\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1979 - acc: 0.9079 - val_loss: 0.1951 - val_acc: 0.9073\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1978 - acc: 0.9081 - val_loss: 0.1975 - val_acc: 0.9078\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9077 - val_loss: 0.1963 - val_acc: 0.9093\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9077 - val_loss: 0.1970 - val_acc: 0.9071\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9079 - val_loss: 0.1959 - val_acc: 0.9067\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1978 - acc: 0.9080 - val_loss: 0.1975 - val_acc: 0.9075\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9082 - val_loss: 0.1973 - val_acc: 0.9080\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9078 - val_loss: 0.1959 - val_acc: 0.9075\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1964 - val_acc: 0.9069\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9080 - val_loss: 0.1965 - val_acc: 0.9073\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9084 - val_loss: 0.1996 - val_acc: 0.9080\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9084 - val_loss: 0.1956 - val_acc: 0.9071\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9081 - val_loss: 0.1956 - val_acc: 0.9091\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9087 - val_loss: 0.1979 - val_acc: 0.9086\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9080 - val_loss: 0.1963 - val_acc: 0.9064\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9080 - val_loss: 0.1971 - val_acc: 0.9082\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.1959 - val_acc: 0.9069\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1978 - acc: 0.9079 - val_loss: 0.1981 - val_acc: 0.9075\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9083 - val_loss: 0.1952 - val_acc: 0.9086\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9080 - val_loss: 0.1961 - val_acc: 0.9060\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1970 - acc: 0.9082 - val_loss: 0.2005 - val_acc: 0.9073\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9082 - val_loss: 0.1952 - val_acc: 0.9069\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1973 - acc: 0.9079 - val_loss: 0.1972 - val_acc: 0.9069\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9081 - val_loss: 0.1968 - val_acc: 0.9073\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9077 - val_loss: 0.1976 - val_acc: 0.9084\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1976 - val_acc: 0.9064\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9053\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1972 - acc: 0.9083 - val_loss: 0.1962 - val_acc: 0.9067\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9083 - val_loss: 0.1962 - val_acc: 0.9078\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1976 - acc: 0.9085 - val_loss: 0.1970 - val_acc: 0.9080\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9086 - val_loss: 0.1967 - val_acc: 0.9069\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9089 - val_loss: 0.1968 - val_acc: 0.9073\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9084 - val_loss: 0.1967 - val_acc: 0.9064\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9091 - val_loss: 0.1964 - val_acc: 0.9062\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1977 - acc: 0.9081 - val_loss: 0.1984 - val_acc: 0.9069\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1973 - acc: 0.9080 - val_loss: 0.1949 - val_acc: 0.9071\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9086 - val_loss: 0.1975 - val_acc: 0.9075\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9087 - val_loss: 0.1964 - val_acc: 0.9064\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1977 - acc: 0.9079 - val_loss: 0.1966 - val_acc: 0.9053\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9083 - val_loss: 0.1964 - val_acc: 0.9080\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9087 - val_loss: 0.1968 - val_acc: 0.9062\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9085 - val_loss: 0.1957 - val_acc: 0.9056\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9081 - val_loss: 0.1991 - val_acc: 0.9056\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9085 - val_loss: 0.1955 - val_acc: 0.9056\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9087 - val_loss: 0.1964 - val_acc: 0.9062\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1972 - acc: 0.9088 - val_loss: 0.1947 - val_acc: 0.9080\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9085 - val_loss: 0.1956 - val_acc: 0.9084\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9082 - val_loss: 0.1968 - val_acc: 0.9095\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9075 - val_loss: 0.1972 - val_acc: 0.9049\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1973 - acc: 0.9080 - val_loss: 0.1960 - val_acc: 0.9069\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9086 - val_loss: 0.1957 - val_acc: 0.9067\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9090 - val_loss: 0.1969 - val_acc: 0.9044\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1976 - acc: 0.9078 - val_loss: 0.1951 - val_acc: 0.9060\n",
            "acc: 90.60%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 167us/step - loss: 0.2662 - acc: 0.8900 - val_loss: 0.2219 - val_acc: 0.8971\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2173 - acc: 0.9038 - val_loss: 0.2152 - val_acc: 0.8991\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2137 - acc: 0.9044 - val_loss: 0.2121 - val_acc: 0.8998\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2109 - acc: 0.9048 - val_loss: 0.2134 - val_acc: 0.8998\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2091 - acc: 0.9054 - val_loss: 0.2105 - val_acc: 0.8989\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2081 - acc: 0.9059 - val_loss: 0.2113 - val_acc: 0.9018\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2068 - acc: 0.9069 - val_loss: 0.2131 - val_acc: 0.9005\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2065 - acc: 0.9064 - val_loss: 0.2107 - val_acc: 0.8996\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2050 - acc: 0.9073 - val_loss: 0.2082 - val_acc: 0.9031\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2043 - acc: 0.9079 - val_loss: 0.2114 - val_acc: 0.9020\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2040 - acc: 0.9073 - val_loss: 0.2094 - val_acc: 0.9020\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2033 - acc: 0.9073 - val_loss: 0.2096 - val_acc: 0.9029\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2025 - acc: 0.9078 - val_loss: 0.2090 - val_acc: 0.9009\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2019 - acc: 0.9074 - val_loss: 0.2085 - val_acc: 0.9005\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2021 - acc: 0.9073 - val_loss: 0.2078 - val_acc: 0.9033\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2019 - acc: 0.9074 - val_loss: 0.2094 - val_acc: 0.9016\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9070 - val_loss: 0.2088 - val_acc: 0.8994\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9078 - val_loss: 0.2105 - val_acc: 0.9002\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2012 - acc: 0.9070 - val_loss: 0.2093 - val_acc: 0.9013\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2012 - acc: 0.9084 - val_loss: 0.2084 - val_acc: 0.9040\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9079 - val_loss: 0.2119 - val_acc: 0.8989\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2008 - acc: 0.9079 - val_loss: 0.2106 - val_acc: 0.9013\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2008 - acc: 0.9082 - val_loss: 0.2070 - val_acc: 0.9002\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9088 - val_loss: 0.2080 - val_acc: 0.8994\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9086 - val_loss: 0.2096 - val_acc: 0.9013\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9091 - val_loss: 0.2085 - val_acc: 0.9007\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9085 - val_loss: 0.2089 - val_acc: 0.9002\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1997 - acc: 0.9084 - val_loss: 0.2124 - val_acc: 0.9029\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9096 - val_loss: 0.2073 - val_acc: 0.9009\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1994 - acc: 0.9087 - val_loss: 0.2071 - val_acc: 0.9029\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9092 - val_loss: 0.2078 - val_acc: 0.9051\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1991 - acc: 0.9077 - val_loss: 0.2116 - val_acc: 0.9022\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9090 - val_loss: 0.2105 - val_acc: 0.9009\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9095 - val_loss: 0.2096 - val_acc: 0.8996\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9094 - val_loss: 0.2092 - val_acc: 0.9025\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9086 - val_loss: 0.2078 - val_acc: 0.9033\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9091 - val_loss: 0.2092 - val_acc: 0.9011\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9087 - val_loss: 0.2064 - val_acc: 0.9007\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1983 - acc: 0.9086 - val_loss: 0.2077 - val_acc: 0.8989\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9085 - val_loss: 0.2075 - val_acc: 0.9018\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1978 - acc: 0.9090 - val_loss: 0.2084 - val_acc: 0.9016\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1981 - acc: 0.9075 - val_loss: 0.2065 - val_acc: 0.9020\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9092 - val_loss: 0.2075 - val_acc: 0.9011\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1980 - acc: 0.9088 - val_loss: 0.2099 - val_acc: 0.8983\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1979 - acc: 0.9086 - val_loss: 0.2079 - val_acc: 0.9002\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9089 - val_loss: 0.2098 - val_acc: 0.9016\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1979 - acc: 0.9087 - val_loss: 0.2075 - val_acc: 0.8998\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1974 - acc: 0.9088 - val_loss: 0.2083 - val_acc: 0.9027\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1975 - acc: 0.9087 - val_loss: 0.2093 - val_acc: 0.9029\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1974 - acc: 0.9094 - val_loss: 0.2070 - val_acc: 0.9027\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9087 - val_loss: 0.2086 - val_acc: 0.9002\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1975 - acc: 0.9092 - val_loss: 0.2104 - val_acc: 0.9011\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1972 - acc: 0.9101 - val_loss: 0.2087 - val_acc: 0.9033\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9094 - val_loss: 0.2058 - val_acc: 0.9013\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1971 - acc: 0.9106 - val_loss: 0.2073 - val_acc: 0.9007\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1965 - acc: 0.9094 - val_loss: 0.2144 - val_acc: 0.8991\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1968 - acc: 0.9103 - val_loss: 0.2085 - val_acc: 0.8994\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1967 - acc: 0.9098 - val_loss: 0.2064 - val_acc: 0.8998\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1970 - acc: 0.9092 - val_loss: 0.2068 - val_acc: 0.8998\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9101 - val_loss: 0.2062 - val_acc: 0.8989\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9108 - val_loss: 0.2066 - val_acc: 0.9027\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1962 - acc: 0.9104 - val_loss: 0.2049 - val_acc: 0.9025\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9102 - val_loss: 0.2063 - val_acc: 0.8987\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1966 - acc: 0.9096 - val_loss: 0.2081 - val_acc: 0.9002\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1963 - acc: 0.9113 - val_loss: 0.2051 - val_acc: 0.9016\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1963 - acc: 0.9106 - val_loss: 0.2067 - val_acc: 0.8987\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1962 - acc: 0.9091 - val_loss: 0.2055 - val_acc: 0.8996\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1961 - acc: 0.9107 - val_loss: 0.2075 - val_acc: 0.9027\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1958 - acc: 0.9100 - val_loss: 0.2067 - val_acc: 0.8980\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1960 - acc: 0.9098 - val_loss: 0.2053 - val_acc: 0.9022\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9103 - val_loss: 0.2067 - val_acc: 0.9013\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1956 - acc: 0.9100 - val_loss: 0.2084 - val_acc: 0.8969\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1958 - acc: 0.9099 - val_loss: 0.2063 - val_acc: 0.9002\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9102 - val_loss: 0.2082 - val_acc: 0.9007\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9111 - val_loss: 0.2063 - val_acc: 0.9025\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1957 - acc: 0.9094 - val_loss: 0.2095 - val_acc: 0.8991\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1957 - acc: 0.9090 - val_loss: 0.2060 - val_acc: 0.9018\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1957 - acc: 0.9097 - val_loss: 0.2051 - val_acc: 0.9013\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9108 - val_loss: 0.2052 - val_acc: 0.9002\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1952 - acc: 0.9105 - val_loss: 0.2054 - val_acc: 0.9040\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1954 - acc: 0.9098 - val_loss: 0.2059 - val_acc: 0.8989\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1953 - acc: 0.9101 - val_loss: 0.2072 - val_acc: 0.9005\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1953 - acc: 0.9098 - val_loss: 0.2071 - val_acc: 0.8998\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1954 - acc: 0.9089 - val_loss: 0.2058 - val_acc: 0.9016\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9098 - val_loss: 0.2061 - val_acc: 0.9000\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1950 - acc: 0.9094 - val_loss: 0.2076 - val_acc: 0.8987\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1955 - acc: 0.9107 - val_loss: 0.2047 - val_acc: 0.9016\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1951 - acc: 0.9096 - val_loss: 0.2049 - val_acc: 0.9005\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1954 - acc: 0.9106 - val_loss: 0.2046 - val_acc: 0.9040\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1952 - acc: 0.9092 - val_loss: 0.2051 - val_acc: 0.9056\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1951 - acc: 0.9097 - val_loss: 0.2055 - val_acc: 0.9044\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9097 - val_loss: 0.2055 - val_acc: 0.9002\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9102 - val_loss: 0.2059 - val_acc: 0.9033\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1951 - acc: 0.9090 - val_loss: 0.2066 - val_acc: 0.9040\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1953 - acc: 0.9096 - val_loss: 0.2091 - val_acc: 0.9009\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9099 - val_loss: 0.2073 - val_acc: 0.9009\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1952 - acc: 0.9105 - val_loss: 0.2074 - val_acc: 0.8994\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9105 - val_loss: 0.2059 - val_acc: 0.9005\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1944 - acc: 0.9102 - val_loss: 0.2083 - val_acc: 0.9033\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9098 - val_loss: 0.2057 - val_acc: 0.8991\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9106 - val_loss: 0.2056 - val_acc: 0.8980\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9103 - val_loss: 0.2081 - val_acc: 0.8985\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1947 - acc: 0.9111 - val_loss: 0.2048 - val_acc: 0.9018\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9107 - val_loss: 0.2056 - val_acc: 0.9025\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9102 - val_loss: 0.2067 - val_acc: 0.9002\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9097 - val_loss: 0.2069 - val_acc: 0.8989\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9097 - val_loss: 0.2062 - val_acc: 0.9027\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9105 - val_loss: 0.2114 - val_acc: 0.8940\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1949 - acc: 0.9105 - val_loss: 0.2054 - val_acc: 0.9040\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1949 - acc: 0.9096 - val_loss: 0.2079 - val_acc: 0.9002\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1944 - acc: 0.9098 - val_loss: 0.2064 - val_acc: 0.9000\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9097 - val_loss: 0.2055 - val_acc: 0.9016\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1949 - acc: 0.9094 - val_loss: 0.2067 - val_acc: 0.8998\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1942 - acc: 0.9096 - val_loss: 0.2060 - val_acc: 0.8994\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9092 - val_loss: 0.2083 - val_acc: 0.8987\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9097 - val_loss: 0.2132 - val_acc: 0.8971\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1950 - acc: 0.9100 - val_loss: 0.2054 - val_acc: 0.9025\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9112 - val_loss: 0.2073 - val_acc: 0.9011\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9101 - val_loss: 0.2069 - val_acc: 0.8994\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1948 - acc: 0.9100 - val_loss: 0.2073 - val_acc: 0.8980\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9093 - val_loss: 0.2066 - val_acc: 0.9007\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9101 - val_loss: 0.2071 - val_acc: 0.9002\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9093 - val_loss: 0.2054 - val_acc: 0.9007\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9108 - val_loss: 0.2045 - val_acc: 0.9029\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9103 - val_loss: 0.2063 - val_acc: 0.9016\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1942 - acc: 0.9106 - val_loss: 0.2075 - val_acc: 0.8991\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9104 - val_loss: 0.2063 - val_acc: 0.9044\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9096 - val_loss: 0.2065 - val_acc: 0.9020\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1951 - acc: 0.9098 - val_loss: 0.2050 - val_acc: 0.9027\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9103 - val_loss: 0.2061 - val_acc: 0.8978\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9103 - val_loss: 0.2047 - val_acc: 0.9025\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1947 - acc: 0.9090 - val_loss: 0.2046 - val_acc: 0.9025\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9094 - val_loss: 0.2078 - val_acc: 0.8989\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9102 - val_loss: 0.2069 - val_acc: 0.9000\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1942 - acc: 0.9096 - val_loss: 0.2071 - val_acc: 0.8991\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1946 - acc: 0.9107 - val_loss: 0.2070 - val_acc: 0.9018\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9097 - val_loss: 0.2074 - val_acc: 0.9036\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9115 - val_loss: 0.2057 - val_acc: 0.9002\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9099 - val_loss: 0.2058 - val_acc: 0.9018\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9105 - val_loss: 0.2067 - val_acc: 0.9009\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1948 - acc: 0.9109 - val_loss: 0.2068 - val_acc: 0.8998\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1946 - acc: 0.9097 - val_loss: 0.2056 - val_acc: 0.8991\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9101 - val_loss: 0.2081 - val_acc: 0.9009\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1944 - acc: 0.9111 - val_loss: 0.2081 - val_acc: 0.9002\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1945 - acc: 0.9092 - val_loss: 0.2067 - val_acc: 0.9022\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9110 - val_loss: 0.2066 - val_acc: 0.9025\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9102 - val_loss: 0.2067 - val_acc: 0.8989\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1943 - acc: 0.9104 - val_loss: 0.2068 - val_acc: 0.9025\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1944 - acc: 0.9099 - val_loss: 0.2068 - val_acc: 0.8989\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1946 - acc: 0.9090 - val_loss: 0.2080 - val_acc: 0.8971\n",
            "acc: 89.71%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 7s 166us/step - loss: 0.2636 - acc: 0.8872 - val_loss: 0.2256 - val_acc: 0.9018\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2171 - acc: 0.9014 - val_loss: 0.2206 - val_acc: 0.9020\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2126 - acc: 0.9031 - val_loss: 0.2162 - val_acc: 0.9007\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2101 - acc: 0.9033 - val_loss: 0.2146 - val_acc: 0.9044\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2079 - acc: 0.9053 - val_loss: 0.2112 - val_acc: 0.9049\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2070 - acc: 0.9065 - val_loss: 0.2146 - val_acc: 0.9044\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2056 - acc: 0.9057 - val_loss: 0.2164 - val_acc: 0.9020\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2053 - acc: 0.9060 - val_loss: 0.2144 - val_acc: 0.9042\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2051 - acc: 0.9061 - val_loss: 0.2109 - val_acc: 0.9058\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2046 - acc: 0.9070 - val_loss: 0.2120 - val_acc: 0.9047\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2050 - acc: 0.9064 - val_loss: 0.2103 - val_acc: 0.9056\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2045 - acc: 0.9069 - val_loss: 0.2112 - val_acc: 0.9044\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2042 - acc: 0.9068 - val_loss: 0.2118 - val_acc: 0.9033\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2039 - acc: 0.9070 - val_loss: 0.2130 - val_acc: 0.9042\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2039 - acc: 0.9078 - val_loss: 0.2102 - val_acc: 0.9027\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2034 - acc: 0.9078 - val_loss: 0.2136 - val_acc: 0.9047\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2035 - acc: 0.9079 - val_loss: 0.2110 - val_acc: 0.9069\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2032 - acc: 0.9070 - val_loss: 0.2115 - val_acc: 0.9049\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2028 - acc: 0.9082 - val_loss: 0.2120 - val_acc: 0.9071\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2031 - acc: 0.9077 - val_loss: 0.2099 - val_acc: 0.9047\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2025 - acc: 0.9078 - val_loss: 0.2111 - val_acc: 0.9022\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2026 - acc: 0.9077 - val_loss: 0.2091 - val_acc: 0.9060\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2021 - acc: 0.9081 - val_loss: 0.2125 - val_acc: 0.9044\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2023 - acc: 0.9068 - val_loss: 0.2115 - val_acc: 0.9056\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2020 - acc: 0.9074 - val_loss: 0.2098 - val_acc: 0.9044\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2024 - acc: 0.9078 - val_loss: 0.2108 - val_acc: 0.9029\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9081 - val_loss: 0.2116 - val_acc: 0.9007\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2017 - acc: 0.9067 - val_loss: 0.2109 - val_acc: 0.9042\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2015 - acc: 0.9078 - val_loss: 0.2104 - val_acc: 0.9020\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2017 - acc: 0.9072 - val_loss: 0.2101 - val_acc: 0.9053\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2016 - acc: 0.9087 - val_loss: 0.2117 - val_acc: 0.9060\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2017 - acc: 0.9079 - val_loss: 0.2126 - val_acc: 0.9049\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2016 - acc: 0.9078 - val_loss: 0.2095 - val_acc: 0.9051\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9079 - val_loss: 0.2099 - val_acc: 0.9049\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2014 - acc: 0.9079 - val_loss: 0.2121 - val_acc: 0.9033\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2014 - acc: 0.9084 - val_loss: 0.2098 - val_acc: 0.9056\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2014 - acc: 0.9075 - val_loss: 0.2104 - val_acc: 0.9056\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9083 - val_loss: 0.2102 - val_acc: 0.9062\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9083 - val_loss: 0.2085 - val_acc: 0.9062\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2012 - acc: 0.9082 - val_loss: 0.2096 - val_acc: 0.9038\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2008 - acc: 0.9084 - val_loss: 0.2116 - val_acc: 0.9038\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9078 - val_loss: 0.2100 - val_acc: 0.9049\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2011 - acc: 0.9080 - val_loss: 0.2099 - val_acc: 0.9060\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9076 - val_loss: 0.2097 - val_acc: 0.9040\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2010 - acc: 0.9080 - val_loss: 0.2081 - val_acc: 0.9069\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2003 - acc: 0.9082 - val_loss: 0.2094 - val_acc: 0.9053\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9079 - val_loss: 0.2098 - val_acc: 0.9042\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2008 - acc: 0.9088 - val_loss: 0.2114 - val_acc: 0.9044\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2005 - acc: 0.9085 - val_loss: 0.2114 - val_acc: 0.9060\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2009 - acc: 0.9083 - val_loss: 0.2098 - val_acc: 0.9051\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9080 - val_loss: 0.2085 - val_acc: 0.9025\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9083 - val_loss: 0.2121 - val_acc: 0.9009\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2004 - acc: 0.9094 - val_loss: 0.2100 - val_acc: 0.9071\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9085 - val_loss: 0.2110 - val_acc: 0.9022\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2006 - acc: 0.9085 - val_loss: 0.2095 - val_acc: 0.9018\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2007 - acc: 0.9077 - val_loss: 0.2112 - val_acc: 0.9042\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2005 - acc: 0.9091 - val_loss: 0.2106 - val_acc: 0.9016\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2005 - acc: 0.9082 - val_loss: 0.2101 - val_acc: 0.9060\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2001 - acc: 0.9088 - val_loss: 0.2102 - val_acc: 0.9047\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2003 - acc: 0.9085 - val_loss: 0.2089 - val_acc: 0.9047\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2004 - acc: 0.9088 - val_loss: 0.2083 - val_acc: 0.9047\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.2002 - acc: 0.9087 - val_loss: 0.2095 - val_acc: 0.9038\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.2002 - acc: 0.9081 - val_loss: 0.2102 - val_acc: 0.9031\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2002 - acc: 0.9084 - val_loss: 0.2097 - val_acc: 0.9047\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2003 - acc: 0.9087 - val_loss: 0.2114 - val_acc: 0.9049\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2000 - acc: 0.9091 - val_loss: 0.2102 - val_acc: 0.9042\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.2000 - acc: 0.9085 - val_loss: 0.2115 - val_acc: 0.9027\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2006 - acc: 0.9081 - val_loss: 0.2106 - val_acc: 0.9044\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.2001 - acc: 0.9093 - val_loss: 0.2119 - val_acc: 0.9027\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1999 - acc: 0.9093 - val_loss: 0.2092 - val_acc: 0.9053\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 27us/step - loss: 0.1999 - acc: 0.9091 - val_loss: 0.2106 - val_acc: 0.9049\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1997 - acc: 0.9089 - val_loss: 0.2113 - val_acc: 0.9040\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1996 - acc: 0.9098 - val_loss: 0.2092 - val_acc: 0.9022\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9091 - val_loss: 0.2108 - val_acc: 0.9027\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1998 - acc: 0.9085 - val_loss: 0.2098 - val_acc: 0.9038\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1999 - acc: 0.9087 - val_loss: 0.2105 - val_acc: 0.9058\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1996 - acc: 0.9090 - val_loss: 0.2098 - val_acc: 0.9051\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9094 - val_loss: 0.2097 - val_acc: 0.9073\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9099 - val_loss: 0.2127 - val_acc: 0.9025\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1998 - acc: 0.9087 - val_loss: 0.2101 - val_acc: 0.9029\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1995 - acc: 0.9089 - val_loss: 0.2136 - val_acc: 0.9047\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9085 - val_loss: 0.2124 - val_acc: 0.9056\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9092 - val_loss: 0.2115 - val_acc: 0.9049\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9092 - val_loss: 0.2122 - val_acc: 0.9060\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2110 - val_acc: 0.9051\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9094 - val_loss: 0.2109 - val_acc: 0.8998\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9097 - val_loss: 0.2104 - val_acc: 0.9056\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1994 - acc: 0.9097 - val_loss: 0.2090 - val_acc: 0.9051\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9096 - val_loss: 0.2089 - val_acc: 0.9047\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9092 - val_loss: 0.2105 - val_acc: 0.9067\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9090 - val_loss: 0.2124 - val_acc: 0.9013\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1994 - acc: 0.9094 - val_loss: 0.2124 - val_acc: 0.9002\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.1990 - acc: 0.9101 - val_loss: 0.2128 - val_acc: 0.9038\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9093 - val_loss: 0.2141 - val_acc: 0.9031\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1993 - acc: 0.9093 - val_loss: 0.2094 - val_acc: 0.9056\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9091 - val_loss: 0.2108 - val_acc: 0.9022\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1991 - acc: 0.9094 - val_loss: 0.2090 - val_acc: 0.9042\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1995 - acc: 0.9092 - val_loss: 0.2099 - val_acc: 0.9031\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9094 - val_loss: 0.2105 - val_acc: 0.9036\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2107 - val_acc: 0.9025\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1990 - acc: 0.9096 - val_loss: 0.2112 - val_acc: 0.9029\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9090 - val_loss: 0.2109 - val_acc: 0.9020\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9096 - val_loss: 0.2104 - val_acc: 0.9025\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1993 - acc: 0.9089 - val_loss: 0.2097 - val_acc: 0.9025\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9087 - val_loss: 0.2104 - val_acc: 0.9056\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2108 - val_acc: 0.9058\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9099 - val_loss: 0.2117 - val_acc: 0.9033\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9091 - val_loss: 0.2118 - val_acc: 0.9027\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9096 - val_loss: 0.2152 - val_acc: 0.9067\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9085 - val_loss: 0.2102 - val_acc: 0.9038\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9090 - val_loss: 0.2116 - val_acc: 0.9018\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1989 - acc: 0.9100 - val_loss: 0.2107 - val_acc: 0.9047\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1992 - acc: 0.9089 - val_loss: 0.2112 - val_acc: 0.9080\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2142 - val_acc: 0.9011\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9102 - val_loss: 0.2122 - val_acc: 0.9005\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9102 - val_loss: 0.2114 - val_acc: 0.9027\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9101 - val_loss: 0.2158 - val_acc: 0.9000\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2128 - val_acc: 0.9033\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9094 - val_loss: 0.2094 - val_acc: 0.9038\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1990 - acc: 0.9087 - val_loss: 0.2112 - val_acc: 0.9044\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1987 - acc: 0.9097 - val_loss: 0.2108 - val_acc: 0.9040\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9095 - val_loss: 0.2099 - val_acc: 0.9040\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1991 - acc: 0.9099 - val_loss: 0.2121 - val_acc: 0.9064\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9094 - val_loss: 0.2121 - val_acc: 0.9051\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9094 - val_loss: 0.2112 - val_acc: 0.9067\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9098 - val_loss: 0.2093 - val_acc: 0.9029\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9100 - val_loss: 0.2109 - val_acc: 0.9044\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9097 - val_loss: 0.2097 - val_acc: 0.9051\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9093 - val_loss: 0.2110 - val_acc: 0.9053\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9091 - val_loss: 0.2114 - val_acc: 0.9058\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9098 - val_loss: 0.2108 - val_acc: 0.9049\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9094 - val_loss: 0.2122 - val_acc: 0.9036\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9096 - val_loss: 0.2110 - val_acc: 0.9058\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1982 - acc: 0.9105 - val_loss: 0.2119 - val_acc: 0.9036\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9108 - val_loss: 0.2100 - val_acc: 0.9049\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9097 - val_loss: 0.2112 - val_acc: 0.9036\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9092 - val_loss: 0.2155 - val_acc: 0.9002\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1987 - acc: 0.9098 - val_loss: 0.2100 - val_acc: 0.9042\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9096 - val_loss: 0.2114 - val_acc: 0.9058\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9101 - val_loss: 0.2122 - val_acc: 0.9042\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1983 - acc: 0.9100 - val_loss: 0.2133 - val_acc: 0.9022\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1986 - acc: 0.9093 - val_loss: 0.2163 - val_acc: 0.9062\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1984 - acc: 0.9099 - val_loss: 0.2145 - val_acc: 0.9051\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1981 - acc: 0.9105 - val_loss: 0.2121 - val_acc: 0.9029\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1988 - acc: 0.9101 - val_loss: 0.2100 - val_acc: 0.9036\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9099 - val_loss: 0.2106 - val_acc: 0.9056\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9099 - val_loss: 0.2099 - val_acc: 0.9022\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1985 - acc: 0.9098 - val_loss: 0.2106 - val_acc: 0.9073\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.1984 - acc: 0.9105 - val_loss: 0.2137 - val_acc: 0.9062\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.1985 - acc: 0.9099 - val_loss: 0.2117 - val_acc: 0.9064\n",
            "acc: 90.64%\n",
            "90.55% (+/- 0.46%)\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   264    264        yes\n",
            "   163    3829        no\n",
            "Accuracy: 0.9054876025745947\n",
            "Sensitivity: 0.5004726791453961\n",
            "Specificity: 0.9591453334001303\n",
            "Precision: 0.6187470780738663\n",
            "f_score: 0.5533605100867566\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lqxC5G2TI9X",
        "colab_type": "code",
        "outputId": "f8eaa770-2ee4-4376-f5e4-2d5b35181596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "percentage_split_NN_PCA(0.25,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 33908 samples, validate on 11303 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "33908/33908 [==============================] - 1s 23us/step - loss: 0.3686 - acc: 0.8585 - val_loss: 0.3165 - val_acc: 0.8859\n",
            "Epoch 2/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3146 - acc: 0.8872 - val_loss: 0.3148 - val_acc: 0.8841\n",
            "Epoch 3/150\n",
            "33908/33908 [==============================] - 0s 14us/step - loss: 0.3115 - acc: 0.8865 - val_loss: 0.3121 - val_acc: 0.8846\n",
            "Epoch 4/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3098 - acc: 0.8867 - val_loss: 0.3116 - val_acc: 0.8860\n",
            "Epoch 5/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3091 - acc: 0.8858 - val_loss: 0.3111 - val_acc: 0.8827\n",
            "Epoch 6/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3089 - acc: 0.8860 - val_loss: 0.3115 - val_acc: 0.8826\n",
            "Epoch 7/150\n",
            "33908/33908 [==============================] - 0s 15us/step - loss: 0.3083 - acc: 0.8870 - val_loss: 0.3090 - val_acc: 0.8860\n",
            "Epoch 8/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3084 - acc: 0.8863 - val_loss: 0.3089 - val_acc: 0.8856\n",
            "Epoch 9/150\n",
            "33908/33908 [==============================] - 0s 15us/step - loss: 0.3080 - acc: 0.8870 - val_loss: 0.3081 - val_acc: 0.8868\n",
            "Epoch 10/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3075 - acc: 0.8868 - val_loss: 0.3085 - val_acc: 0.8854\n",
            "Epoch 11/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3076 - acc: 0.8871 - val_loss: 0.3090 - val_acc: 0.8848\n",
            "Epoch 12/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8867 - val_loss: 0.3078 - val_acc: 0.8842\n",
            "Epoch 13/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3071 - acc: 0.8868 - val_loss: 0.3069 - val_acc: 0.8853\n",
            "Epoch 14/150\n",
            "33908/33908 [==============================] - 0s 14us/step - loss: 0.3069 - acc: 0.8863 - val_loss: 0.3105 - val_acc: 0.8821\n",
            "Epoch 15/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8866 - val_loss: 0.3090 - val_acc: 0.8845\n",
            "Epoch 16/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8869 - val_loss: 0.3071 - val_acc: 0.8846\n",
            "Epoch 17/150\n",
            "33908/33908 [==============================] - 0s 14us/step - loss: 0.3062 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8860\n",
            "Epoch 18/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8873 - val_loss: 0.3069 - val_acc: 0.8856\n",
            "Epoch 19/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8883 - val_loss: 0.3071 - val_acc: 0.8850\n",
            "Epoch 20/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8865 - val_loss: 0.3075 - val_acc: 0.8838\n",
            "Epoch 21/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8872 - val_loss: 0.3080 - val_acc: 0.8848\n",
            "Epoch 22/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8875 - val_loss: 0.3077 - val_acc: 0.8854\n",
            "Epoch 23/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3062 - acc: 0.8873 - val_loss: 0.3063 - val_acc: 0.8847\n",
            "Epoch 24/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8872 - val_loss: 0.3069 - val_acc: 0.8847\n",
            "Epoch 25/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8870 - val_loss: 0.3074 - val_acc: 0.8839\n",
            "Epoch 26/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8873 - val_loss: 0.3062 - val_acc: 0.8859\n",
            "Epoch 27/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3062 - acc: 0.8873 - val_loss: 0.3070 - val_acc: 0.8837\n",
            "Epoch 28/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8875 - val_loss: 0.3076 - val_acc: 0.8835\n",
            "Epoch 29/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8881 - val_loss: 0.3094 - val_acc: 0.8840\n",
            "Epoch 30/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8870 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 31/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8876 - val_loss: 0.3072 - val_acc: 0.8847\n",
            "Epoch 32/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8876 - val_loss: 0.3083 - val_acc: 0.8830\n",
            "Epoch 33/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8857\n",
            "Epoch 34/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8883 - val_loss: 0.3066 - val_acc: 0.8859\n",
            "Epoch 35/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8874 - val_loss: 0.3070 - val_acc: 0.8866\n",
            "Epoch 36/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8873 - val_loss: 0.3088 - val_acc: 0.8845\n",
            "Epoch 37/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8875 - val_loss: 0.3066 - val_acc: 0.8855\n",
            "Epoch 38/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3060 - acc: 0.8870 - val_loss: 0.3075 - val_acc: 0.8844\n",
            "Epoch 39/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3057 - acc: 0.8878 - val_loss: 0.3073 - val_acc: 0.8852\n",
            "Epoch 40/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3060 - acc: 0.8863 - val_loss: 0.3063 - val_acc: 0.8848\n",
            "Epoch 41/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8868 - val_loss: 0.3066 - val_acc: 0.8857\n",
            "Epoch 42/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8877 - val_loss: 0.3069 - val_acc: 0.8843\n",
            "Epoch 43/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8870 - val_loss: 0.3062 - val_acc: 0.8853\n",
            "Epoch 44/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8869 - val_loss: 0.3067 - val_acc: 0.8853\n",
            "Epoch 45/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8873 - val_loss: 0.3066 - val_acc: 0.8851\n",
            "Epoch 46/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3057 - acc: 0.8876 - val_loss: 0.3067 - val_acc: 0.8839\n",
            "Epoch 47/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8876 - val_loss: 0.3084 - val_acc: 0.8833\n",
            "Epoch 48/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8870 - val_loss: 0.3065 - val_acc: 0.8840\n",
            "Epoch 49/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8866 - val_loss: 0.3090 - val_acc: 0.8853\n",
            "Epoch 50/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3060 - acc: 0.8872 - val_loss: 0.3059 - val_acc: 0.8844\n",
            "Epoch 51/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8874 - val_loss: 0.3068 - val_acc: 0.8845\n",
            "Epoch 52/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8872 - val_loss: 0.3071 - val_acc: 0.8841\n",
            "Epoch 53/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8878 - val_loss: 0.3072 - val_acc: 0.8830\n",
            "Epoch 54/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8868 - val_loss: 0.3069 - val_acc: 0.8849\n",
            "Epoch 55/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3058 - acc: 0.8865 - val_loss: 0.3072 - val_acc: 0.8839\n",
            "Epoch 56/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8875 - val_loss: 0.3062 - val_acc: 0.8856\n",
            "Epoch 57/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8871 - val_loss: 0.3065 - val_acc: 0.8848\n",
            "Epoch 58/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8862 - val_loss: 0.3073 - val_acc: 0.8854\n",
            "Epoch 59/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8868 - val_loss: 0.3071 - val_acc: 0.8846\n",
            "Epoch 60/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8869 - val_loss: 0.3070 - val_acc: 0.8852\n",
            "Epoch 61/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8863 - val_loss: 0.3064 - val_acc: 0.8850\n",
            "Epoch 62/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8872 - val_loss: 0.3071 - val_acc: 0.8842\n",
            "Epoch 63/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8840\n",
            "Epoch 64/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3054 - acc: 0.8875 - val_loss: 0.3078 - val_acc: 0.8850\n",
            "Epoch 65/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3051 - acc: 0.8875 - val_loss: 0.3083 - val_acc: 0.8816\n",
            "Epoch 66/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3073 - val_acc: 0.8834\n",
            "Epoch 67/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8875 - val_loss: 0.3069 - val_acc: 0.8850\n",
            "Epoch 68/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8874 - val_loss: 0.3075 - val_acc: 0.8846\n",
            "Epoch 69/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8871 - val_loss: 0.3069 - val_acc: 0.8837\n",
            "Epoch 70/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8869 - val_loss: 0.3072 - val_acc: 0.8855\n",
            "Epoch 71/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8875 - val_loss: 0.3087 - val_acc: 0.8829\n",
            "Epoch 72/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8870 - val_loss: 0.3066 - val_acc: 0.8856\n",
            "Epoch 73/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3075 - val_acc: 0.8860\n",
            "Epoch 74/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8875 - val_loss: 0.3072 - val_acc: 0.8845\n",
            "Epoch 75/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8874 - val_loss: 0.3071 - val_acc: 0.8852\n",
            "Epoch 76/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8875 - val_loss: 0.3071 - val_acc: 0.8828\n",
            "Epoch 77/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8871 - val_loss: 0.3073 - val_acc: 0.8855\n",
            "Epoch 78/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3071 - val_acc: 0.8849\n",
            "Epoch 79/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8872 - val_loss: 0.3062 - val_acc: 0.8837\n",
            "Epoch 80/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3054 - acc: 0.8869 - val_loss: 0.3067 - val_acc: 0.8860\n",
            "Epoch 81/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8873 - val_loss: 0.3068 - val_acc: 0.8868\n",
            "Epoch 82/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8866 - val_loss: 0.3072 - val_acc: 0.8844\n",
            "Epoch 83/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3051 - acc: 0.8873 - val_loss: 0.3066 - val_acc: 0.8859\n",
            "Epoch 84/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8866 - val_loss: 0.3060 - val_acc: 0.8865\n",
            "Epoch 85/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3068 - val_acc: 0.8850\n",
            "Epoch 86/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8873 - val_loss: 0.3062 - val_acc: 0.8860\n",
            "Epoch 87/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8878 - val_loss: 0.3065 - val_acc: 0.8839\n",
            "Epoch 88/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3072 - val_acc: 0.8847\n",
            "Epoch 89/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8874 - val_loss: 0.3061 - val_acc: 0.8844\n",
            "Epoch 90/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8866 - val_loss: 0.3062 - val_acc: 0.8869\n",
            "Epoch 91/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3057 - val_acc: 0.8858\n",
            "Epoch 92/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3060 - val_acc: 0.8861\n",
            "Epoch 93/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3065 - val_acc: 0.8867\n",
            "Epoch 94/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8870 - val_loss: 0.3060 - val_acc: 0.8868\n",
            "Epoch 95/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3065 - val_acc: 0.8864\n",
            "Epoch 96/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3066 - val_acc: 0.8869\n",
            "Epoch 97/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8870 - val_loss: 0.3063 - val_acc: 0.8853\n",
            "Epoch 98/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3047 - acc: 0.8868 - val_loss: 0.3065 - val_acc: 0.8856\n",
            "Epoch 99/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3062 - val_acc: 0.8844\n",
            "Epoch 100/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8868 - val_loss: 0.3069 - val_acc: 0.8861\n",
            "Epoch 101/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3074 - val_acc: 0.8862\n",
            "Epoch 102/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3056 - val_acc: 0.8865\n",
            "Epoch 103/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8870 - val_loss: 0.3074 - val_acc: 0.8836\n",
            "Epoch 104/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8868 - val_loss: 0.3065 - val_acc: 0.8847\n",
            "Epoch 105/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8871 - val_loss: 0.3063 - val_acc: 0.8877\n",
            "Epoch 106/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3057 - val_acc: 0.8876\n",
            "Epoch 107/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8866 - val_loss: 0.3071 - val_acc: 0.8852\n",
            "Epoch 108/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3050 - acc: 0.8868 - val_loss: 0.3065 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8868 - val_loss: 0.3060 - val_acc: 0.8860\n",
            "Epoch 110/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8870 - val_loss: 0.3070 - val_acc: 0.8842\n",
            "Epoch 111/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8868 - val_loss: 0.3076 - val_acc: 0.8839\n",
            "Epoch 112/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8880 - val_loss: 0.3083 - val_acc: 0.8843\n",
            "Epoch 113/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3057 - val_acc: 0.8866\n",
            "Epoch 114/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3072 - val_acc: 0.8840\n",
            "Epoch 115/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3046 - acc: 0.8867 - val_loss: 0.3058 - val_acc: 0.8870\n",
            "Epoch 116/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8867 - val_loss: 0.3059 - val_acc: 0.8860\n",
            "Epoch 117/150\n",
            "33908/33908 [==============================] - 1s 18us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3077 - val_acc: 0.8819\n",
            "Epoch 118/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8867 - val_loss: 0.3060 - val_acc: 0.8861\n",
            "Epoch 119/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8868 - val_loss: 0.3061 - val_acc: 0.8855\n",
            "Epoch 120/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8871 - val_loss: 0.3059 - val_acc: 0.8860\n",
            "Epoch 121/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8864 - val_loss: 0.3068 - val_acc: 0.8874\n",
            "Epoch 122/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3064 - val_acc: 0.8861\n",
            "Epoch 123/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8866 - val_loss: 0.3070 - val_acc: 0.8860\n",
            "Epoch 124/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3061 - val_acc: 0.8859\n",
            "Epoch 125/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8871 - val_loss: 0.3067 - val_acc: 0.8856\n",
            "Epoch 126/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3046 - acc: 0.8875 - val_loss: 0.3069 - val_acc: 0.8859\n",
            "Epoch 127/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3067 - val_acc: 0.8872\n",
            "Epoch 128/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3072 - val_acc: 0.8868\n",
            "Epoch 129/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3061 - val_acc: 0.8862\n",
            "Epoch 130/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8870 - val_loss: 0.3058 - val_acc: 0.8852\n",
            "Epoch 131/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8865 - val_loss: 0.3061 - val_acc: 0.8859\n",
            "Epoch 132/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3074 - val_acc: 0.8847\n",
            "Epoch 133/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8870 - val_loss: 0.3066 - val_acc: 0.8848\n",
            "Epoch 134/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3045 - acc: 0.8868 - val_loss: 0.3070 - val_acc: 0.8852\n",
            "Epoch 135/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8873 - val_loss: 0.3065 - val_acc: 0.8859\n",
            "Epoch 136/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3064 - val_acc: 0.8841\n",
            "Epoch 137/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3057 - val_acc: 0.8868\n",
            "Epoch 138/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8876 - val_loss: 0.3065 - val_acc: 0.8841\n",
            "Epoch 139/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3070 - val_acc: 0.8853\n",
            "Epoch 140/150\n",
            "33908/33908 [==============================] - 1s 17us/step - loss: 0.3046 - acc: 0.8869 - val_loss: 0.3064 - val_acc: 0.8843\n",
            "Epoch 141/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8869 - val_loss: 0.3089 - val_acc: 0.8836\n",
            "Epoch 142/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8869 - val_loss: 0.3077 - val_acc: 0.8861\n",
            "Epoch 143/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8867 - val_loss: 0.3066 - val_acc: 0.8870\n",
            "Epoch 144/150\n",
            "33908/33908 [==============================] - 1s 16us/step - loss: 0.3043 - acc: 0.8875 - val_loss: 0.3072 - val_acc: 0.8839\n",
            "Epoch 145/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8870 - val_loss: 0.3065 - val_acc: 0.8865\n",
            "Epoch 146/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3069 - val_acc: 0.8833\n",
            "Epoch 147/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8870 - val_loss: 0.3078 - val_acc: 0.8841\n",
            "Epoch 148/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3066 - val_acc: 0.8845\n",
            "Epoch 149/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8869 - val_loss: 0.3073 - val_acc: 0.8846\n",
            "Epoch 150/150\n",
            "33908/33908 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3063 - val_acc: 0.8845\n",
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   234    1088        yes\n",
            "   218    9763        no\n",
            "Accuracy: 0.884455454304167\n",
            "Sensitivity: 0.17700453857791226\n",
            "Specificity: 0.9781585011521892\n",
            "Precision: 0.5176991150442478\n",
            "f_score: 0.26381059751972946\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmjXXMxuTRR4",
        "colab_type": "code",
        "outputId": "8d331d7e-8cc7-4baf-ebf9-a1246d5a7f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "crossvalidate_NN_PCA(10,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40689 samples, validate on 4522 samples\n",
            "Epoch 1/150\n",
            "40689/40689 [==============================] - 1s 19us/step - loss: 0.3348 - acc: 0.8761 - val_loss: 0.3214 - val_acc: 0.8824\n",
            "Epoch 2/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3127 - acc: 0.8854 - val_loss: 0.3186 - val_acc: 0.8837\n",
            "Epoch 3/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3114 - acc: 0.8845 - val_loss: 0.3173 - val_acc: 0.8839\n",
            "Epoch 4/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3112 - acc: 0.8846 - val_loss: 0.3178 - val_acc: 0.8819\n",
            "Epoch 5/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3109 - acc: 0.8847 - val_loss: 0.3164 - val_acc: 0.8830\n",
            "Epoch 6/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3103 - acc: 0.8850 - val_loss: 0.3149 - val_acc: 0.8861\n",
            "Epoch 7/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3097 - acc: 0.8858 - val_loss: 0.3141 - val_acc: 0.8841\n",
            "Epoch 8/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3094 - acc: 0.8857 - val_loss: 0.3147 - val_acc: 0.8852\n",
            "Epoch 9/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3094 - acc: 0.8860 - val_loss: 0.3151 - val_acc: 0.8859\n",
            "Epoch 10/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3091 - acc: 0.8863 - val_loss: 0.3135 - val_acc: 0.8832\n",
            "Epoch 11/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3091 - acc: 0.8865 - val_loss: 0.3126 - val_acc: 0.8863\n",
            "Epoch 12/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8854 - val_loss: 0.3118 - val_acc: 0.8863\n",
            "Epoch 13/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8867 - val_loss: 0.3133 - val_acc: 0.8874\n",
            "Epoch 14/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8860 - val_loss: 0.3119 - val_acc: 0.8863\n",
            "Epoch 15/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8867 - val_loss: 0.3125 - val_acc: 0.8859\n",
            "Epoch 16/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8872 - val_loss: 0.3127 - val_acc: 0.8857\n",
            "Epoch 17/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8865 - val_loss: 0.3115 - val_acc: 0.8857\n",
            "Epoch 18/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8869 - val_loss: 0.3128 - val_acc: 0.8852\n",
            "Epoch 19/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8869 - val_loss: 0.3127 - val_acc: 0.8854\n",
            "Epoch 20/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3074 - acc: 0.8865 - val_loss: 0.3113 - val_acc: 0.8870\n",
            "Epoch 21/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8861\n",
            "Epoch 22/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3070 - acc: 0.8861 - val_loss: 0.3120 - val_acc: 0.8843\n",
            "Epoch 23/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8863 - val_loss: 0.3103 - val_acc: 0.8879\n",
            "Epoch 24/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8863 - val_loss: 0.3113 - val_acc: 0.8857\n",
            "Epoch 25/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8865 - val_loss: 0.3099 - val_acc: 0.8859\n",
            "Epoch 26/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8863 - val_loss: 0.3149 - val_acc: 0.8824\n",
            "Epoch 27/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8865 - val_loss: 0.3116 - val_acc: 0.8863\n",
            "Epoch 28/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8861 - val_loss: 0.3106 - val_acc: 0.8859\n",
            "Epoch 29/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3057 - acc: 0.8865 - val_loss: 0.3104 - val_acc: 0.8866\n",
            "Epoch 30/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8866 - val_loss: 0.3098 - val_acc: 0.8837\n",
            "Epoch 31/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8863 - val_loss: 0.3103 - val_acc: 0.8854\n",
            "Epoch 32/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8863 - val_loss: 0.3102 - val_acc: 0.8850\n",
            "Epoch 33/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8868 - val_loss: 0.3096 - val_acc: 0.8846\n",
            "Epoch 34/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8863 - val_loss: 0.3098 - val_acc: 0.8832\n",
            "Epoch 35/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8861 - val_loss: 0.3106 - val_acc: 0.8854\n",
            "Epoch 36/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3048 - acc: 0.8870 - val_loss: 0.3096 - val_acc: 0.8852\n",
            "Epoch 37/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8855 - val_loss: 0.3096 - val_acc: 0.8863\n",
            "Epoch 38/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8861 - val_loss: 0.3116 - val_acc: 0.8837\n",
            "Epoch 39/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8857 - val_loss: 0.3106 - val_acc: 0.8846\n",
            "Epoch 40/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3048 - acc: 0.8866 - val_loss: 0.3092 - val_acc: 0.8861\n",
            "Epoch 41/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3045 - acc: 0.8865 - val_loss: 0.3107 - val_acc: 0.8877\n",
            "Epoch 42/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8859 - val_loss: 0.3105 - val_acc: 0.8848\n",
            "Epoch 43/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8867 - val_loss: 0.3101 - val_acc: 0.8852\n",
            "Epoch 44/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8865 - val_loss: 0.3098 - val_acc: 0.8874\n",
            "Epoch 45/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8869 - val_loss: 0.3101 - val_acc: 0.8846\n",
            "Epoch 46/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8861 - val_loss: 0.3090 - val_acc: 0.8854\n",
            "Epoch 47/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8861 - val_loss: 0.3089 - val_acc: 0.8854\n",
            "Epoch 48/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8861 - val_loss: 0.3095 - val_acc: 0.8848\n",
            "Epoch 49/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8857 - val_loss: 0.3102 - val_acc: 0.8846\n",
            "Epoch 50/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8860 - val_loss: 0.3097 - val_acc: 0.8846\n",
            "Epoch 51/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8862 - val_loss: 0.3095 - val_acc: 0.8854\n",
            "Epoch 52/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8857 - val_loss: 0.3103 - val_acc: 0.8866\n",
            "Epoch 53/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8863 - val_loss: 0.3102 - val_acc: 0.8857\n",
            "Epoch 54/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8859 - val_loss: 0.3101 - val_acc: 0.8854\n",
            "Epoch 55/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8867 - val_loss: 0.3094 - val_acc: 0.8854\n",
            "Epoch 56/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8862 - val_loss: 0.3089 - val_acc: 0.8866\n",
            "Epoch 57/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3044 - acc: 0.8862 - val_loss: 0.3091 - val_acc: 0.8863\n",
            "Epoch 58/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3042 - acc: 0.8864 - val_loss: 0.3089 - val_acc: 0.8857\n",
            "Epoch 59/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3043 - acc: 0.8868 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 60/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8859 - val_loss: 0.3099 - val_acc: 0.8846\n",
            "Epoch 61/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3043 - acc: 0.8862 - val_loss: 0.3097 - val_acc: 0.8854\n",
            "Epoch 62/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3040 - acc: 0.8868 - val_loss: 0.3099 - val_acc: 0.8870\n",
            "Epoch 63/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8864 - val_loss: 0.3102 - val_acc: 0.8868\n",
            "Epoch 64/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3045 - acc: 0.8864 - val_loss: 0.3111 - val_acc: 0.8843\n",
            "Epoch 65/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8854 - val_loss: 0.3103 - val_acc: 0.8863\n",
            "Epoch 66/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3044 - acc: 0.8857 - val_loss: 0.3097 - val_acc: 0.8852\n",
            "Epoch 67/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8858 - val_loss: 0.3083 - val_acc: 0.8848\n",
            "Epoch 68/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8863 - val_loss: 0.3090 - val_acc: 0.8837\n",
            "Epoch 69/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8861 - val_loss: 0.3098 - val_acc: 0.8859\n",
            "Epoch 70/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8861 - val_loss: 0.3109 - val_acc: 0.8830\n",
            "Epoch 71/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8854 - val_loss: 0.3090 - val_acc: 0.8846\n",
            "Epoch 72/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8861 - val_loss: 0.3087 - val_acc: 0.8868\n",
            "Epoch 73/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3045 - acc: 0.8858 - val_loss: 0.3091 - val_acc: 0.8874\n",
            "Epoch 74/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8858 - val_loss: 0.3099 - val_acc: 0.8850\n",
            "Epoch 75/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8860 - val_loss: 0.3098 - val_acc: 0.8848\n",
            "Epoch 76/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3041 - acc: 0.8862 - val_loss: 0.3098 - val_acc: 0.8859\n",
            "Epoch 77/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8860 - val_loss: 0.3083 - val_acc: 0.8868\n",
            "Epoch 78/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3124 - val_acc: 0.8870\n",
            "Epoch 79/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8861 - val_loss: 0.3096 - val_acc: 0.8872\n",
            "Epoch 80/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8859 - val_loss: 0.3092 - val_acc: 0.8859\n",
            "Epoch 81/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8868 - val_loss: 0.3093 - val_acc: 0.8859\n",
            "Epoch 82/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3042 - acc: 0.8861 - val_loss: 0.3098 - val_acc: 0.8857\n",
            "Epoch 83/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8864 - val_loss: 0.3105 - val_acc: 0.8859\n",
            "Epoch 84/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8861 - val_loss: 0.3104 - val_acc: 0.8839\n",
            "Epoch 85/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8862 - val_loss: 0.3095 - val_acc: 0.8861\n",
            "Epoch 86/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8866 - val_loss: 0.3089 - val_acc: 0.8866\n",
            "Epoch 87/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3041 - acc: 0.8858 - val_loss: 0.3086 - val_acc: 0.8874\n",
            "Epoch 88/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3042 - acc: 0.8858 - val_loss: 0.3093 - val_acc: 0.8881\n",
            "Epoch 89/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8865 - val_loss: 0.3102 - val_acc: 0.8846\n",
            "Epoch 90/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8869 - val_loss: 0.3099 - val_acc: 0.8863\n",
            "Epoch 91/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8864 - val_loss: 0.3087 - val_acc: 0.8872\n",
            "Epoch 92/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8869 - val_loss: 0.3104 - val_acc: 0.8859\n",
            "Epoch 93/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8865 - val_loss: 0.3115 - val_acc: 0.8852\n",
            "Epoch 94/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3041 - acc: 0.8865 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 95/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3044 - acc: 0.8862 - val_loss: 0.3086 - val_acc: 0.8877\n",
            "Epoch 96/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8866 - val_loss: 0.3100 - val_acc: 0.8868\n",
            "Epoch 97/150\n",
            "40689/40689 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8868 - val_loss: 0.3093 - val_acc: 0.8879\n",
            "Epoch 98/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3040 - acc: 0.8864 - val_loss: 0.3111 - val_acc: 0.8857\n",
            "Epoch 99/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8857 - val_loss: 0.3124 - val_acc: 0.8859\n",
            "Epoch 100/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8859 - val_loss: 0.3101 - val_acc: 0.8848\n",
            "Epoch 101/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8859 - val_loss: 0.3107 - val_acc: 0.8881\n",
            "Epoch 102/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8865 - val_loss: 0.3099 - val_acc: 0.8883\n",
            "Epoch 103/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3039 - acc: 0.8858 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 104/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8862 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 105/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8865 - val_loss: 0.3102 - val_acc: 0.8857\n",
            "Epoch 106/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8863 - val_loss: 0.3107 - val_acc: 0.8870\n",
            "Epoch 107/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8859 - val_loss: 0.3109 - val_acc: 0.8857\n",
            "Epoch 108/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8860 - val_loss: 0.3103 - val_acc: 0.8854\n",
            "Epoch 109/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8862 - val_loss: 0.3100 - val_acc: 0.8879\n",
            "Epoch 110/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3109 - val_acc: 0.8852\n",
            "Epoch 111/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8863 - val_loss: 0.3111 - val_acc: 0.8861\n",
            "Epoch 112/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8861 - val_loss: 0.3101 - val_acc: 0.8861\n",
            "Epoch 113/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3040 - acc: 0.8864 - val_loss: 0.3105 - val_acc: 0.8877\n",
            "Epoch 114/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8860 - val_loss: 0.3097 - val_acc: 0.8872\n",
            "Epoch 115/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8863 - val_loss: 0.3116 - val_acc: 0.8879\n",
            "Epoch 116/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8868 - val_loss: 0.3118 - val_acc: 0.8868\n",
            "Epoch 117/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8864 - val_loss: 0.3111 - val_acc: 0.8881\n",
            "Epoch 118/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8865 - val_loss: 0.3113 - val_acc: 0.8879\n",
            "Epoch 119/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8862 - val_loss: 0.3101 - val_acc: 0.8877\n",
            "Epoch 120/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8861 - val_loss: 0.3126 - val_acc: 0.8866\n",
            "Epoch 121/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8862 - val_loss: 0.3101 - val_acc: 0.8881\n",
            "Epoch 122/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8865 - val_loss: 0.3110 - val_acc: 0.8850\n",
            "Epoch 123/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8854 - val_loss: 0.3105 - val_acc: 0.8866\n",
            "Epoch 124/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8860 - val_loss: 0.3103 - val_acc: 0.8863\n",
            "Epoch 125/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8866 - val_loss: 0.3096 - val_acc: 0.8866\n",
            "Epoch 126/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3036 - acc: 0.8868 - val_loss: 0.3104 - val_acc: 0.8852\n",
            "Epoch 127/150\n",
            "40689/40689 [==============================] - 1s 16us/step - loss: 0.3038 - acc: 0.8859 - val_loss: 0.3100 - val_acc: 0.8861\n",
            "Epoch 128/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8863 - val_loss: 0.3103 - val_acc: 0.8868\n",
            "Epoch 129/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3096 - val_acc: 0.8877\n",
            "Epoch 130/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8862 - val_loss: 0.3118 - val_acc: 0.8857\n",
            "Epoch 131/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3099 - val_acc: 0.8877\n",
            "Epoch 132/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8863 - val_loss: 0.3106 - val_acc: 0.8870\n",
            "Epoch 133/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8859 - val_loss: 0.3116 - val_acc: 0.8861\n",
            "Epoch 134/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8861 - val_loss: 0.3104 - val_acc: 0.8859\n",
            "Epoch 135/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8866 - val_loss: 0.3114 - val_acc: 0.8870\n",
            "Epoch 136/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3037 - acc: 0.8862 - val_loss: 0.3099 - val_acc: 0.8872\n",
            "Epoch 137/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8863 - val_loss: 0.3101 - val_acc: 0.8863\n",
            "Epoch 138/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8869 - val_loss: 0.3103 - val_acc: 0.8877\n",
            "Epoch 139/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8865 - val_loss: 0.3100 - val_acc: 0.8861\n",
            "Epoch 140/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8865 - val_loss: 0.3118 - val_acc: 0.8872\n",
            "Epoch 141/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8862 - val_loss: 0.3093 - val_acc: 0.8861\n",
            "Epoch 142/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8869 - val_loss: 0.3106 - val_acc: 0.8872\n",
            "Epoch 143/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3032 - acc: 0.8865 - val_loss: 0.3109 - val_acc: 0.8868\n",
            "Epoch 144/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8863 - val_loss: 0.3115 - val_acc: 0.8843\n",
            "Epoch 145/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8863 - val_loss: 0.3115 - val_acc: 0.8857\n",
            "Epoch 146/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8866 - val_loss: 0.3103 - val_acc: 0.8879\n",
            "Epoch 147/150\n",
            "40689/40689 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8870 - val_loss: 0.3108 - val_acc: 0.8868\n",
            "Epoch 148/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8863 - val_loss: 0.3105 - val_acc: 0.8863\n",
            "Epoch 149/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8859 - val_loss: 0.3107 - val_acc: 0.8877\n",
            "Epoch 150/150\n",
            "40689/40689 [==============================] - 1s 13us/step - loss: 0.3032 - acc: 0.8862 - val_loss: 0.3121 - val_acc: 0.8863\n",
            "acc: 88.63%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 21us/step - loss: 0.3453 - acc: 0.8679 - val_loss: 0.3142 - val_acc: 0.8879\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3118 - acc: 0.8870 - val_loss: 0.3128 - val_acc: 0.8879\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3099 - acc: 0.8868 - val_loss: 0.3133 - val_acc: 0.8872\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3091 - acc: 0.8873 - val_loss: 0.3125 - val_acc: 0.8881\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3088 - acc: 0.8870 - val_loss: 0.3152 - val_acc: 0.8808\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3086 - acc: 0.8873 - val_loss: 0.3116 - val_acc: 0.8885\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3083 - acc: 0.8867 - val_loss: 0.3106 - val_acc: 0.8876\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8862 - val_loss: 0.3111 - val_acc: 0.8881\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3078 - acc: 0.8876 - val_loss: 0.3110 - val_acc: 0.8856\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3077 - acc: 0.8869 - val_loss: 0.3121 - val_acc: 0.8874\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3072 - acc: 0.8867 - val_loss: 0.3111 - val_acc: 0.8868\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3066 - acc: 0.8871 - val_loss: 0.3110 - val_acc: 0.8856\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3067 - acc: 0.8871 - val_loss: 0.3099 - val_acc: 0.8863\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.3100 - val_acc: 0.8854\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8871 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3064 - acc: 0.8875 - val_loss: 0.3091 - val_acc: 0.8887\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3063 - acc: 0.8872 - val_loss: 0.3104 - val_acc: 0.8872\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3105 - val_acc: 0.8830\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8877 - val_loss: 0.3098 - val_acc: 0.8850\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8872 - val_loss: 0.3134 - val_acc: 0.8830\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8876 - val_loss: 0.3076 - val_acc: 0.8859\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8877 - val_loss: 0.3113 - val_acc: 0.8872\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8874 - val_loss: 0.3079 - val_acc: 0.8868\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3050 - acc: 0.8880 - val_loss: 0.3099 - val_acc: 0.8870\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8878 - val_loss: 0.3101 - val_acc: 0.8861\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3105 - val_acc: 0.8883\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8877 - val_loss: 0.3102 - val_acc: 0.8865\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8879 - val_loss: 0.3085 - val_acc: 0.8845\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8873 - val_loss: 0.3097 - val_acc: 0.8821\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3090 - val_acc: 0.8852\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3100 - val_acc: 0.8872\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8873 - val_loss: 0.3086 - val_acc: 0.8863\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8870 - val_loss: 0.3082 - val_acc: 0.8861\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8874 - val_loss: 0.3087 - val_acc: 0.8881\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8873 - val_loss: 0.3088 - val_acc: 0.8872\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3040 - acc: 0.8876 - val_loss: 0.3108 - val_acc: 0.8883\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3039 - acc: 0.8884 - val_loss: 0.3078 - val_acc: 0.8872\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8879 - val_loss: 0.3094 - val_acc: 0.8854\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8878 - val_loss: 0.3102 - val_acc: 0.8872\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8883 - val_loss: 0.3084 - val_acc: 0.8859\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8877 - val_loss: 0.3072 - val_acc: 0.8879\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8877 - val_loss: 0.3082 - val_acc: 0.8868\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8882 - val_loss: 0.3111 - val_acc: 0.8870\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8881 - val_loss: 0.3081 - val_acc: 0.8865\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8880 - val_loss: 0.3100 - val_acc: 0.8839\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8881 - val_loss: 0.3087 - val_acc: 0.8861\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8873 - val_loss: 0.3089 - val_acc: 0.8839\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8876 - val_loss: 0.3073 - val_acc: 0.8859\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8883 - val_loss: 0.3093 - val_acc: 0.8834\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8881 - val_loss: 0.3083 - val_acc: 0.8848\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3038 - acc: 0.8877 - val_loss: 0.3096 - val_acc: 0.8854\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8878 - val_loss: 0.3096 - val_acc: 0.8870\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8880 - val_loss: 0.3084 - val_acc: 0.8850\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3101 - val_acc: 0.8856\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3035 - acc: 0.8878 - val_loss: 0.3092 - val_acc: 0.8848\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8874 - val_loss: 0.3081 - val_acc: 0.8861\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8875 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8878 - val_loss: 0.3094 - val_acc: 0.8861\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8880 - val_loss: 0.3093 - val_acc: 0.8863\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8874 - val_loss: 0.3093 - val_acc: 0.8863\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8875 - val_loss: 0.3078 - val_acc: 0.8852\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8879 - val_loss: 0.3129 - val_acc: 0.8841\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8878 - val_loss: 0.3096 - val_acc: 0.8861\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.8880 - val_loss: 0.3083 - val_acc: 0.8868\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8879 - val_loss: 0.3093 - val_acc: 0.8854\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8876 - val_loss: 0.3090 - val_acc: 0.8854\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8883 - val_loss: 0.3089 - val_acc: 0.8848\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8881 - val_loss: 0.3090 - val_acc: 0.8874\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8882 - val_loss: 0.3078 - val_acc: 0.8845\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3031 - acc: 0.8880 - val_loss: 0.3086 - val_acc: 0.8848\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8874 - val_loss: 0.3101 - val_acc: 0.8854\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8879 - val_loss: 0.3076 - val_acc: 0.8850\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8879 - val_loss: 0.3079 - val_acc: 0.8852\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8881 - val_loss: 0.3080 - val_acc: 0.8856\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8882 - val_loss: 0.3082 - val_acc: 0.8845\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8877 - val_loss: 0.3078 - val_acc: 0.8854\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3033 - acc: 0.8882 - val_loss: 0.3074 - val_acc: 0.8861\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8876 - val_loss: 0.3082 - val_acc: 0.8861\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8874 - val_loss: 0.3082 - val_acc: 0.8868\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8876 - val_loss: 0.3079 - val_acc: 0.8868\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8883 - val_loss: 0.3094 - val_acc: 0.8845\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8875 - val_loss: 0.3082 - val_acc: 0.8872\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3076 - val_acc: 0.8852\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3089 - val_acc: 0.8870\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8884 - val_loss: 0.3103 - val_acc: 0.8859\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3034 - acc: 0.8881 - val_loss: 0.3076 - val_acc: 0.8870\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8873 - val_loss: 0.3088 - val_acc: 0.8852\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3084 - val_acc: 0.8850\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8880 - val_loss: 0.3084 - val_acc: 0.8865\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8877 - val_loss: 0.3087 - val_acc: 0.8861\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8878 - val_loss: 0.3095 - val_acc: 0.8865\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8880 - val_loss: 0.3082 - val_acc: 0.8854\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8883 - val_loss: 0.3068 - val_acc: 0.8865\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3032 - acc: 0.8876 - val_loss: 0.3090 - val_acc: 0.8865\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8873 - val_loss: 0.3077 - val_acc: 0.8856\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3075 - val_acc: 0.8856\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3077 - val_acc: 0.8863\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3100 - val_acc: 0.8876\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8879 - val_loss: 0.3091 - val_acc: 0.8848\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3075 - val_acc: 0.8856\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3084 - val_acc: 0.8863\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3030 - acc: 0.8880 - val_loss: 0.3072 - val_acc: 0.8854\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8883 - val_loss: 0.3115 - val_acc: 0.8852\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8877 - val_loss: 0.3075 - val_acc: 0.8863\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8880 - val_loss: 0.3074 - val_acc: 0.8845\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3078 - val_acc: 0.8859\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3074 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8877 - val_loss: 0.3100 - val_acc: 0.8852\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8885 - val_loss: 0.3099 - val_acc: 0.8841\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3098 - val_acc: 0.8872\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3097 - val_acc: 0.8865\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8885 - val_loss: 0.3076 - val_acc: 0.8848\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8876 - val_loss: 0.3086 - val_acc: 0.8852\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8876 - val_loss: 0.3077 - val_acc: 0.8876\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8881 - val_loss: 0.3083 - val_acc: 0.8854\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3031 - acc: 0.8878 - val_loss: 0.3081 - val_acc: 0.8879\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8884 - val_loss: 0.3080 - val_acc: 0.8856\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8875 - val_loss: 0.3076 - val_acc: 0.8843\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3085 - val_acc: 0.8852\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8878 - val_loss: 0.3084 - val_acc: 0.8856\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3026 - acc: 0.8877 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8879 - val_loss: 0.3068 - val_acc: 0.8856\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8877 - val_loss: 0.3089 - val_acc: 0.8848\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8877 - val_loss: 0.3114 - val_acc: 0.8865\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8881 - val_loss: 0.3076 - val_acc: 0.8850\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8879 - val_loss: 0.3078 - val_acc: 0.8841\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3030 - acc: 0.8881 - val_loss: 0.3084 - val_acc: 0.8845\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3078 - val_acc: 0.8856\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8882 - val_loss: 0.3096 - val_acc: 0.8870\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8879 - val_loss: 0.3086 - val_acc: 0.8856\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3026 - acc: 0.8884 - val_loss: 0.3094 - val_acc: 0.8854\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3103 - val_acc: 0.8852\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8876 - val_loss: 0.3078 - val_acc: 0.8856\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8884 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3028 - acc: 0.8886 - val_loss: 0.3081 - val_acc: 0.8848\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3030 - acc: 0.8878 - val_loss: 0.3077 - val_acc: 0.8854\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3025 - acc: 0.8880 - val_loss: 0.3100 - val_acc: 0.8850\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3078 - val_acc: 0.8859\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3076 - val_acc: 0.8856\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8877 - val_loss: 0.3080 - val_acc: 0.8854\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8878 - val_loss: 0.3097 - val_acc: 0.8868\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3080 - val_acc: 0.8868\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8878 - val_loss: 0.3115 - val_acc: 0.8852\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8884 - val_loss: 0.3075 - val_acc: 0.8850\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3030 - acc: 0.8882 - val_loss: 0.3083 - val_acc: 0.8848\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8883 - val_loss: 0.3080 - val_acc: 0.8852\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3028 - acc: 0.8879 - val_loss: 0.3088 - val_acc: 0.8870\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3072 - val_acc: 0.8863\n",
            "acc: 88.63%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 24us/step - loss: 0.3452 - acc: 0.8732 - val_loss: 0.3138 - val_acc: 0.8894\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3155 - acc: 0.8851 - val_loss: 0.3109 - val_acc: 0.8881\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3136 - acc: 0.8851 - val_loss: 0.3099 - val_acc: 0.8887\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3131 - acc: 0.8849 - val_loss: 0.3108 - val_acc: 0.8887\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3122 - acc: 0.8852 - val_loss: 0.3093 - val_acc: 0.8850\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3122 - acc: 0.8841 - val_loss: 0.3105 - val_acc: 0.8828\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3119 - acc: 0.8847 - val_loss: 0.3083 - val_acc: 0.8850\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3114 - acc: 0.8849 - val_loss: 0.3091 - val_acc: 0.8859\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3110 - acc: 0.8847 - val_loss: 0.3082 - val_acc: 0.8852\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3107 - acc: 0.8851 - val_loss: 0.3087 - val_acc: 0.8856\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3108 - acc: 0.8845 - val_loss: 0.3110 - val_acc: 0.8879\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3104 - acc: 0.8845 - val_loss: 0.3099 - val_acc: 0.8845\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3105 - acc: 0.8848 - val_loss: 0.3083 - val_acc: 0.8874\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3102 - acc: 0.8853 - val_loss: 0.3097 - val_acc: 0.8861\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3103 - acc: 0.8853 - val_loss: 0.3088 - val_acc: 0.8865\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3103 - acc: 0.8850 - val_loss: 0.3087 - val_acc: 0.8874\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8856 - val_loss: 0.3098 - val_acc: 0.8868\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8852 - val_loss: 0.3096 - val_acc: 0.8861\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3100 - acc: 0.8857 - val_loss: 0.3082 - val_acc: 0.8870\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3103 - acc: 0.8854 - val_loss: 0.3082 - val_acc: 0.8863\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3098 - acc: 0.8853 - val_loss: 0.3093 - val_acc: 0.8863\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3097 - acc: 0.8852 - val_loss: 0.3088 - val_acc: 0.8854\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8855 - val_loss: 0.3087 - val_acc: 0.8856\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8846 - val_loss: 0.3090 - val_acc: 0.8865\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8852 - val_loss: 0.3096 - val_acc: 0.8856\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3094 - acc: 0.8851 - val_loss: 0.3096 - val_acc: 0.8854\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3096 - acc: 0.8859 - val_loss: 0.3097 - val_acc: 0.8865\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3096 - acc: 0.8851 - val_loss: 0.3086 - val_acc: 0.8861\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3096 - acc: 0.8856 - val_loss: 0.3091 - val_acc: 0.8859\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3096 - acc: 0.8855 - val_loss: 0.3087 - val_acc: 0.8854\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8858 - val_loss: 0.3097 - val_acc: 0.8870\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3094 - acc: 0.8853 - val_loss: 0.3101 - val_acc: 0.8837\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8852 - val_loss: 0.3096 - val_acc: 0.8859\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3094 - acc: 0.8854 - val_loss: 0.3094 - val_acc: 0.8861\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8856 - val_loss: 0.3103 - val_acc: 0.8863\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3094 - acc: 0.8853 - val_loss: 0.3108 - val_acc: 0.8856\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8857 - val_loss: 0.3095 - val_acc: 0.8865\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3092 - acc: 0.8856 - val_loss: 0.3159 - val_acc: 0.8832\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3092 - acc: 0.8852 - val_loss: 0.3097 - val_acc: 0.8868\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8857 - val_loss: 0.3099 - val_acc: 0.8861\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3093 - acc: 0.8857 - val_loss: 0.3092 - val_acc: 0.8868\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8858 - val_loss: 0.3090 - val_acc: 0.8872\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8862 - val_loss: 0.3096 - val_acc: 0.8876\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3089 - acc: 0.8858 - val_loss: 0.3123 - val_acc: 0.8854\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3089 - acc: 0.8856 - val_loss: 0.3095 - val_acc: 0.8890\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8858 - val_loss: 0.3095 - val_acc: 0.8881\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3088 - acc: 0.8857 - val_loss: 0.3096 - val_acc: 0.8861\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8852 - val_loss: 0.3094 - val_acc: 0.8881\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8860 - val_loss: 0.3088 - val_acc: 0.8879\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8863 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8862 - val_loss: 0.3109 - val_acc: 0.8852\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3086 - acc: 0.8856 - val_loss: 0.3096 - val_acc: 0.8876\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8859 - val_loss: 0.3088 - val_acc: 0.8883\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8862 - val_loss: 0.3086 - val_acc: 0.8892\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3087 - acc: 0.8858 - val_loss: 0.3090 - val_acc: 0.8876\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8857 - val_loss: 0.3084 - val_acc: 0.8890\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8862 - val_loss: 0.3095 - val_acc: 0.8883\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8861 - val_loss: 0.3094 - val_acc: 0.8859\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8857 - val_loss: 0.3087 - val_acc: 0.8881\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8861 - val_loss: 0.3081 - val_acc: 0.8874\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3080 - acc: 0.8857 - val_loss: 0.3091 - val_acc: 0.8892\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8862 - val_loss: 0.3089 - val_acc: 0.8870\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3081 - acc: 0.8862 - val_loss: 0.3093 - val_acc: 0.8885\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3084 - acc: 0.8866 - val_loss: 0.3079 - val_acc: 0.8887\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3081 - acc: 0.8858 - val_loss: 0.3089 - val_acc: 0.8901\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8856 - val_loss: 0.3078 - val_acc: 0.8892\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3081 - acc: 0.8859 - val_loss: 0.3094 - val_acc: 0.8872\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3078 - acc: 0.8857 - val_loss: 0.3078 - val_acc: 0.8890\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8859 - val_loss: 0.3081 - val_acc: 0.8887\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8853 - val_loss: 0.3085 - val_acc: 0.8890\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8864 - val_loss: 0.3089 - val_acc: 0.8890\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3079 - acc: 0.8855 - val_loss: 0.3087 - val_acc: 0.8876\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8856 - val_loss: 0.3086 - val_acc: 0.8879\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8855 - val_loss: 0.3085 - val_acc: 0.8892\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3079 - acc: 0.8861 - val_loss: 0.3086 - val_acc: 0.8883\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8855 - val_loss: 0.3084 - val_acc: 0.8876\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8852 - val_loss: 0.3086 - val_acc: 0.8879\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8861 - val_loss: 0.3095 - val_acc: 0.8887\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3077 - acc: 0.8863 - val_loss: 0.3083 - val_acc: 0.8865\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8860 - val_loss: 0.3081 - val_acc: 0.8898\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3078 - acc: 0.8859 - val_loss: 0.3075 - val_acc: 0.8879\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8858 - val_loss: 0.3084 - val_acc: 0.8892\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8857 - val_loss: 0.3090 - val_acc: 0.8885\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3079 - acc: 0.8861 - val_loss: 0.3091 - val_acc: 0.8885\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8853 - val_loss: 0.3097 - val_acc: 0.8876\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8858 - val_loss: 0.3101 - val_acc: 0.8881\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8858 - val_loss: 0.3088 - val_acc: 0.8890\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3083 - val_acc: 0.8887\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3077 - acc: 0.8864 - val_loss: 0.3085 - val_acc: 0.8883\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3077 - acc: 0.8858 - val_loss: 0.3103 - val_acc: 0.8870\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8863 - val_loss: 0.3091 - val_acc: 0.8883\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3077 - acc: 0.8861 - val_loss: 0.3086 - val_acc: 0.8883\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8867 - val_loss: 0.3076 - val_acc: 0.8881\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8862 - val_loss: 0.3112 - val_acc: 0.8837\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8864 - val_loss: 0.3085 - val_acc: 0.8901\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8860 - val_loss: 0.3088 - val_acc: 0.8881\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8858 - val_loss: 0.3088 - val_acc: 0.8859\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3076 - acc: 0.8865 - val_loss: 0.3095 - val_acc: 0.8870\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3089 - val_acc: 0.8872\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8855 - val_loss: 0.3089 - val_acc: 0.8892\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3072 - acc: 0.8868 - val_loss: 0.3086 - val_acc: 0.8887\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8860 - val_loss: 0.3083 - val_acc: 0.8881\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3076 - acc: 0.8862 - val_loss: 0.3082 - val_acc: 0.8881\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8861 - val_loss: 0.3085 - val_acc: 0.8870\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3097 - val_acc: 0.8887\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8862 - val_loss: 0.3087 - val_acc: 0.8894\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8863 - val_loss: 0.3083 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8869 - val_loss: 0.3087 - val_acc: 0.8887\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8864 - val_loss: 0.3080 - val_acc: 0.8879\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8862 - val_loss: 0.3085 - val_acc: 0.8868\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3082 - val_acc: 0.8876\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8863 - val_loss: 0.3088 - val_acc: 0.8876\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8866 - val_loss: 0.3078 - val_acc: 0.8879\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8859 - val_loss: 0.3079 - val_acc: 0.8881\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8872 - val_loss: 0.3104 - val_acc: 0.8843\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8872 - val_loss: 0.3104 - val_acc: 0.8870\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8861 - val_loss: 0.3092 - val_acc: 0.8848\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8861 - val_loss: 0.3084 - val_acc: 0.8883\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8865 - val_loss: 0.3083 - val_acc: 0.8898\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3088 - val_acc: 0.8885\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3075 - acc: 0.8863 - val_loss: 0.3091 - val_acc: 0.8874\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3072 - acc: 0.8865 - val_loss: 0.3093 - val_acc: 0.8885\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8858 - val_loss: 0.3088 - val_acc: 0.8863\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8863 - val_loss: 0.3093 - val_acc: 0.8874\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8863 - val_loss: 0.3081 - val_acc: 0.8872\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8859 - val_loss: 0.3088 - val_acc: 0.8874\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8869 - val_loss: 0.3085 - val_acc: 0.8879\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8866 - val_loss: 0.3093 - val_acc: 0.8874\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3074 - acc: 0.8864 - val_loss: 0.3084 - val_acc: 0.8879\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3072 - acc: 0.8866 - val_loss: 0.3107 - val_acc: 0.8865\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8860 - val_loss: 0.3089 - val_acc: 0.8868\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8864 - val_loss: 0.3099 - val_acc: 0.8868\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8861 - val_loss: 0.3094 - val_acc: 0.8883\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8862 - val_loss: 0.3079 - val_acc: 0.8883\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8864 - val_loss: 0.3073 - val_acc: 0.8885\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8867 - val_loss: 0.3098 - val_acc: 0.8885\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3075 - acc: 0.8861 - val_loss: 0.3082 - val_acc: 0.8868\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8869 - val_loss: 0.3073 - val_acc: 0.8883\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3073 - acc: 0.8866 - val_loss: 0.3097 - val_acc: 0.8874\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8870 - val_loss: 0.3080 - val_acc: 0.8872\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3072 - acc: 0.8866 - val_loss: 0.3090 - val_acc: 0.8874\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8870 - val_loss: 0.3094 - val_acc: 0.8879\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8862 - val_loss: 0.3092 - val_acc: 0.8892\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8865 - val_loss: 0.3084 - val_acc: 0.8876\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3073 - acc: 0.8866 - val_loss: 0.3076 - val_acc: 0.8881\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3074 - acc: 0.8862 - val_loss: 0.3078 - val_acc: 0.8890\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8870 - val_loss: 0.3099 - val_acc: 0.8887\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8858 - val_loss: 0.3081 - val_acc: 0.8883\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.3078 - val_acc: 0.8876\n",
            "acc: 88.76%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 25us/step - loss: 0.3492 - acc: 0.8691 - val_loss: 0.3230 - val_acc: 0.8863\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3139 - acc: 0.8863 - val_loss: 0.3172 - val_acc: 0.8865\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3121 - acc: 0.8863 - val_loss: 0.3172 - val_acc: 0.8852\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3113 - acc: 0.8870 - val_loss: 0.3164 - val_acc: 0.8850\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3103 - acc: 0.8868 - val_loss: 0.3163 - val_acc: 0.8854\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3097 - acc: 0.8868 - val_loss: 0.3162 - val_acc: 0.8876\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8867 - val_loss: 0.3163 - val_acc: 0.8863\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3083 - acc: 0.8863 - val_loss: 0.3141 - val_acc: 0.8859\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8882 - val_loss: 0.3147 - val_acc: 0.8841\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3077 - acc: 0.8870 - val_loss: 0.3130 - val_acc: 0.8841\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8870 - val_loss: 0.3159 - val_acc: 0.8825\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.3134 - val_acc: 0.8850\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8874 - val_loss: 0.3127 - val_acc: 0.8874\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8871 - val_loss: 0.3132 - val_acc: 0.8885\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8867 - val_loss: 0.3114 - val_acc: 0.8863\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8863 - val_loss: 0.3120 - val_acc: 0.8865\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8870 - val_loss: 0.3152 - val_acc: 0.8861\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8866 - val_loss: 0.3117 - val_acc: 0.8841\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8875 - val_loss: 0.3111 - val_acc: 0.8879\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8868 - val_loss: 0.3117 - val_acc: 0.8870\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8869 - val_loss: 0.3121 - val_acc: 0.8823\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8866 - val_loss: 0.3114 - val_acc: 0.8883\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8876 - val_loss: 0.3103 - val_acc: 0.8868\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8866 - val_loss: 0.3107 - val_acc: 0.8874\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8868 - val_loss: 0.3116 - val_acc: 0.8872\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8864 - val_loss: 0.3116 - val_acc: 0.8874\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3053 - acc: 0.8872 - val_loss: 0.3130 - val_acc: 0.8825\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8870 - val_loss: 0.3106 - val_acc: 0.8868\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8863 - val_loss: 0.3117 - val_acc: 0.8885\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3109 - val_acc: 0.8856\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8868 - val_loss: 0.3113 - val_acc: 0.8872\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3109 - val_acc: 0.8821\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8863 - val_loss: 0.3120 - val_acc: 0.8856\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8870 - val_loss: 0.3123 - val_acc: 0.8828\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8864 - val_loss: 0.3117 - val_acc: 0.8839\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8868 - val_loss: 0.3154 - val_acc: 0.8861\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8864 - val_loss: 0.3160 - val_acc: 0.8825\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8869 - val_loss: 0.3112 - val_acc: 0.8854\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8863 - val_loss: 0.3128 - val_acc: 0.8841\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8869 - val_loss: 0.3115 - val_acc: 0.8876\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3041 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8848\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3037 - acc: 0.8865 - val_loss: 0.3124 - val_acc: 0.8850\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3041 - acc: 0.8875 - val_loss: 0.3110 - val_acc: 0.8832\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8864 - val_loss: 0.3146 - val_acc: 0.8841\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8871 - val_loss: 0.3117 - val_acc: 0.8845\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8870 - val_loss: 0.3124 - val_acc: 0.8870\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8872 - val_loss: 0.3120 - val_acc: 0.8832\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8873 - val_loss: 0.3114 - val_acc: 0.8865\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3041 - acc: 0.8860 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8860 - val_loss: 0.3116 - val_acc: 0.8865\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8868 - val_loss: 0.3131 - val_acc: 0.8859\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8869 - val_loss: 0.3120 - val_acc: 0.8868\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8867 - val_loss: 0.3126 - val_acc: 0.8841\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8865 - val_loss: 0.3119 - val_acc: 0.8870\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8871 - val_loss: 0.3114 - val_acc: 0.8852\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8869 - val_loss: 0.3139 - val_acc: 0.8806\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8871 - val_loss: 0.3121 - val_acc: 0.8854\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3039 - acc: 0.8868 - val_loss: 0.3133 - val_acc: 0.8823\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3122 - val_acc: 0.8834\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8873 - val_loss: 0.3121 - val_acc: 0.8841\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8864 - val_loss: 0.3125 - val_acc: 0.8823\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8871 - val_loss: 0.3115 - val_acc: 0.8865\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3038 - acc: 0.8870 - val_loss: 0.3119 - val_acc: 0.8830\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8870 - val_loss: 0.3140 - val_acc: 0.8841\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3037 - acc: 0.8870 - val_loss: 0.3115 - val_acc: 0.8845\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8870 - val_loss: 0.3118 - val_acc: 0.8872\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8871 - val_loss: 0.3148 - val_acc: 0.8814\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8867 - val_loss: 0.3118 - val_acc: 0.8856\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3118 - val_acc: 0.8852\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8863 - val_loss: 0.3130 - val_acc: 0.8830\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8841\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8865 - val_loss: 0.3110 - val_acc: 0.8859\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8869 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8867 - val_loss: 0.3134 - val_acc: 0.8850\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8863 - val_loss: 0.3134 - val_acc: 0.8870\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8867 - val_loss: 0.3124 - val_acc: 0.8852\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8874 - val_loss: 0.3142 - val_acc: 0.8839\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8867 - val_loss: 0.3126 - val_acc: 0.8865\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8869 - val_loss: 0.3123 - val_acc: 0.8848\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8867 - val_loss: 0.3152 - val_acc: 0.8819\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8869 - val_loss: 0.3136 - val_acc: 0.8832\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8873 - val_loss: 0.3132 - val_acc: 0.8863\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8872 - val_loss: 0.3117 - val_acc: 0.8828\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3122 - val_acc: 0.8845\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8868 - val_loss: 0.3129 - val_acc: 0.8845\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8867 - val_loss: 0.3118 - val_acc: 0.8870\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8870 - val_loss: 0.3121 - val_acc: 0.8868\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8866 - val_loss: 0.3125 - val_acc: 0.8856\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8869 - val_loss: 0.3114 - val_acc: 0.8845\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8865 - val_loss: 0.3131 - val_acc: 0.8845\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8869 - val_loss: 0.3139 - val_acc: 0.8823\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8870 - val_loss: 0.3125 - val_acc: 0.8848\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8870 - val_loss: 0.3139 - val_acc: 0.8821\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8873 - val_loss: 0.3137 - val_acc: 0.8817\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3041 - acc: 0.8872 - val_loss: 0.3129 - val_acc: 0.8850\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8864 - val_loss: 0.3133 - val_acc: 0.8832\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3037 - acc: 0.8869 - val_loss: 0.3147 - val_acc: 0.8812\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3039 - acc: 0.8873 - val_loss: 0.3112 - val_acc: 0.8845\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8873 - val_loss: 0.3114 - val_acc: 0.8830\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8872 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3133 - val_acc: 0.8837\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3035 - acc: 0.8874 - val_loss: 0.3142 - val_acc: 0.8806\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8870 - val_loss: 0.3119 - val_acc: 0.8843\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8876 - val_loss: 0.3135 - val_acc: 0.8865\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3036 - acc: 0.8876 - val_loss: 0.3115 - val_acc: 0.8856\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3036 - acc: 0.8861 - val_loss: 0.3142 - val_acc: 0.8845\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8868 - val_loss: 0.3118 - val_acc: 0.8848\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3040 - acc: 0.8863 - val_loss: 0.3119 - val_acc: 0.8870\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8872 - val_loss: 0.3118 - val_acc: 0.8863\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8873 - val_loss: 0.3136 - val_acc: 0.8839\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8863 - val_loss: 0.3118 - val_acc: 0.8856\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3038 - acc: 0.8869 - val_loss: 0.3126 - val_acc: 0.8843\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8867 - val_loss: 0.3125 - val_acc: 0.8834\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8866 - val_loss: 0.3127 - val_acc: 0.8845\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3038 - acc: 0.8876 - val_loss: 0.3122 - val_acc: 0.8861\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8871 - val_loss: 0.3124 - val_acc: 0.8839\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8873 - val_loss: 0.3126 - val_acc: 0.8828\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3037 - acc: 0.8865 - val_loss: 0.3125 - val_acc: 0.8854\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8870 - val_loss: 0.3107 - val_acc: 0.8868\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8867 - val_loss: 0.3124 - val_acc: 0.8850\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8871 - val_loss: 0.3109 - val_acc: 0.8861\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3127 - val_acc: 0.8839\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8866 - val_loss: 0.3131 - val_acc: 0.8850\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3038 - acc: 0.8868 - val_loss: 0.3119 - val_acc: 0.8850\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8861 - val_loss: 0.3123 - val_acc: 0.8834\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8868 - val_loss: 0.3121 - val_acc: 0.8854\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8870 - val_loss: 0.3126 - val_acc: 0.8832\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 13us/step - loss: 0.3033 - acc: 0.8865 - val_loss: 0.3110 - val_acc: 0.8870\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8868 - val_loss: 0.3123 - val_acc: 0.8848\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3033 - acc: 0.8870 - val_loss: 0.3125 - val_acc: 0.8845\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8867 - val_loss: 0.3116 - val_acc: 0.8841\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8868 - val_loss: 0.3117 - val_acc: 0.8856\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8872 - val_loss: 0.3134 - val_acc: 0.8823\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8871 - val_loss: 0.3117 - val_acc: 0.8832\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8868 - val_loss: 0.3117 - val_acc: 0.8843\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3034 - acc: 0.8872 - val_loss: 0.3100 - val_acc: 0.8859\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8867 - val_loss: 0.3164 - val_acc: 0.8806\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8869 - val_loss: 0.3121 - val_acc: 0.8856\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8872 - val_loss: 0.3130 - val_acc: 0.8830\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8864 - val_loss: 0.3108 - val_acc: 0.8839\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8866 - val_loss: 0.3109 - val_acc: 0.8874\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8867 - val_loss: 0.3123 - val_acc: 0.8865\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8873 - val_loss: 0.3118 - val_acc: 0.8830\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3036 - acc: 0.8871 - val_loss: 0.3137 - val_acc: 0.8843\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8870 - val_loss: 0.3127 - val_acc: 0.8850\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8871 - val_loss: 0.3120 - val_acc: 0.8841\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3031 - acc: 0.8866 - val_loss: 0.3121 - val_acc: 0.8848\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8873 - val_loss: 0.3124 - val_acc: 0.8868\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3038 - acc: 0.8866 - val_loss: 0.3121 - val_acc: 0.8883\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8871 - val_loss: 0.3124 - val_acc: 0.8841\n",
            "acc: 88.41%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 26us/step - loss: 0.3435 - acc: 0.8733 - val_loss: 0.3191 - val_acc: 0.8868\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3130 - acc: 0.8852 - val_loss: 0.3145 - val_acc: 0.8859\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3112 - acc: 0.8861 - val_loss: 0.3140 - val_acc: 0.8850\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3107 - acc: 0.8856 - val_loss: 0.3127 - val_acc: 0.8832\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3102 - acc: 0.8865 - val_loss: 0.3119 - val_acc: 0.8883\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3096 - acc: 0.8868 - val_loss: 0.3124 - val_acc: 0.8845\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8873 - val_loss: 0.3107 - val_acc: 0.8868\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3087 - acc: 0.8884 - val_loss: 0.3099 - val_acc: 0.8885\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8879 - val_loss: 0.3129 - val_acc: 0.8868\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8889 - val_loss: 0.3092 - val_acc: 0.8865\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8873 - val_loss: 0.3084 - val_acc: 0.8887\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8884 - val_loss: 0.3094 - val_acc: 0.8870\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8877 - val_loss: 0.3095 - val_acc: 0.8883\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8882 - val_loss: 0.3079 - val_acc: 0.8881\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8879 - val_loss: 0.3095 - val_acc: 0.8876\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8886 - val_loss: 0.3082 - val_acc: 0.8872\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8882 - val_loss: 0.3079 - val_acc: 0.8894\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8882 - val_loss: 0.3080 - val_acc: 0.8872\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3069 - acc: 0.8884 - val_loss: 0.3074 - val_acc: 0.8887\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8884 - val_loss: 0.3080 - val_acc: 0.8872\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8878 - val_loss: 0.3085 - val_acc: 0.8879\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8887 - val_loss: 0.3113 - val_acc: 0.8854\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8887 - val_loss: 0.3077 - val_acc: 0.8868\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8880 - val_loss: 0.3083 - val_acc: 0.8850\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3066 - acc: 0.8881 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8884 - val_loss: 0.3111 - val_acc: 0.8876\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8885 - val_loss: 0.3079 - val_acc: 0.8868\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8891 - val_loss: 0.3074 - val_acc: 0.8872\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3064 - acc: 0.8886 - val_loss: 0.3073 - val_acc: 0.8879\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8888 - val_loss: 0.3081 - val_acc: 0.8872\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8889 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8884 - val_loss: 0.3089 - val_acc: 0.8872\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8881 - val_loss: 0.3067 - val_acc: 0.8874\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8885 - val_loss: 0.3066 - val_acc: 0.8885\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3064 - acc: 0.8883 - val_loss: 0.3080 - val_acc: 0.8870\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8887 - val_loss: 0.3068 - val_acc: 0.8861\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8887 - val_loss: 0.3089 - val_acc: 0.8863\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8883 - val_loss: 0.3062 - val_acc: 0.8876\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8894 - val_loss: 0.3091 - val_acc: 0.8872\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8881 - val_loss: 0.3081 - val_acc: 0.8874\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3064 - acc: 0.8882 - val_loss: 0.3078 - val_acc: 0.8870\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8883 - val_loss: 0.3087 - val_acc: 0.8874\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3064 - acc: 0.8887 - val_loss: 0.3082 - val_acc: 0.8887\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8888 - val_loss: 0.3070 - val_acc: 0.8883\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8885 - val_loss: 0.3072 - val_acc: 0.8881\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8890 - val_loss: 0.3063 - val_acc: 0.8887\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8889 - val_loss: 0.3074 - val_acc: 0.8883\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8888 - val_loss: 0.3080 - val_acc: 0.8870\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3073 - val_acc: 0.8859\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8887 - val_loss: 0.3078 - val_acc: 0.8859\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8886 - val_loss: 0.3085 - val_acc: 0.8854\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8884 - val_loss: 0.3061 - val_acc: 0.8885\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8879 - val_loss: 0.3070 - val_acc: 0.8885\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3063 - acc: 0.8887 - val_loss: 0.3066 - val_acc: 0.8883\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8882 - val_loss: 0.3062 - val_acc: 0.8863\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3063 - val_acc: 0.8885\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8883 - val_loss: 0.3079 - val_acc: 0.8883\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8881 - val_loss: 0.3069 - val_acc: 0.8868\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8881 - val_loss: 0.3057 - val_acc: 0.8883\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8884 - val_loss: 0.3053 - val_acc: 0.8887\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8892 - val_loss: 0.3072 - val_acc: 0.8885\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8887 - val_loss: 0.3057 - val_acc: 0.8876\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3062 - acc: 0.8878 - val_loss: 0.3074 - val_acc: 0.8874\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8885 - val_loss: 0.3063 - val_acc: 0.8879\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8885 - val_loss: 0.3064 - val_acc: 0.8894\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8884 - val_loss: 0.3075 - val_acc: 0.8885\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8881 - val_loss: 0.3061 - val_acc: 0.8892\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8886 - val_loss: 0.3089 - val_acc: 0.8861\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8888 - val_loss: 0.3061 - val_acc: 0.8901\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8886 - val_loss: 0.3068 - val_acc: 0.8870\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8879 - val_loss: 0.3073 - val_acc: 0.8874\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8888 - val_loss: 0.3063 - val_acc: 0.8892\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8881 - val_loss: 0.3065 - val_acc: 0.8883\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8882 - val_loss: 0.3070 - val_acc: 0.8876\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3061 - acc: 0.8885 - val_loss: 0.3069 - val_acc: 0.8876\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8882 - val_loss: 0.3070 - val_acc: 0.8879\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8879 - val_loss: 0.3058 - val_acc: 0.8874\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8890 - val_loss: 0.3072 - val_acc: 0.8863\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3061 - acc: 0.8882 - val_loss: 0.3064 - val_acc: 0.8868\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3058 - acc: 0.8879 - val_loss: 0.3063 - val_acc: 0.8881\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8879 - val_loss: 0.3069 - val_acc: 0.8881\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8886 - val_loss: 0.3063 - val_acc: 0.8883\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8885 - val_loss: 0.3061 - val_acc: 0.8890\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8876 - val_loss: 0.3078 - val_acc: 0.8879\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8886 - val_loss: 0.3065 - val_acc: 0.8883\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8877 - val_loss: 0.3061 - val_acc: 0.8896\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8886 - val_loss: 0.3083 - val_acc: 0.8879\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3076 - val_acc: 0.8898\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8890 - val_loss: 0.3065 - val_acc: 0.8883\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8888 - val_loss: 0.3063 - val_acc: 0.8896\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3057 - val_acc: 0.8894\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8885 - val_loss: 0.3077 - val_acc: 0.8863\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3067 - val_acc: 0.8879\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8887 - val_loss: 0.3079 - val_acc: 0.8885\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3055 - acc: 0.8883 - val_loss: 0.3069 - val_acc: 0.8883\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8886 - val_loss: 0.3054 - val_acc: 0.8881\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3061 - acc: 0.8884 - val_loss: 0.3062 - val_acc: 0.8887\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8879 - val_loss: 0.3067 - val_acc: 0.8881\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8883 - val_loss: 0.3064 - val_acc: 0.8903\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8885 - val_loss: 0.3064 - val_acc: 0.8876\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8881 - val_loss: 0.3063 - val_acc: 0.8894\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8881 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8880 - val_loss: 0.3058 - val_acc: 0.8896\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8884 - val_loss: 0.3060 - val_acc: 0.8916\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8885 - val_loss: 0.3066 - val_acc: 0.8876\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3058 - acc: 0.8890 - val_loss: 0.3061 - val_acc: 0.8883\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3062 - acc: 0.8882 - val_loss: 0.3056 - val_acc: 0.8907\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8889 - val_loss: 0.3061 - val_acc: 0.8887\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8883 - val_loss: 0.3069 - val_acc: 0.8876\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8888 - val_loss: 0.3058 - val_acc: 0.8879\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8888 - val_loss: 0.3063 - val_acc: 0.8896\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3063 - acc: 0.8882 - val_loss: 0.3064 - val_acc: 0.8890\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8889 - val_loss: 0.3061 - val_acc: 0.8901\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8885 - val_loss: 0.3063 - val_acc: 0.8901\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3062 - val_acc: 0.8912\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8883 - val_loss: 0.3068 - val_acc: 0.8905\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8884 - val_loss: 0.3080 - val_acc: 0.8883\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8884 - val_loss: 0.3060 - val_acc: 0.8887\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8887 - val_loss: 0.3065 - val_acc: 0.8890\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8887 - val_loss: 0.3059 - val_acc: 0.8890\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8883 - val_loss: 0.3057 - val_acc: 0.8910\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8887 - val_loss: 0.3064 - val_acc: 0.8896\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8890 - val_loss: 0.3054 - val_acc: 0.8876\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8886 - val_loss: 0.3068 - val_acc: 0.8903\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8885 - val_loss: 0.3083 - val_acc: 0.8903\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8882 - val_loss: 0.3065 - val_acc: 0.8887\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8887 - val_loss: 0.3057 - val_acc: 0.8883\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3057 - acc: 0.8883 - val_loss: 0.3063 - val_acc: 0.8907\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8884 - val_loss: 0.3065 - val_acc: 0.8898\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8883 - val_loss: 0.3055 - val_acc: 0.8898\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8880 - val_loss: 0.3057 - val_acc: 0.8898\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8881 - val_loss: 0.3063 - val_acc: 0.8887\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8884 - val_loss: 0.3071 - val_acc: 0.8890\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8889 - val_loss: 0.3057 - val_acc: 0.8881\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8887 - val_loss: 0.3069 - val_acc: 0.8898\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8888 - val_loss: 0.3076 - val_acc: 0.8874\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8881 - val_loss: 0.3053 - val_acc: 0.8883\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8888 - val_loss: 0.3069 - val_acc: 0.8885\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8884 - val_loss: 0.3068 - val_acc: 0.8903\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8884 - val_loss: 0.3069 - val_acc: 0.8879\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8885 - val_loss: 0.3073 - val_acc: 0.8883\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8879 - val_loss: 0.3055 - val_acc: 0.8905\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8882 - val_loss: 0.3052 - val_acc: 0.8887\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8882 - val_loss: 0.3063 - val_acc: 0.8892\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8884 - val_loss: 0.3062 - val_acc: 0.8898\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8882 - val_loss: 0.3077 - val_acc: 0.8881\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8879 - val_loss: 0.3059 - val_acc: 0.8894\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8883 - val_loss: 0.3072 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8884 - val_loss: 0.3071 - val_acc: 0.8898\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3059 - acc: 0.8881 - val_loss: 0.3070 - val_acc: 0.8903\n",
            "acc: 89.03%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 28us/step - loss: 0.3372 - acc: 0.8743 - val_loss: 0.3055 - val_acc: 0.8903\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3136 - acc: 0.8866 - val_loss: 0.3019 - val_acc: 0.8907\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3126 - acc: 0.8856 - val_loss: 0.3008 - val_acc: 0.8885\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3118 - acc: 0.8867 - val_loss: 0.3017 - val_acc: 0.8916\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3110 - acc: 0.8864 - val_loss: 0.3015 - val_acc: 0.8894\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3107 - acc: 0.8872 - val_loss: 0.3016 - val_acc: 0.8868\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3104 - acc: 0.8867 - val_loss: 0.3003 - val_acc: 0.8901\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3099 - acc: 0.8871 - val_loss: 0.3003 - val_acc: 0.8901\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3099 - acc: 0.8870 - val_loss: 0.2983 - val_acc: 0.8892\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3098 - acc: 0.8877 - val_loss: 0.2991 - val_acc: 0.8881\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8869 - val_loss: 0.2985 - val_acc: 0.8912\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3098 - acc: 0.8864 - val_loss: 0.3024 - val_acc: 0.8865\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3092 - acc: 0.8876 - val_loss: 0.2996 - val_acc: 0.8903\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3092 - acc: 0.8868 - val_loss: 0.2985 - val_acc: 0.8894\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3093 - acc: 0.8871 - val_loss: 0.2984 - val_acc: 0.8890\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3095 - acc: 0.8865 - val_loss: 0.3000 - val_acc: 0.8861\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3094 - acc: 0.8869 - val_loss: 0.2998 - val_acc: 0.8894\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8876 - val_loss: 0.3006 - val_acc: 0.8894\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8870 - val_loss: 0.2976 - val_acc: 0.8923\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8867 - val_loss: 0.3007 - val_acc: 0.8876\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3087 - acc: 0.8870 - val_loss: 0.2995 - val_acc: 0.8901\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3091 - acc: 0.8871 - val_loss: 0.2984 - val_acc: 0.8894\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3090 - acc: 0.8879 - val_loss: 0.2997 - val_acc: 0.8883\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8868 - val_loss: 0.2980 - val_acc: 0.8901\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3086 - acc: 0.8868 - val_loss: 0.2978 - val_acc: 0.8910\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3085 - acc: 0.8875 - val_loss: 0.2976 - val_acc: 0.8898\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8867 - val_loss: 0.2977 - val_acc: 0.8905\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8873 - val_loss: 0.2975 - val_acc: 0.8879\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8872 - val_loss: 0.2990 - val_acc: 0.8892\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8873 - val_loss: 0.2994 - val_acc: 0.8845\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8869 - val_loss: 0.2972 - val_acc: 0.8907\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8870 - val_loss: 0.2982 - val_acc: 0.8905\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8877 - val_loss: 0.2967 - val_acc: 0.8870\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8874 - val_loss: 0.2961 - val_acc: 0.8874\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3085 - acc: 0.8871 - val_loss: 0.2978 - val_acc: 0.8903\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8867 - val_loss: 0.2973 - val_acc: 0.8903\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3084 - acc: 0.8874 - val_loss: 0.2969 - val_acc: 0.8887\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3083 - acc: 0.8875 - val_loss: 0.2955 - val_acc: 0.8905\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8874 - val_loss: 0.2957 - val_acc: 0.8872\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8869 - val_loss: 0.2956 - val_acc: 0.8898\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8875 - val_loss: 0.2981 - val_acc: 0.8901\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3081 - acc: 0.8876 - val_loss: 0.2978 - val_acc: 0.8907\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8876 - val_loss: 0.2966 - val_acc: 0.8901\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3080 - acc: 0.8868 - val_loss: 0.2956 - val_acc: 0.8912\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8873 - val_loss: 0.2972 - val_acc: 0.8910\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8867 - val_loss: 0.2990 - val_acc: 0.8865\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8872 - val_loss: 0.2971 - val_acc: 0.8879\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8877 - val_loss: 0.2989 - val_acc: 0.8890\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3080 - acc: 0.8869 - val_loss: 0.2967 - val_acc: 0.8885\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8874 - val_loss: 0.2979 - val_acc: 0.8903\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8868 - val_loss: 0.2951 - val_acc: 0.8896\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3079 - acc: 0.8872 - val_loss: 0.2961 - val_acc: 0.8887\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3082 - acc: 0.8872 - val_loss: 0.2965 - val_acc: 0.8903\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8867 - val_loss: 0.2970 - val_acc: 0.8896\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8868 - val_loss: 0.2954 - val_acc: 0.8903\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8870 - val_loss: 0.2975 - val_acc: 0.8896\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8874 - val_loss: 0.2956 - val_acc: 0.8910\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.2975 - val_acc: 0.8894\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3077 - acc: 0.8878 - val_loss: 0.2996 - val_acc: 0.8872\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.2957 - val_acc: 0.8894\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8865 - val_loss: 0.2954 - val_acc: 0.8910\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8878 - val_loss: 0.2993 - val_acc: 0.8883\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8872 - val_loss: 0.2967 - val_acc: 0.8896\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8872 - val_loss: 0.2963 - val_acc: 0.8901\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3079 - acc: 0.8870 - val_loss: 0.2962 - val_acc: 0.8885\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8865 - val_loss: 0.2978 - val_acc: 0.8885\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8870 - val_loss: 0.2989 - val_acc: 0.8896\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8873 - val_loss: 0.2967 - val_acc: 0.8887\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8871 - val_loss: 0.3019 - val_acc: 0.8870\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8870 - val_loss: 0.2993 - val_acc: 0.8843\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3075 - acc: 0.8867 - val_loss: 0.2990 - val_acc: 0.8898\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3075 - acc: 0.8866 - val_loss: 0.2982 - val_acc: 0.8898\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3075 - acc: 0.8869 - val_loss: 0.2967 - val_acc: 0.8885\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8866 - val_loss: 0.2999 - val_acc: 0.8850\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3077 - acc: 0.8878 - val_loss: 0.2999 - val_acc: 0.8852\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8868 - val_loss: 0.2986 - val_acc: 0.8890\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8867 - val_loss: 0.2995 - val_acc: 0.8879\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3075 - acc: 0.8869 - val_loss: 0.2976 - val_acc: 0.8885\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8867 - val_loss: 0.2959 - val_acc: 0.8905\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8873 - val_loss: 0.2976 - val_acc: 0.8876\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8879 - val_loss: 0.2962 - val_acc: 0.8883\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3076 - acc: 0.8873 - val_loss: 0.2976 - val_acc: 0.8885\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8874 - val_loss: 0.2966 - val_acc: 0.8896\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8869 - val_loss: 0.2968 - val_acc: 0.8894\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8871 - val_loss: 0.2996 - val_acc: 0.8859\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8869 - val_loss: 0.2976 - val_acc: 0.8894\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8874 - val_loss: 0.2966 - val_acc: 0.8898\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8867 - val_loss: 0.2991 - val_acc: 0.8896\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8872 - val_loss: 0.2973 - val_acc: 0.8883\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8872 - val_loss: 0.2972 - val_acc: 0.8912\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3074 - acc: 0.8871 - val_loss: 0.2970 - val_acc: 0.8876\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8875 - val_loss: 0.2963 - val_acc: 0.8903\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8875 - val_loss: 0.2960 - val_acc: 0.8896\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8870 - val_loss: 0.2997 - val_acc: 0.8856\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3073 - acc: 0.8866 - val_loss: 0.2972 - val_acc: 0.8894\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8872 - val_loss: 0.2959 - val_acc: 0.8910\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8873 - val_loss: 0.2966 - val_acc: 0.8903\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8866 - val_loss: 0.2958 - val_acc: 0.8914\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3074 - acc: 0.8876 - val_loss: 0.2973 - val_acc: 0.8905\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8875 - val_loss: 0.2962 - val_acc: 0.8907\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8868 - val_loss: 0.2972 - val_acc: 0.8905\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8863 - val_loss: 0.2969 - val_acc: 0.8916\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8875 - val_loss: 0.2983 - val_acc: 0.8856\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8873 - val_loss: 0.2979 - val_acc: 0.8885\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3070 - acc: 0.8874 - val_loss: 0.2997 - val_acc: 0.8863\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3073 - acc: 0.8864 - val_loss: 0.2974 - val_acc: 0.8861\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3073 - acc: 0.8869 - val_loss: 0.2966 - val_acc: 0.8910\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8871 - val_loss: 0.2966 - val_acc: 0.8881\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8880 - val_loss: 0.2976 - val_acc: 0.8910\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8863 - val_loss: 0.2966 - val_acc: 0.8883\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8870 - val_loss: 0.2962 - val_acc: 0.8912\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.2965 - val_acc: 0.8892\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8880 - val_loss: 0.2958 - val_acc: 0.8907\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3069 - acc: 0.8871 - val_loss: 0.2962 - val_acc: 0.8921\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8862 - val_loss: 0.2956 - val_acc: 0.8923\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8870 - val_loss: 0.2944 - val_acc: 0.8921\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8871 - val_loss: 0.2986 - val_acc: 0.8883\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8866 - val_loss: 0.2958 - val_acc: 0.8914\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8870 - val_loss: 0.2962 - val_acc: 0.8916\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3071 - acc: 0.8862 - val_loss: 0.2976 - val_acc: 0.8894\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8872 - val_loss: 0.2974 - val_acc: 0.8856\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8864 - val_loss: 0.2955 - val_acc: 0.8894\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3067 - acc: 0.8873 - val_loss: 0.2975 - val_acc: 0.8905\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8865 - val_loss: 0.2957 - val_acc: 0.8905\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2982 - val_acc: 0.8879\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8861 - val_loss: 0.2949 - val_acc: 0.8907\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8864 - val_loss: 0.2967 - val_acc: 0.8905\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3068 - acc: 0.8866 - val_loss: 0.2973 - val_acc: 0.8903\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3068 - acc: 0.8874 - val_loss: 0.2966 - val_acc: 0.8892\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2971 - val_acc: 0.8921\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8869 - val_loss: 0.2972 - val_acc: 0.8918\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8863 - val_loss: 0.2960 - val_acc: 0.8901\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8863 - val_loss: 0.2966 - val_acc: 0.8916\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8865 - val_loss: 0.2949 - val_acc: 0.8907\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3072 - acc: 0.8860 - val_loss: 0.2959 - val_acc: 0.8916\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2972 - val_acc: 0.8910\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8872 - val_loss: 0.2979 - val_acc: 0.8885\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8870 - val_loss: 0.2971 - val_acc: 0.8923\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8864 - val_loss: 0.2949 - val_acc: 0.8918\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8866 - val_loss: 0.2963 - val_acc: 0.8923\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2941 - val_acc: 0.8910\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8865 - val_loss: 0.2989 - val_acc: 0.8850\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3068 - acc: 0.8870 - val_loss: 0.2979 - val_acc: 0.8907\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3067 - acc: 0.8870 - val_loss: 0.2956 - val_acc: 0.8912\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8875 - val_loss: 0.2996 - val_acc: 0.8859\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8864 - val_loss: 0.2974 - val_acc: 0.8879\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3069 - acc: 0.8865 - val_loss: 0.2964 - val_acc: 0.8921\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3066 - acc: 0.8873 - val_loss: 0.2963 - val_acc: 0.8905\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3065 - acc: 0.8870 - val_loss: 0.2978 - val_acc: 0.8907\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8868 - val_loss: 0.3001 - val_acc: 0.8865\n",
            "acc: 88.65%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 29us/step - loss: 0.3439 - acc: 0.8696 - val_loss: 0.3206 - val_acc: 0.8841\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3154 - acc: 0.8857 - val_loss: 0.3159 - val_acc: 0.8852\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3138 - acc: 0.8850 - val_loss: 0.3135 - val_acc: 0.8854\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3128 - acc: 0.8856 - val_loss: 0.3145 - val_acc: 0.8856\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3124 - acc: 0.8858 - val_loss: 0.3129 - val_acc: 0.8852\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3121 - acc: 0.8860 - val_loss: 0.3118 - val_acc: 0.8859\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3108 - acc: 0.8868 - val_loss: 0.3132 - val_acc: 0.8830\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3106 - acc: 0.8867 - val_loss: 0.3105 - val_acc: 0.8863\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3101 - acc: 0.8860 - val_loss: 0.3103 - val_acc: 0.8868\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3100 - acc: 0.8861 - val_loss: 0.3094 - val_acc: 0.8859\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3097 - acc: 0.8864 - val_loss: 0.3094 - val_acc: 0.8856\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3096 - acc: 0.8864 - val_loss: 0.3096 - val_acc: 0.8865\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3091 - acc: 0.8869 - val_loss: 0.3100 - val_acc: 0.8859\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3087 - acc: 0.8867 - val_loss: 0.3081 - val_acc: 0.8863\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3084 - acc: 0.8865 - val_loss: 0.3077 - val_acc: 0.8865\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8865 - val_loss: 0.3088 - val_acc: 0.8865\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8867 - val_loss: 0.3079 - val_acc: 0.8854\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8865 - val_loss: 0.3076 - val_acc: 0.8856\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3076 - acc: 0.8859 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3074 - acc: 0.8860 - val_loss: 0.3060 - val_acc: 0.8863\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3071 - acc: 0.8855 - val_loss: 0.3062 - val_acc: 0.8859\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3071 - acc: 0.8867 - val_loss: 0.3077 - val_acc: 0.8863\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3067 - acc: 0.8863 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3066 - acc: 0.8863 - val_loss: 0.3051 - val_acc: 0.8865\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8869 - val_loss: 0.3065 - val_acc: 0.8856\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8857 - val_loss: 0.3064 - val_acc: 0.8856\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3063 - acc: 0.8870 - val_loss: 0.3051 - val_acc: 0.8863\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8858 - val_loss: 0.3066 - val_acc: 0.8865\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3062 - acc: 0.8860 - val_loss: 0.3067 - val_acc: 0.8863\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8863 - val_loss: 0.3079 - val_acc: 0.8854\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8868 - val_loss: 0.3062 - val_acc: 0.8863\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8864 - val_loss: 0.3065 - val_acc: 0.8865\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8867 - val_loss: 0.3078 - val_acc: 0.8863\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8868 - val_loss: 0.3057 - val_acc: 0.8859\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8856 - val_loss: 0.3062 - val_acc: 0.8856\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8867 - val_loss: 0.3069 - val_acc: 0.8856\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8861 - val_loss: 0.3050 - val_acc: 0.8863\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8868 - val_loss: 0.3055 - val_acc: 0.8845\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3066 - val_acc: 0.8863\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3060 - acc: 0.8865 - val_loss: 0.3086 - val_acc: 0.8859\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 20us/step - loss: 0.3062 - acc: 0.8860 - val_loss: 0.3062 - val_acc: 0.8852\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3058 - acc: 0.8866 - val_loss: 0.3061 - val_acc: 0.8865\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8866 - val_loss: 0.3052 - val_acc: 0.8850\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8864 - val_loss: 0.3074 - val_acc: 0.8852\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3061 - acc: 0.8858 - val_loss: 0.3061 - val_acc: 0.8856\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8862 - val_loss: 0.3050 - val_acc: 0.8868\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8859 - val_loss: 0.3052 - val_acc: 0.8845\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8860 - val_loss: 0.3045 - val_acc: 0.8865\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3052 - val_acc: 0.8850\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8863 - val_loss: 0.3066 - val_acc: 0.8852\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8863 - val_loss: 0.3067 - val_acc: 0.8868\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8863 - val_loss: 0.3052 - val_acc: 0.8856\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8859 - val_loss: 0.3057 - val_acc: 0.8859\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8864 - val_loss: 0.3048 - val_acc: 0.8872\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8868 - val_loss: 0.3062 - val_acc: 0.8854\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8863 - val_loss: 0.3053 - val_acc: 0.8861\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8866 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8867 - val_loss: 0.3059 - val_acc: 0.8854\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8860 - val_loss: 0.3072 - val_acc: 0.8856\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8867 - val_loss: 0.3057 - val_acc: 0.8859\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8863 - val_loss: 0.3047 - val_acc: 0.8865\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8867 - val_loss: 0.3056 - val_acc: 0.8874\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8860 - val_loss: 0.3055 - val_acc: 0.8859\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8868 - val_loss: 0.3056 - val_acc: 0.8856\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8860 - val_loss: 0.3062 - val_acc: 0.8865\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8868 - val_loss: 0.3066 - val_acc: 0.8845\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8865 - val_loss: 0.3056 - val_acc: 0.8859\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3057 - acc: 0.8866 - val_loss: 0.3047 - val_acc: 0.8863\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3059 - acc: 0.8861 - val_loss: 0.3045 - val_acc: 0.8859\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3056 - acc: 0.8863 - val_loss: 0.3047 - val_acc: 0.8856\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8869 - val_loss: 0.3052 - val_acc: 0.8854\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8861 - val_loss: 0.3046 - val_acc: 0.8852\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8864 - val_loss: 0.3071 - val_acc: 0.8825\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8860 - val_loss: 0.3051 - val_acc: 0.8859\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8863 - val_loss: 0.3071 - val_acc: 0.8852\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3050 - val_acc: 0.8861\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8863 - val_loss: 0.3043 - val_acc: 0.8854\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8865 - val_loss: 0.3050 - val_acc: 0.8845\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8860 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8867 - val_loss: 0.3045 - val_acc: 0.8868\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8864 - val_loss: 0.3050 - val_acc: 0.8856\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8864 - val_loss: 0.3045 - val_acc: 0.8868\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8870 - val_loss: 0.3048 - val_acc: 0.8861\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8864 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8857 - val_loss: 0.3055 - val_acc: 0.8850\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8861 - val_loss: 0.3052 - val_acc: 0.8861\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8868 - val_loss: 0.3051 - val_acc: 0.8856\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8870 - val_loss: 0.3053 - val_acc: 0.8850\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8864 - val_loss: 0.3047 - val_acc: 0.8856\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8865 - val_loss: 0.3052 - val_acc: 0.8848\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8861 - val_loss: 0.3058 - val_acc: 0.8856\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8848\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3057 - acc: 0.8861 - val_loss: 0.3053 - val_acc: 0.8856\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8866 - val_loss: 0.3042 - val_acc: 0.8856\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3056 - acc: 0.8865 - val_loss: 0.3054 - val_acc: 0.8854\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8874 - val_loss: 0.3041 - val_acc: 0.8856\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8870 - val_loss: 0.3049 - val_acc: 0.8861\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8863 - val_loss: 0.3057 - val_acc: 0.8837\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3056 - acc: 0.8867 - val_loss: 0.3048 - val_acc: 0.8861\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3047 - val_acc: 0.8848\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8863 - val_loss: 0.3047 - val_acc: 0.8863\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8867 - val_loss: 0.3047 - val_acc: 0.8845\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3056 - val_acc: 0.8823\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8867 - val_loss: 0.3036 - val_acc: 0.8848\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3053 - acc: 0.8865 - val_loss: 0.3054 - val_acc: 0.8854\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8860 - val_loss: 0.3058 - val_acc: 0.8843\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8869 - val_loss: 0.3054 - val_acc: 0.8845\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8868 - val_loss: 0.3053 - val_acc: 0.8852\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8862 - val_loss: 0.3053 - val_acc: 0.8856\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8869 - val_loss: 0.3050 - val_acc: 0.8848\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3053 - acc: 0.8870 - val_loss: 0.3066 - val_acc: 0.8852\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8867 - val_loss: 0.3048 - val_acc: 0.8850\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8869 - val_loss: 0.3042 - val_acc: 0.8856\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8873 - val_loss: 0.3065 - val_acc: 0.8841\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8865 - val_loss: 0.3055 - val_acc: 0.8861\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8864 - val_loss: 0.3049 - val_acc: 0.8850\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8867 - val_loss: 0.3041 - val_acc: 0.8856\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8860 - val_loss: 0.3040 - val_acc: 0.8850\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8874 - val_loss: 0.3041 - val_acc: 0.8870\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8862 - val_loss: 0.3042 - val_acc: 0.8856\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8869 - val_loss: 0.3037 - val_acc: 0.8854\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8874 - val_loss: 0.3042 - val_acc: 0.8859\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8870 - val_loss: 0.3043 - val_acc: 0.8856\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8867 - val_loss: 0.3045 - val_acc: 0.8861\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8866 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8870 - val_loss: 0.3047 - val_acc: 0.8854\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3054 - acc: 0.8867 - val_loss: 0.3049 - val_acc: 0.8872\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8868 - val_loss: 0.3062 - val_acc: 0.8852\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8870 - val_loss: 0.3056 - val_acc: 0.8848\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8867 - val_loss: 0.3048 - val_acc: 0.8861\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8871 - val_loss: 0.3042 - val_acc: 0.8848\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8867 - val_loss: 0.3059 - val_acc: 0.8832\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8864 - val_loss: 0.3045 - val_acc: 0.8863\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8865 - val_loss: 0.3045 - val_acc: 0.8856\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8865 - val_loss: 0.3046 - val_acc: 0.8848\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8865 - val_loss: 0.3054 - val_acc: 0.8856\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3053 - acc: 0.8870 - val_loss: 0.3058 - val_acc: 0.8854\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3050 - val_acc: 0.8859\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8852\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8872 - val_loss: 0.3066 - val_acc: 0.8834\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3052 - acc: 0.8866 - val_loss: 0.3070 - val_acc: 0.8832\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8867 - val_loss: 0.3042 - val_acc: 0.8861\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8866 - val_loss: 0.3044 - val_acc: 0.8850\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3050 - acc: 0.8869 - val_loss: 0.3052 - val_acc: 0.8854\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8871 - val_loss: 0.3043 - val_acc: 0.8852\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3059 - val_acc: 0.8850\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3054 - acc: 0.8871 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3052 - acc: 0.8872 - val_loss: 0.3051 - val_acc: 0.8854\n",
            "acc: 88.54%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 30us/step - loss: 0.3472 - acc: 0.8702 - val_loss: 0.3128 - val_acc: 0.8845\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3139 - acc: 0.8857 - val_loss: 0.3095 - val_acc: 0.8837\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3122 - acc: 0.8863 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3115 - acc: 0.8872 - val_loss: 0.3073 - val_acc: 0.8850\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3109 - acc: 0.8867 - val_loss: 0.3081 - val_acc: 0.8843\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3105 - acc: 0.8869 - val_loss: 0.3068 - val_acc: 0.8861\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3107 - acc: 0.8861 - val_loss: 0.3060 - val_acc: 0.8850\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3104 - acc: 0.8867 - val_loss: 0.3063 - val_acc: 0.8854\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3102 - acc: 0.8867 - val_loss: 0.3075 - val_acc: 0.8852\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3104 - acc: 0.8872 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3097 - acc: 0.8873 - val_loss: 0.3075 - val_acc: 0.8848\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3102 - acc: 0.8870 - val_loss: 0.3049 - val_acc: 0.8843\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3096 - acc: 0.8874 - val_loss: 0.3051 - val_acc: 0.8865\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3095 - acc: 0.8882 - val_loss: 0.3062 - val_acc: 0.8845\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3097 - acc: 0.8869 - val_loss: 0.3062 - val_acc: 0.8852\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3095 - acc: 0.8876 - val_loss: 0.3044 - val_acc: 0.8852\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3092 - acc: 0.8882 - val_loss: 0.3059 - val_acc: 0.8852\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3093 - acc: 0.8878 - val_loss: 0.3044 - val_acc: 0.8859\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3088 - acc: 0.8874 - val_loss: 0.3057 - val_acc: 0.8863\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3089 - acc: 0.8873 - val_loss: 0.3045 - val_acc: 0.8859\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3087 - acc: 0.8876 - val_loss: 0.3049 - val_acc: 0.8854\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3083 - acc: 0.8880 - val_loss: 0.3075 - val_acc: 0.8839\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3082 - acc: 0.8878 - val_loss: 0.3057 - val_acc: 0.8861\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3081 - acc: 0.8876 - val_loss: 0.3042 - val_acc: 0.8854\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8873 - val_loss: 0.3049 - val_acc: 0.8861\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8876 - val_loss: 0.3034 - val_acc: 0.8865\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3077 - acc: 0.8880 - val_loss: 0.3033 - val_acc: 0.8861\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3074 - acc: 0.8876 - val_loss: 0.3034 - val_acc: 0.8854\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3072 - acc: 0.8877 - val_loss: 0.3031 - val_acc: 0.8868\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8877 - val_loss: 0.3036 - val_acc: 0.8859\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3073 - acc: 0.8878 - val_loss: 0.3028 - val_acc: 0.8868\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8873 - val_loss: 0.3024 - val_acc: 0.8868\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3069 - acc: 0.8872 - val_loss: 0.3028 - val_acc: 0.8870\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8875 - val_loss: 0.3033 - val_acc: 0.8865\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8876 - val_loss: 0.3036 - val_acc: 0.8848\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3069 - acc: 0.8874 - val_loss: 0.3025 - val_acc: 0.8872\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3068 - acc: 0.8878 - val_loss: 0.3035 - val_acc: 0.8870\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3067 - acc: 0.8878 - val_loss: 0.3033 - val_acc: 0.8868\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8876 - val_loss: 0.3038 - val_acc: 0.8859\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3066 - acc: 0.8879 - val_loss: 0.3034 - val_acc: 0.8874\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3065 - acc: 0.8880 - val_loss: 0.3025 - val_acc: 0.8885\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3064 - acc: 0.8877 - val_loss: 0.3028 - val_acc: 0.8865\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3065 - acc: 0.8875 - val_loss: 0.3016 - val_acc: 0.8876\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3062 - acc: 0.8884 - val_loss: 0.3031 - val_acc: 0.8868\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3061 - acc: 0.8877 - val_loss: 0.3021 - val_acc: 0.8874\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8880 - val_loss: 0.3015 - val_acc: 0.8870\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3061 - acc: 0.8879 - val_loss: 0.3009 - val_acc: 0.8879\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3063 - acc: 0.8876 - val_loss: 0.3027 - val_acc: 0.8861\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8876 - val_loss: 0.3016 - val_acc: 0.8870\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3060 - acc: 0.8877 - val_loss: 0.3019 - val_acc: 0.8870\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8881 - val_loss: 0.3011 - val_acc: 0.8874\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3059 - acc: 0.8876 - val_loss: 0.3051 - val_acc: 0.8868\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8877 - val_loss: 0.3028 - val_acc: 0.8870\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3057 - acc: 0.8879 - val_loss: 0.3019 - val_acc: 0.8872\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8877 - val_loss: 0.3022 - val_acc: 0.8868\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3056 - acc: 0.8878 - val_loss: 0.3015 - val_acc: 0.8881\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3055 - acc: 0.8880 - val_loss: 0.3023 - val_acc: 0.8881\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8876 - val_loss: 0.3037 - val_acc: 0.8859\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8873 - val_loss: 0.3033 - val_acc: 0.8854\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3039 - val_acc: 0.8876\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3015 - val_acc: 0.8874\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8875 - val_loss: 0.3036 - val_acc: 0.8865\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8871 - val_loss: 0.3016 - val_acc: 0.8881\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3054 - acc: 0.8873 - val_loss: 0.3038 - val_acc: 0.8874\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8872 - val_loss: 0.3020 - val_acc: 0.8890\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3050 - acc: 0.8875 - val_loss: 0.3012 - val_acc: 0.8870\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3050 - acc: 0.8876 - val_loss: 0.3028 - val_acc: 0.8885\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3052 - acc: 0.8877 - val_loss: 0.3008 - val_acc: 0.8879\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3052 - acc: 0.8875 - val_loss: 0.3023 - val_acc: 0.8879\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8871 - val_loss: 0.3015 - val_acc: 0.8883\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3051 - acc: 0.8876 - val_loss: 0.3035 - val_acc: 0.8874\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8875 - val_loss: 0.3053 - val_acc: 0.8872\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8877 - val_loss: 0.3026 - val_acc: 0.8885\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3051 - acc: 0.8879 - val_loss: 0.3016 - val_acc: 0.8887\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8876 - val_loss: 0.3026 - val_acc: 0.8868\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3010 - val_acc: 0.8896\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3029 - val_acc: 0.8861\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3049 - acc: 0.8875 - val_loss: 0.3022 - val_acc: 0.8883\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8873 - val_loss: 0.3021 - val_acc: 0.8887\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8876 - val_loss: 0.3008 - val_acc: 0.8887\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3053 - acc: 0.8876 - val_loss: 0.3015 - val_acc: 0.8890\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3049 - acc: 0.8877 - val_loss: 0.3028 - val_acc: 0.8883\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8874 - val_loss: 0.3021 - val_acc: 0.8885\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8865 - val_loss: 0.3023 - val_acc: 0.8887\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3037 - val_acc: 0.8870\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8873 - val_loss: 0.3017 - val_acc: 0.8885\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8872 - val_loss: 0.3009 - val_acc: 0.8883\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3049 - acc: 0.8877 - val_loss: 0.3024 - val_acc: 0.8876\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3049 - acc: 0.8874 - val_loss: 0.3025 - val_acc: 0.8896\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3050 - acc: 0.8878 - val_loss: 0.3018 - val_acc: 0.8881\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3021 - val_acc: 0.8883\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3018 - val_acc: 0.8885\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8878 - val_loss: 0.3024 - val_acc: 0.8885\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3013 - val_acc: 0.8883\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3026 - val_acc: 0.8859\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3018 - val_acc: 0.8876\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8874 - val_loss: 0.3021 - val_acc: 0.8885\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8874 - val_loss: 0.3010 - val_acc: 0.8887\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8880 - val_loss: 0.3011 - val_acc: 0.8885\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8875 - val_loss: 0.3021 - val_acc: 0.8852\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8878 - val_loss: 0.3038 - val_acc: 0.8865\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8874 - val_loss: 0.3027 - val_acc: 0.8881\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3046 - acc: 0.8874 - val_loss: 0.3027 - val_acc: 0.8868\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3044 - acc: 0.8872 - val_loss: 0.3035 - val_acc: 0.8870\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8877 - val_loss: 0.3015 - val_acc: 0.8879\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3045 - acc: 0.8875 - val_loss: 0.3038 - val_acc: 0.8865\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8879 - val_loss: 0.3043 - val_acc: 0.8859\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3020 - val_acc: 0.8894\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3029 - val_acc: 0.8870\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8873 - val_loss: 0.3012 - val_acc: 0.8879\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8877 - val_loss: 0.3018 - val_acc: 0.8876\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8871 - val_loss: 0.3016 - val_acc: 0.8892\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3013 - val_acc: 0.8885\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3015 - val_acc: 0.8885\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8871 - val_loss: 0.3022 - val_acc: 0.8896\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8870 - val_loss: 0.3012 - val_acc: 0.8890\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8872 - val_loss: 0.3028 - val_acc: 0.8868\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8874 - val_loss: 0.3019 - val_acc: 0.8885\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8870 - val_loss: 0.3018 - val_acc: 0.8885\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8876 - val_loss: 0.3019 - val_acc: 0.8887\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8875 - val_loss: 0.3017 - val_acc: 0.8883\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8877 - val_loss: 0.3034 - val_acc: 0.8879\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8877 - val_loss: 0.3026 - val_acc: 0.8887\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8876 - val_loss: 0.3044 - val_acc: 0.8850\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8874 - val_loss: 0.3025 - val_acc: 0.8876\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8878 - val_loss: 0.3026 - val_acc: 0.8887\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8879 - val_loss: 0.3017 - val_acc: 0.8879\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8878 - val_loss: 0.3025 - val_acc: 0.8876\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8873 - val_loss: 0.3024 - val_acc: 0.8892\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8873 - val_loss: 0.3027 - val_acc: 0.8883\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3045 - acc: 0.8875 - val_loss: 0.3012 - val_acc: 0.8876\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3048 - acc: 0.8873 - val_loss: 0.3032 - val_acc: 0.8887\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8877 - val_loss: 0.3018 - val_acc: 0.8865\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8872 - val_loss: 0.3032 - val_acc: 0.8874\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8875 - val_loss: 0.3027 - val_acc: 0.8890\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3047 - acc: 0.8869 - val_loss: 0.3014 - val_acc: 0.8890\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.8875 - val_loss: 0.3038 - val_acc: 0.8876\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3046 - acc: 0.8873 - val_loss: 0.3021 - val_acc: 0.8881\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8866 - val_loss: 0.3025 - val_acc: 0.8881\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3042 - acc: 0.8873 - val_loss: 0.3026 - val_acc: 0.8876\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8874 - val_loss: 0.3030 - val_acc: 0.8870\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3047 - acc: 0.8876 - val_loss: 0.3021 - val_acc: 0.8881\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3050 - acc: 0.8873 - val_loss: 0.3016 - val_acc: 0.8885\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8872 - val_loss: 0.3026 - val_acc: 0.8883\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3047 - acc: 0.8878 - val_loss: 0.3016 - val_acc: 0.8890\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8878 - val_loss: 0.3022 - val_acc: 0.8885\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3045 - acc: 0.8876 - val_loss: 0.3016 - val_acc: 0.8894\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8873 - val_loss: 0.3023 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8870 - val_loss: 0.3013 - val_acc: 0.8887\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8871 - val_loss: 0.3014 - val_acc: 0.8901\n",
            "acc: 89.01%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 32us/step - loss: 0.3414 - acc: 0.8748 - val_loss: 0.3104 - val_acc: 0.8872\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3132 - acc: 0.8856 - val_loss: 0.3075 - val_acc: 0.8850\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3101 - acc: 0.8870 - val_loss: 0.3073 - val_acc: 0.8845\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3088 - acc: 0.8863 - val_loss: 0.3089 - val_acc: 0.8832\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3088 - acc: 0.8866 - val_loss: 0.3066 - val_acc: 0.8856\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3082 - acc: 0.8866 - val_loss: 0.3056 - val_acc: 0.8852\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.3102 - val_acc: 0.8861\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3080 - acc: 0.8872 - val_loss: 0.3071 - val_acc: 0.8854\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3078 - acc: 0.8869 - val_loss: 0.3065 - val_acc: 0.8845\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3071 - acc: 0.8874 - val_loss: 0.3068 - val_acc: 0.8843\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3070 - acc: 0.8874 - val_loss: 0.3075 - val_acc: 0.8819\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3070 - acc: 0.8869 - val_loss: 0.3045 - val_acc: 0.8850\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3066 - acc: 0.8872 - val_loss: 0.3050 - val_acc: 0.8859\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3067 - acc: 0.8871 - val_loss: 0.3069 - val_acc: 0.8876\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3065 - acc: 0.8863 - val_loss: 0.3050 - val_acc: 0.8868\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3063 - acc: 0.8867 - val_loss: 0.3045 - val_acc: 0.8881\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3060 - acc: 0.8868 - val_loss: 0.3047 - val_acc: 0.8808\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3058 - acc: 0.8864 - val_loss: 0.3048 - val_acc: 0.8850\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3058 - acc: 0.8872 - val_loss: 0.3045 - val_acc: 0.8868\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3056 - acc: 0.8864 - val_loss: 0.3031 - val_acc: 0.8861\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3052 - acc: 0.8876 - val_loss: 0.3022 - val_acc: 0.8879\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3053 - acc: 0.8874 - val_loss: 0.3018 - val_acc: 0.8856\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8877 - val_loss: 0.3026 - val_acc: 0.8863\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8881 - val_loss: 0.3025 - val_acc: 0.8848\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3048 - acc: 0.8881 - val_loss: 0.3027 - val_acc: 0.8874\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8880 - val_loss: 0.3026 - val_acc: 0.8845\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3044 - acc: 0.8875 - val_loss: 0.3021 - val_acc: 0.8870\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3046 - acc: 0.8881 - val_loss: 0.3023 - val_acc: 0.8856\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3043 - acc: 0.8878 - val_loss: 0.3013 - val_acc: 0.8874\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3040 - acc: 0.8879 - val_loss: 0.3070 - val_acc: 0.8856\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3044 - acc: 0.8879 - val_loss: 0.3011 - val_acc: 0.8863\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3040 - acc: 0.8884 - val_loss: 0.3017 - val_acc: 0.8870\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3040 - acc: 0.8887 - val_loss: 0.3042 - val_acc: 0.8852\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3038 - acc: 0.8881 - val_loss: 0.3010 - val_acc: 0.8870\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8879 - val_loss: 0.3030 - val_acc: 0.8854\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8876 - val_loss: 0.3019 - val_acc: 0.8859\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3038 - acc: 0.8885 - val_loss: 0.3005 - val_acc: 0.8859\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8883 - val_loss: 0.3035 - val_acc: 0.8850\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8879 - val_loss: 0.2999 - val_acc: 0.8865\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3037 - acc: 0.8883 - val_loss: 0.3011 - val_acc: 0.8854\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3036 - acc: 0.8879 - val_loss: 0.3012 - val_acc: 0.8885\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3037 - acc: 0.8882 - val_loss: 0.3016 - val_acc: 0.8865\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3035 - acc: 0.8878 - val_loss: 0.3017 - val_acc: 0.8870\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3035 - acc: 0.8875 - val_loss: 0.3003 - val_acc: 0.8879\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3033 - acc: 0.8879 - val_loss: 0.3007 - val_acc: 0.8852\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8886 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3035 - acc: 0.8882 - val_loss: 0.3008 - val_acc: 0.8859\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3035 - acc: 0.8884 - val_loss: 0.3013 - val_acc: 0.8870\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8879 - val_loss: 0.3003 - val_acc: 0.8868\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8883 - val_loss: 0.3018 - val_acc: 0.8874\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8877 - val_loss: 0.3002 - val_acc: 0.8879\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3030 - acc: 0.8881 - val_loss: 0.3008 - val_acc: 0.8874\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3030 - acc: 0.8886 - val_loss: 0.3009 - val_acc: 0.8814\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3034 - acc: 0.8879 - val_loss: 0.3015 - val_acc: 0.8865\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3009 - val_acc: 0.8850\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8880 - val_loss: 0.2993 - val_acc: 0.8872\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8882 - val_loss: 0.3008 - val_acc: 0.8865\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3002 - val_acc: 0.8883\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3024 - acc: 0.8883 - val_loss: 0.3035 - val_acc: 0.8885\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3025 - acc: 0.8881 - val_loss: 0.3021 - val_acc: 0.8856\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8873 - val_loss: 0.3011 - val_acc: 0.8861\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8880 - val_loss: 0.3009 - val_acc: 0.8832\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3029 - acc: 0.8891 - val_loss: 0.3001 - val_acc: 0.8852\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3032 - acc: 0.8880 - val_loss: 0.3000 - val_acc: 0.8879\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8880 - val_loss: 0.2998 - val_acc: 0.8876\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3027 - acc: 0.8880 - val_loss: 0.3002 - val_acc: 0.8863\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3028 - acc: 0.8881 - val_loss: 0.3021 - val_acc: 0.8874\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3028 - acc: 0.8883 - val_loss: 0.3013 - val_acc: 0.8874\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3026 - acc: 0.8885 - val_loss: 0.3009 - val_acc: 0.8874\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3026 - acc: 0.8881 - val_loss: 0.3014 - val_acc: 0.8881\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3024 - acc: 0.8884 - val_loss: 0.3006 - val_acc: 0.8868\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3024 - acc: 0.8879 - val_loss: 0.3020 - val_acc: 0.8872\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3027 - acc: 0.8876 - val_loss: 0.3016 - val_acc: 0.8870\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3025 - acc: 0.8880 - val_loss: 0.3014 - val_acc: 0.8885\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3025 - acc: 0.8882 - val_loss: 0.3007 - val_acc: 0.8887\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8889 - val_loss: 0.3010 - val_acc: 0.8870\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8882 - val_loss: 0.3007 - val_acc: 0.8859\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8886 - val_loss: 0.3013 - val_acc: 0.8865\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3023 - acc: 0.8881 - val_loss: 0.3007 - val_acc: 0.8876\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3024 - acc: 0.8879 - val_loss: 0.3007 - val_acc: 0.8896\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8888 - val_loss: 0.3012 - val_acc: 0.8859\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3024 - acc: 0.8881 - val_loss: 0.3027 - val_acc: 0.8881\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8883 - val_loss: 0.3023 - val_acc: 0.8870\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8885 - val_loss: 0.3017 - val_acc: 0.8879\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3019 - acc: 0.8882 - val_loss: 0.3031 - val_acc: 0.8870\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3008 - val_acc: 0.8865\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3022 - acc: 0.8884 - val_loss: 0.3004 - val_acc: 0.8872\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3006 - val_acc: 0.8870\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3022 - acc: 0.8883 - val_loss: 0.3011 - val_acc: 0.8881\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8884 - val_loss: 0.3009 - val_acc: 0.8879\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3022 - acc: 0.8880 - val_loss: 0.3002 - val_acc: 0.8876\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8886 - val_loss: 0.3015 - val_acc: 0.8863\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8883 - val_loss: 0.3008 - val_acc: 0.8874\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8884 - val_loss: 0.3021 - val_acc: 0.8881\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3015 - val_acc: 0.8885\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3023 - acc: 0.8879 - val_loss: 0.3019 - val_acc: 0.8870\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3022 - acc: 0.8878 - val_loss: 0.3022 - val_acc: 0.8868\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8879 - val_loss: 0.3017 - val_acc: 0.8876\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8884 - val_loss: 0.3021 - val_acc: 0.8883\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8881 - val_loss: 0.3016 - val_acc: 0.8885\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8877 - val_loss: 0.3012 - val_acc: 0.8883\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8882 - val_loss: 0.3012 - val_acc: 0.8892\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3020 - acc: 0.8884 - val_loss: 0.3009 - val_acc: 0.8887\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3020 - acc: 0.8882 - val_loss: 0.3010 - val_acc: 0.8883\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3020 - acc: 0.8881 - val_loss: 0.3006 - val_acc: 0.8885\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8878 - val_loss: 0.3023 - val_acc: 0.8883\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8883 - val_loss: 0.3025 - val_acc: 0.8863\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3019 - acc: 0.8881 - val_loss: 0.3019 - val_acc: 0.8881\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8888 - val_loss: 0.3027 - val_acc: 0.8848\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8881 - val_loss: 0.3013 - val_acc: 0.8872\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8880 - val_loss: 0.3011 - val_acc: 0.8876\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3014 - val_acc: 0.8879\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3019 - acc: 0.8884 - val_loss: 0.3017 - val_acc: 0.8879\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8888 - val_loss: 0.3009 - val_acc: 0.8876\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8884 - val_loss: 0.3005 - val_acc: 0.8887\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3033 - val_acc: 0.8863\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8884 - val_loss: 0.3012 - val_acc: 0.8874\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3016 - val_acc: 0.8879\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8886 - val_loss: 0.3027 - val_acc: 0.8887\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3022 - acc: 0.8884 - val_loss: 0.3024 - val_acc: 0.8874\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3019 - acc: 0.8883 - val_loss: 0.3037 - val_acc: 0.8863\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8884 - val_loss: 0.3013 - val_acc: 0.8883\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8878 - val_loss: 0.3023 - val_acc: 0.8870\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8882 - val_loss: 0.3011 - val_acc: 0.8872\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8880 - val_loss: 0.3019 - val_acc: 0.8883\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8883 - val_loss: 0.3025 - val_acc: 0.8861\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3017 - acc: 0.8886 - val_loss: 0.3026 - val_acc: 0.8881\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8883 - val_loss: 0.3013 - val_acc: 0.8890\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8887 - val_loss: 0.3026 - val_acc: 0.8876\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3019 - acc: 0.8885 - val_loss: 0.3027 - val_acc: 0.8863\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3019 - acc: 0.8876 - val_loss: 0.3024 - val_acc: 0.8872\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3007 - val_acc: 0.8879\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8885 - val_loss: 0.3011 - val_acc: 0.8879\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8880 - val_loss: 0.3025 - val_acc: 0.8872\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8887 - val_loss: 0.3015 - val_acc: 0.8876\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3021 - acc: 0.8880 - val_loss: 0.3015 - val_acc: 0.8874\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3018 - acc: 0.8885 - val_loss: 0.3017 - val_acc: 0.8870\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8880 - val_loss: 0.3018 - val_acc: 0.8870\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3018 - acc: 0.8881 - val_loss: 0.3016 - val_acc: 0.8868\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8881 - val_loss: 0.3020 - val_acc: 0.8881\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3020 - acc: 0.8880 - val_loss: 0.2997 - val_acc: 0.8887\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8889 - val_loss: 0.3025 - val_acc: 0.8876\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3019 - acc: 0.8884 - val_loss: 0.3020 - val_acc: 0.8861\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3019 - acc: 0.8879 - val_loss: 0.3034 - val_acc: 0.8863\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3020 - acc: 0.8886 - val_loss: 0.3025 - val_acc: 0.8861\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3017 - acc: 0.8884 - val_loss: 0.3046 - val_acc: 0.8885\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8887 - val_loss: 0.3024 - val_acc: 0.8890\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8881 - val_loss: 0.3013 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3015 - acc: 0.8881 - val_loss: 0.3014 - val_acc: 0.8890\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8877 - val_loss: 0.3013 - val_acc: 0.8874\n",
            "acc: 88.74%\n",
            "Train on 40690 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "40690/40690 [==============================] - 1s 34us/step - loss: 0.3421 - acc: 0.8717 - val_loss: 0.3224 - val_acc: 0.8856\n",
            "Epoch 2/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3124 - acc: 0.8848 - val_loss: 0.3146 - val_acc: 0.8856\n",
            "Epoch 3/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3099 - acc: 0.8857 - val_loss: 0.3132 - val_acc: 0.8874\n",
            "Epoch 4/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3079 - acc: 0.8855 - val_loss: 0.3122 - val_acc: 0.8848\n",
            "Epoch 5/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3064 - acc: 0.8851 - val_loss: 0.3110 - val_acc: 0.8828\n",
            "Epoch 6/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3055 - acc: 0.8856 - val_loss: 0.3087 - val_acc: 0.8841\n",
            "Epoch 7/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8846 - val_loss: 0.3093 - val_acc: 0.8859\n",
            "Epoch 8/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3039 - acc: 0.8849 - val_loss: 0.3080 - val_acc: 0.8834\n",
            "Epoch 9/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3031 - acc: 0.8851 - val_loss: 0.3095 - val_acc: 0.8856\n",
            "Epoch 10/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3031 - acc: 0.8851 - val_loss: 0.3066 - val_acc: 0.8865\n",
            "Epoch 11/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3028 - acc: 0.8853 - val_loss: 0.3077 - val_acc: 0.8879\n",
            "Epoch 12/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3023 - acc: 0.8846 - val_loss: 0.3104 - val_acc: 0.8850\n",
            "Epoch 13/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3023 - acc: 0.8846 - val_loss: 0.3057 - val_acc: 0.8856\n",
            "Epoch 14/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3015 - acc: 0.8853 - val_loss: 0.3069 - val_acc: 0.8859\n",
            "Epoch 15/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3012 - acc: 0.8853 - val_loss: 0.3073 - val_acc: 0.8868\n",
            "Epoch 16/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3016 - acc: 0.8846 - val_loss: 0.3052 - val_acc: 0.8876\n",
            "Epoch 17/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3013 - acc: 0.8847 - val_loss: 0.3060 - val_acc: 0.8892\n",
            "Epoch 18/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3009 - acc: 0.8853 - val_loss: 0.3076 - val_acc: 0.8852\n",
            "Epoch 19/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3007 - acc: 0.8845 - val_loss: 0.3062 - val_acc: 0.8861\n",
            "Epoch 20/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3009 - acc: 0.8844 - val_loss: 0.3068 - val_acc: 0.8865\n",
            "Epoch 21/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3007 - acc: 0.8844 - val_loss: 0.3071 - val_acc: 0.8894\n",
            "Epoch 22/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3006 - acc: 0.8853 - val_loss: 0.3065 - val_acc: 0.8845\n",
            "Epoch 23/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.3005 - acc: 0.8846 - val_loss: 0.3079 - val_acc: 0.8870\n",
            "Epoch 24/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3007 - acc: 0.8852 - val_loss: 0.3061 - val_acc: 0.8870\n",
            "Epoch 25/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.3003 - acc: 0.8851 - val_loss: 0.3050 - val_acc: 0.8863\n",
            "Epoch 26/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8853 - val_loss: 0.3060 - val_acc: 0.8890\n",
            "Epoch 27/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.3004 - acc: 0.8851 - val_loss: 0.3078 - val_acc: 0.8885\n",
            "Epoch 28/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3004 - acc: 0.8849 - val_loss: 0.3072 - val_acc: 0.8850\n",
            "Epoch 29/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8845 - val_loss: 0.3061 - val_acc: 0.8843\n",
            "Epoch 30/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3005 - acc: 0.8848 - val_loss: 0.3051 - val_acc: 0.8854\n",
            "Epoch 31/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3001 - acc: 0.8839 - val_loss: 0.3063 - val_acc: 0.8885\n",
            "Epoch 32/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8848 - val_loss: 0.3055 - val_acc: 0.8865\n",
            "Epoch 33/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3003 - acc: 0.8843 - val_loss: 0.3066 - val_acc: 0.8868\n",
            "Epoch 34/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3000 - acc: 0.8853 - val_loss: 0.3058 - val_acc: 0.8870\n",
            "Epoch 35/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.3002 - acc: 0.8841 - val_loss: 0.3074 - val_acc: 0.8863\n",
            "Epoch 36/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3000 - acc: 0.8843 - val_loss: 0.3053 - val_acc: 0.8892\n",
            "Epoch 37/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8850 - val_loss: 0.3053 - val_acc: 0.8868\n",
            "Epoch 38/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2998 - acc: 0.8846 - val_loss: 0.3045 - val_acc: 0.8872\n",
            "Epoch 39/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2998 - acc: 0.8848 - val_loss: 0.3062 - val_acc: 0.8850\n",
            "Epoch 40/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2999 - acc: 0.8856 - val_loss: 0.3055 - val_acc: 0.8865\n",
            "Epoch 41/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2998 - acc: 0.8852 - val_loss: 0.3053 - val_acc: 0.8852\n",
            "Epoch 42/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8853 - val_loss: 0.3076 - val_acc: 0.8881\n",
            "Epoch 43/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2998 - acc: 0.8852 - val_loss: 0.3060 - val_acc: 0.8894\n",
            "Epoch 44/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2995 - acc: 0.8851 - val_loss: 0.3047 - val_acc: 0.8868\n",
            "Epoch 45/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2996 - acc: 0.8852 - val_loss: 0.3062 - val_acc: 0.8881\n",
            "Epoch 46/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8847 - val_loss: 0.3060 - val_acc: 0.8856\n",
            "Epoch 47/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8847 - val_loss: 0.3046 - val_acc: 0.8865\n",
            "Epoch 48/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8850 - val_loss: 0.3071 - val_acc: 0.8896\n",
            "Epoch 49/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8849 - val_loss: 0.3063 - val_acc: 0.8837\n",
            "Epoch 50/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2995 - acc: 0.8855 - val_loss: 0.3067 - val_acc: 0.8850\n",
            "Epoch 51/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2994 - acc: 0.8854 - val_loss: 0.3070 - val_acc: 0.8890\n",
            "Epoch 52/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2995 - acc: 0.8852 - val_loss: 0.3064 - val_acc: 0.8854\n",
            "Epoch 53/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2995 - acc: 0.8854 - val_loss: 0.3064 - val_acc: 0.8876\n",
            "Epoch 54/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2994 - acc: 0.8851 - val_loss: 0.3052 - val_acc: 0.8887\n",
            "Epoch 55/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8848 - val_loss: 0.3068 - val_acc: 0.8859\n",
            "Epoch 56/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8858 - val_loss: 0.3055 - val_acc: 0.8852\n",
            "Epoch 57/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2991 - acc: 0.8857 - val_loss: 0.3066 - val_acc: 0.8885\n",
            "Epoch 58/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2994 - acc: 0.8855 - val_loss: 0.3060 - val_acc: 0.8868\n",
            "Epoch 59/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8846 - val_loss: 0.3042 - val_acc: 0.8859\n",
            "Epoch 60/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8859 - val_loss: 0.3055 - val_acc: 0.8874\n",
            "Epoch 61/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2990 - acc: 0.8856 - val_loss: 0.3067 - val_acc: 0.8863\n",
            "Epoch 62/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2991 - acc: 0.8853 - val_loss: 0.3064 - val_acc: 0.8872\n",
            "Epoch 63/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.2995 - acc: 0.8852 - val_loss: 0.3052 - val_acc: 0.8861\n",
            "Epoch 64/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8852 - val_loss: 0.3042 - val_acc: 0.8870\n",
            "Epoch 65/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8852 - val_loss: 0.3051 - val_acc: 0.8879\n",
            "Epoch 66/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2991 - acc: 0.8847 - val_loss: 0.3050 - val_acc: 0.8850\n",
            "Epoch 67/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8852 - val_loss: 0.3074 - val_acc: 0.8868\n",
            "Epoch 68/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8852 - val_loss: 0.3054 - val_acc: 0.8852\n",
            "Epoch 69/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2989 - acc: 0.8861 - val_loss: 0.3063 - val_acc: 0.8881\n",
            "Epoch 70/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2990 - acc: 0.8850 - val_loss: 0.3075 - val_acc: 0.8856\n",
            "Epoch 71/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2993 - acc: 0.8849 - val_loss: 0.3056 - val_acc: 0.8870\n",
            "Epoch 72/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8854 - val_loss: 0.3076 - val_acc: 0.8870\n",
            "Epoch 73/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2990 - acc: 0.8853 - val_loss: 0.3057 - val_acc: 0.8870\n",
            "Epoch 74/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2987 - acc: 0.8860 - val_loss: 0.3066 - val_acc: 0.8859\n",
            "Epoch 75/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3064 - val_acc: 0.8861\n",
            "Epoch 76/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2989 - acc: 0.8854 - val_loss: 0.3061 - val_acc: 0.8859\n",
            "Epoch 77/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8856 - val_loss: 0.3062 - val_acc: 0.8863\n",
            "Epoch 78/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2993 - acc: 0.8857 - val_loss: 0.3072 - val_acc: 0.8865\n",
            "Epoch 79/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2989 - acc: 0.8851 - val_loss: 0.3048 - val_acc: 0.8854\n",
            "Epoch 80/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2987 - acc: 0.8853 - val_loss: 0.3075 - val_acc: 0.8854\n",
            "Epoch 81/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2988 - acc: 0.8853 - val_loss: 0.3076 - val_acc: 0.8863\n",
            "Epoch 82/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8846 - val_loss: 0.3071 - val_acc: 0.8865\n",
            "Epoch 83/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2986 - acc: 0.8857 - val_loss: 0.3055 - val_acc: 0.8881\n",
            "Epoch 84/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2992 - acc: 0.8843 - val_loss: 0.3054 - val_acc: 0.8874\n",
            "Epoch 85/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3055 - val_acc: 0.8861\n",
            "Epoch 86/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2990 - acc: 0.8861 - val_loss: 0.3072 - val_acc: 0.8850\n",
            "Epoch 87/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2990 - acc: 0.8855 - val_loss: 0.3056 - val_acc: 0.8845\n",
            "Epoch 88/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8854 - val_loss: 0.3074 - val_acc: 0.8868\n",
            "Epoch 89/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2988 - acc: 0.8852 - val_loss: 0.3067 - val_acc: 0.8874\n",
            "Epoch 90/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8853 - val_loss: 0.3065 - val_acc: 0.8854\n",
            "Epoch 91/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8853 - val_loss: 0.3073 - val_acc: 0.8839\n",
            "Epoch 92/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2990 - acc: 0.8841 - val_loss: 0.3060 - val_acc: 0.8883\n",
            "Epoch 93/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8852 - val_loss: 0.3071 - val_acc: 0.8881\n",
            "Epoch 94/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2988 - acc: 0.8858 - val_loss: 0.3065 - val_acc: 0.8852\n",
            "Epoch 95/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8856 - val_loss: 0.3063 - val_acc: 0.8848\n",
            "Epoch 96/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2988 - acc: 0.8849 - val_loss: 0.3062 - val_acc: 0.8865\n",
            "Epoch 97/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3050 - val_acc: 0.8872\n",
            "Epoch 98/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2989 - acc: 0.8850 - val_loss: 0.3048 - val_acc: 0.8892\n",
            "Epoch 99/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2987 - acc: 0.8857 - val_loss: 0.3067 - val_acc: 0.8874\n",
            "Epoch 100/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3053 - val_acc: 0.8868\n",
            "Epoch 101/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8855 - val_loss: 0.3066 - val_acc: 0.8845\n",
            "Epoch 102/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2987 - acc: 0.8847 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 103/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8856 - val_loss: 0.3050 - val_acc: 0.8870\n",
            "Epoch 104/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3056 - val_acc: 0.8865\n",
            "Epoch 105/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8853 - val_loss: 0.3061 - val_acc: 0.8881\n",
            "Epoch 106/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2983 - acc: 0.8851 - val_loss: 0.3057 - val_acc: 0.8865\n",
            "Epoch 107/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2990 - acc: 0.8846 - val_loss: 0.3062 - val_acc: 0.8870\n",
            "Epoch 108/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2986 - acc: 0.8846 - val_loss: 0.3069 - val_acc: 0.8881\n",
            "Epoch 109/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8851 - val_loss: 0.3067 - val_acc: 0.8861\n",
            "Epoch 110/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8853 - val_loss: 0.3048 - val_acc: 0.8852\n",
            "Epoch 111/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8849 - val_loss: 0.3057 - val_acc: 0.8863\n",
            "Epoch 112/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8857 - val_loss: 0.3053 - val_acc: 0.8870\n",
            "Epoch 113/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8849 - val_loss: 0.3045 - val_acc: 0.8852\n",
            "Epoch 114/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3071 - val_acc: 0.8876\n",
            "Epoch 115/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2983 - acc: 0.8854 - val_loss: 0.3069 - val_acc: 0.8863\n",
            "Epoch 116/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2988 - acc: 0.8855 - val_loss: 0.3045 - val_acc: 0.8863\n",
            "Epoch 117/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8847 - val_loss: 0.3054 - val_acc: 0.8876\n",
            "Epoch 118/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8848 - val_loss: 0.3055 - val_acc: 0.8885\n",
            "Epoch 119/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8854 - val_loss: 0.3059 - val_acc: 0.8859\n",
            "Epoch 120/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3068 - val_acc: 0.8874\n",
            "Epoch 121/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8846 - val_loss: 0.3053 - val_acc: 0.8861\n",
            "Epoch 122/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2986 - acc: 0.8852 - val_loss: 0.3064 - val_acc: 0.8868\n",
            "Epoch 123/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2986 - acc: 0.8853 - val_loss: 0.3067 - val_acc: 0.8881\n",
            "Epoch 124/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8851 - val_loss: 0.3064 - val_acc: 0.8859\n",
            "Epoch 125/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8855 - val_loss: 0.3051 - val_acc: 0.8870\n",
            "Epoch 126/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8852 - val_loss: 0.3053 - val_acc: 0.8870\n",
            "Epoch 127/150\n",
            "40690/40690 [==============================] - 1s 18us/step - loss: 0.2983 - acc: 0.8859 - val_loss: 0.3052 - val_acc: 0.8872\n",
            "Epoch 128/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8849 - val_loss: 0.3054 - val_acc: 0.8863\n",
            "Epoch 129/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8852 - val_loss: 0.3053 - val_acc: 0.8872\n",
            "Epoch 130/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3078 - val_acc: 0.8876\n",
            "Epoch 131/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8843 - val_loss: 0.3070 - val_acc: 0.8859\n",
            "Epoch 132/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8855 - val_loss: 0.3056 - val_acc: 0.8861\n",
            "Epoch 133/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8853 - val_loss: 0.3058 - val_acc: 0.8859\n",
            "Epoch 134/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2982 - acc: 0.8857 - val_loss: 0.3056 - val_acc: 0.8854\n",
            "Epoch 135/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2986 - acc: 0.8859 - val_loss: 0.3056 - val_acc: 0.8874\n",
            "Epoch 136/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8844 - val_loss: 0.3058 - val_acc: 0.8887\n",
            "Epoch 137/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2984 - acc: 0.8851 - val_loss: 0.3062 - val_acc: 0.8859\n",
            "Epoch 138/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8847 - val_loss: 0.3055 - val_acc: 0.8872\n",
            "Epoch 139/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8851 - val_loss: 0.3052 - val_acc: 0.8863\n",
            "Epoch 140/150\n",
            "40690/40690 [==============================] - 1s 17us/step - loss: 0.2984 - acc: 0.8845 - val_loss: 0.3059 - val_acc: 0.8879\n",
            "Epoch 141/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2985 - acc: 0.8853 - val_loss: 0.3052 - val_acc: 0.8865\n",
            "Epoch 142/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2982 - acc: 0.8850 - val_loss: 0.3065 - val_acc: 0.8861\n",
            "Epoch 143/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8848 - val_loss: 0.3045 - val_acc: 0.8865\n",
            "Epoch 144/150\n",
            "40690/40690 [==============================] - 1s 15us/step - loss: 0.2986 - acc: 0.8846 - val_loss: 0.3037 - val_acc: 0.8856\n",
            "Epoch 145/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8846 - val_loss: 0.3045 - val_acc: 0.8885\n",
            "Epoch 146/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8849 - val_loss: 0.3046 - val_acc: 0.8856\n",
            "Epoch 147/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8855 - val_loss: 0.3050 - val_acc: 0.8854\n",
            "Epoch 148/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2985 - acc: 0.8850 - val_loss: 0.3052 - val_acc: 0.8883\n",
            "Epoch 149/150\n",
            "40690/40690 [==============================] - 1s 14us/step - loss: 0.2984 - acc: 0.8850 - val_loss: 0.3060 - val_acc: 0.8868\n",
            "Epoch 150/150\n",
            "40690/40690 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8842 - val_loss: 0.3044 - val_acc: 0.8856\n",
            "acc: 88.56%\n",
            "88.70% (+/- 0.19%)\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   68    460        yes\n",
            "   50    3941        no\n",
            "Accuracy: 0.886974408882794\n",
            "Sensitivity: 0.13008130081300814\n",
            "Specificity: 0.9872501377686489\n",
            "Precision: 0.5747702589807854\n",
            "f_score: 0.21214924452667286\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pepM7OrAnFCl",
        "colab_type": "code",
        "outputId": "5b544540-596f-493c-cc6a-8cb9dce1a1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fpr_nn_p,tpr_nn_p ,thresholds_nn_p,auc_nn_p=percentage_split_NN_SMOTE(0.25,X,Y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 59882 samples, validate on 11303 samples\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "59882/59882 [==============================] - 2s 27us/step - loss: 0.3586 - acc: 0.8441 - val_loss: 0.3683 - val_acc: 0.8314\n",
            "Epoch 2/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.3131 - acc: 0.8710 - val_loss: 0.3732 - val_acc: 0.8259\n",
            "Epoch 3/150\n",
            "59882/59882 [==============================] - 1s 15us/step - loss: 0.3033 - acc: 0.8757 - val_loss: 0.3784 - val_acc: 0.8245\n",
            "Epoch 4/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8783 - val_loss: 0.3580 - val_acc: 0.8356\n",
            "Epoch 5/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2955 - acc: 0.8802 - val_loss: 0.3861 - val_acc: 0.8233\n",
            "Epoch 6/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2936 - acc: 0.8804 - val_loss: 0.3466 - val_acc: 0.8384\n",
            "Epoch 7/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2924 - acc: 0.8813 - val_loss: 0.3425 - val_acc: 0.8424\n",
            "Epoch 8/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2916 - acc: 0.8812 - val_loss: 0.3442 - val_acc: 0.8380\n",
            "Epoch 9/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8821 - val_loss: 0.3590 - val_acc: 0.8356\n",
            "Epoch 10/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8815 - val_loss: 0.3659 - val_acc: 0.8316\n",
            "Epoch 11/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8814 - val_loss: 0.3213 - val_acc: 0.8464\n",
            "Epoch 12/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8810 - val_loss: 0.3519 - val_acc: 0.8320\n",
            "Epoch 13/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8813 - val_loss: 0.3308 - val_acc: 0.8459\n",
            "Epoch 14/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2900 - acc: 0.8805 - val_loss: 0.3317 - val_acc: 0.8411\n",
            "Epoch 15/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8816 - val_loss: 0.3448 - val_acc: 0.8407\n",
            "Epoch 16/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2895 - acc: 0.8824 - val_loss: 0.3496 - val_acc: 0.8356\n",
            "Epoch 17/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8819 - val_loss: 0.3260 - val_acc: 0.8492\n",
            "Epoch 18/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2898 - acc: 0.8810 - val_loss: 0.3395 - val_acc: 0.8383\n",
            "Epoch 19/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8818 - val_loss: 0.3424 - val_acc: 0.8317\n",
            "Epoch 20/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8817 - val_loss: 0.3395 - val_acc: 0.8376\n",
            "Epoch 21/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2888 - acc: 0.8815 - val_loss: 0.3492 - val_acc: 0.8410\n",
            "Epoch 22/150\n",
            "59882/59882 [==============================] - 1s 19us/step - loss: 0.2887 - acc: 0.8822 - val_loss: 0.3299 - val_acc: 0.8465\n",
            "Epoch 23/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8817 - val_loss: 0.3594 - val_acc: 0.8257\n",
            "Epoch 24/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8815 - val_loss: 0.3299 - val_acc: 0.8460\n",
            "Epoch 25/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8822 - val_loss: 0.3498 - val_acc: 0.8283\n",
            "Epoch 26/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8818 - val_loss: 0.3503 - val_acc: 0.8265\n",
            "Epoch 27/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8826 - val_loss: 0.3449 - val_acc: 0.8342\n",
            "Epoch 28/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8817 - val_loss: 0.3716 - val_acc: 0.8249\n",
            "Epoch 29/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8829 - val_loss: 0.3530 - val_acc: 0.8277\n",
            "Epoch 30/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8818 - val_loss: 0.3394 - val_acc: 0.8405\n",
            "Epoch 31/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8815 - val_loss: 0.3413 - val_acc: 0.8320\n",
            "Epoch 32/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8823 - val_loss: 0.3463 - val_acc: 0.8321\n",
            "Epoch 33/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8817 - val_loss: 0.3359 - val_acc: 0.8439\n",
            "Epoch 34/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8820 - val_loss: 0.3564 - val_acc: 0.8223\n",
            "Epoch 35/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8819 - val_loss: 0.3357 - val_acc: 0.8404\n",
            "Epoch 36/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8813 - val_loss: 0.3355 - val_acc: 0.8384\n",
            "Epoch 37/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8813 - val_loss: 0.3611 - val_acc: 0.8332\n",
            "Epoch 38/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8826 - val_loss: 0.3218 - val_acc: 0.8429\n",
            "Epoch 39/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2879 - acc: 0.8822 - val_loss: 0.3341 - val_acc: 0.8333\n",
            "Epoch 40/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8817 - val_loss: 0.3383 - val_acc: 0.8413\n",
            "Epoch 41/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8815 - val_loss: 0.3601 - val_acc: 0.8230\n",
            "Epoch 42/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8815 - val_loss: 0.3380 - val_acc: 0.8306\n",
            "Epoch 43/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8815 - val_loss: 0.3430 - val_acc: 0.8393\n",
            "Epoch 44/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8823 - val_loss: 0.3746 - val_acc: 0.8179\n",
            "Epoch 45/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8811 - val_loss: 0.3439 - val_acc: 0.8345\n",
            "Epoch 46/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8817 - val_loss: 0.3160 - val_acc: 0.8505\n",
            "Epoch 47/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8817 - val_loss: 0.3322 - val_acc: 0.8353\n",
            "Epoch 48/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8820 - val_loss: 0.3556 - val_acc: 0.8340\n",
            "Epoch 49/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8816 - val_loss: 0.3348 - val_acc: 0.8379\n",
            "Epoch 50/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8828 - val_loss: 0.3304 - val_acc: 0.8431\n",
            "Epoch 51/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8813 - val_loss: 0.3420 - val_acc: 0.8379\n",
            "Epoch 52/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8818 - val_loss: 0.3226 - val_acc: 0.8458\n",
            "Epoch 53/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8818 - val_loss: 0.3342 - val_acc: 0.8433\n",
            "Epoch 54/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8825 - val_loss: 0.3477 - val_acc: 0.8321\n",
            "Epoch 55/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8820 - val_loss: 0.3510 - val_acc: 0.8257\n",
            "Epoch 56/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8814 - val_loss: 0.3422 - val_acc: 0.8404\n",
            "Epoch 57/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8829 - val_loss: 0.3428 - val_acc: 0.8358\n",
            "Epoch 58/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8827 - val_loss: 0.3465 - val_acc: 0.8350\n",
            "Epoch 59/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8826 - val_loss: 0.3269 - val_acc: 0.8461\n",
            "Epoch 60/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8827 - val_loss: 0.3485 - val_acc: 0.8300\n",
            "Epoch 61/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8811 - val_loss: 0.3291 - val_acc: 0.8440\n",
            "Epoch 62/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8816 - val_loss: 0.3438 - val_acc: 0.8365\n",
            "Epoch 63/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8826 - val_loss: 0.3305 - val_acc: 0.8355\n",
            "Epoch 64/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8824 - val_loss: 0.3571 - val_acc: 0.8299\n",
            "Epoch 65/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8810 - val_loss: 0.3502 - val_acc: 0.8325\n",
            "Epoch 66/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8835 - val_loss: 0.3566 - val_acc: 0.8262\n",
            "Epoch 67/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8822 - val_loss: 0.3513 - val_acc: 0.8273\n",
            "Epoch 68/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8822 - val_loss: 0.3654 - val_acc: 0.8193\n",
            "Epoch 69/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8812 - val_loss: 0.3445 - val_acc: 0.8298\n",
            "Epoch 70/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8826 - val_loss: 0.3627 - val_acc: 0.8246\n",
            "Epoch 71/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8823 - val_loss: 0.3471 - val_acc: 0.8340\n",
            "Epoch 72/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8826 - val_loss: 0.3663 - val_acc: 0.8282\n",
            "Epoch 73/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8813 - val_loss: 0.3442 - val_acc: 0.8379\n",
            "Epoch 74/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8829 - val_loss: 0.3578 - val_acc: 0.8261\n",
            "Epoch 75/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8813 - val_loss: 0.3536 - val_acc: 0.8362\n",
            "Epoch 76/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8821 - val_loss: 0.3392 - val_acc: 0.8356\n",
            "Epoch 77/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8838 - val_loss: 0.3391 - val_acc: 0.8343\n",
            "Epoch 78/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8822 - val_loss: 0.3538 - val_acc: 0.8246\n",
            "Epoch 79/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8819 - val_loss: 0.3272 - val_acc: 0.8442\n",
            "Epoch 80/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8821 - val_loss: 0.3789 - val_acc: 0.8147\n",
            "Epoch 81/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8817 - val_loss: 0.3383 - val_acc: 0.8400\n",
            "Epoch 82/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8820 - val_loss: 0.3278 - val_acc: 0.8421\n",
            "Epoch 83/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8811 - val_loss: 0.3472 - val_acc: 0.8242\n",
            "Epoch 84/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8822 - val_loss: 0.3489 - val_acc: 0.8318\n",
            "Epoch 85/150\n",
            "59882/59882 [==============================] - 1s 15us/step - loss: 0.2872 - acc: 0.8815 - val_loss: 0.3322 - val_acc: 0.8408\n",
            "Epoch 86/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8809 - val_loss: 0.3575 - val_acc: 0.8215\n",
            "Epoch 87/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8824 - val_loss: 0.3317 - val_acc: 0.8394\n",
            "Epoch 88/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8819 - val_loss: 0.3490 - val_acc: 0.8283\n",
            "Epoch 89/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8823 - val_loss: 0.3463 - val_acc: 0.8385\n",
            "Epoch 90/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8830 - val_loss: 0.3294 - val_acc: 0.8413\n",
            "Epoch 91/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8829 - val_loss: 0.3619 - val_acc: 0.8347\n",
            "Epoch 92/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8823 - val_loss: 0.3388 - val_acc: 0.8359\n",
            "Epoch 93/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8815 - val_loss: 0.3576 - val_acc: 0.8352\n",
            "Epoch 94/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8824 - val_loss: 0.3233 - val_acc: 0.8471\n",
            "Epoch 95/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8823 - val_loss: 0.3601 - val_acc: 0.8181\n",
            "Epoch 96/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8818 - val_loss: 0.3332 - val_acc: 0.8438\n",
            "Epoch 97/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8829 - val_loss: 0.3410 - val_acc: 0.8404\n",
            "Epoch 98/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8822 - val_loss: 0.3418 - val_acc: 0.8315\n",
            "Epoch 99/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8827 - val_loss: 0.3440 - val_acc: 0.8290\n",
            "Epoch 100/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8816 - val_loss: 0.3494 - val_acc: 0.8346\n",
            "Epoch 101/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8816 - val_loss: 0.3412 - val_acc: 0.8301\n",
            "Epoch 102/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2869 - acc: 0.8820 - val_loss: 0.3502 - val_acc: 0.8338\n",
            "Epoch 103/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8821 - val_loss: 0.3415 - val_acc: 0.8304\n",
            "Epoch 104/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8823 - val_loss: 0.3523 - val_acc: 0.8276\n",
            "Epoch 105/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8826 - val_loss: 0.3457 - val_acc: 0.8277\n",
            "Epoch 106/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8820 - val_loss: 0.3292 - val_acc: 0.8453\n",
            "Epoch 107/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8822 - val_loss: 0.3635 - val_acc: 0.8227\n",
            "Epoch 108/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8832 - val_loss: 0.3496 - val_acc: 0.8316\n",
            "Epoch 109/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8820 - val_loss: 0.3572 - val_acc: 0.8353\n",
            "Epoch 110/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2869 - acc: 0.8820 - val_loss: 0.3450 - val_acc: 0.8318\n",
            "Epoch 111/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2874 - acc: 0.8827 - val_loss: 0.3510 - val_acc: 0.8293\n",
            "Epoch 112/150\n",
            "59882/59882 [==============================] - 1s 19us/step - loss: 0.2875 - acc: 0.8826 - val_loss: 0.3319 - val_acc: 0.8421\n",
            "Epoch 113/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2872 - acc: 0.8828 - val_loss: 0.3581 - val_acc: 0.8215\n",
            "Epoch 114/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8821 - val_loss: 0.3518 - val_acc: 0.8262\n",
            "Epoch 115/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2876 - acc: 0.8815 - val_loss: 0.3529 - val_acc: 0.8335\n",
            "Epoch 116/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8826 - val_loss: 0.3383 - val_acc: 0.8301\n",
            "Epoch 117/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8821 - val_loss: 0.3715 - val_acc: 0.8315\n",
            "Epoch 118/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8815 - val_loss: 0.3398 - val_acc: 0.8334\n",
            "Epoch 119/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8821 - val_loss: 0.3640 - val_acc: 0.8234\n",
            "Epoch 120/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8823 - val_loss: 0.3662 - val_acc: 0.8208\n",
            "Epoch 121/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8815 - val_loss: 0.3522 - val_acc: 0.8384\n",
            "Epoch 122/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8829 - val_loss: 0.3384 - val_acc: 0.8383\n",
            "Epoch 123/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8826 - val_loss: 0.3418 - val_acc: 0.8400\n",
            "Epoch 124/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8829 - val_loss: 0.3487 - val_acc: 0.8288\n",
            "Epoch 125/150\n",
            "59882/59882 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8823 - val_loss: 0.3299 - val_acc: 0.8400\n",
            "Epoch 126/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8815 - val_loss: 0.3346 - val_acc: 0.8381\n",
            "Epoch 127/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8825 - val_loss: 0.3471 - val_acc: 0.8340\n",
            "Epoch 128/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8832 - val_loss: 0.3362 - val_acc: 0.8413\n",
            "Epoch 129/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8822 - val_loss: 0.3593 - val_acc: 0.8329\n",
            "Epoch 130/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2867 - acc: 0.8825 - val_loss: 0.3348 - val_acc: 0.8421\n",
            "Epoch 131/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8827 - val_loss: 0.3447 - val_acc: 0.8382\n",
            "Epoch 132/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2860 - acc: 0.8830 - val_loss: 0.3359 - val_acc: 0.8435\n",
            "Epoch 133/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8826 - val_loss: 0.3416 - val_acc: 0.8271\n",
            "Epoch 134/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8832 - val_loss: 0.3425 - val_acc: 0.8397\n",
            "Epoch 135/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8820 - val_loss: 0.3355 - val_acc: 0.8338\n",
            "Epoch 136/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8825 - val_loss: 0.3422 - val_acc: 0.8307\n",
            "Epoch 137/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8826 - val_loss: 0.3617 - val_acc: 0.8240\n",
            "Epoch 138/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8823 - val_loss: 0.3349 - val_acc: 0.8396\n",
            "Epoch 139/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8827 - val_loss: 0.3444 - val_acc: 0.8333\n",
            "Epoch 140/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2876 - acc: 0.8821 - val_loss: 0.3569 - val_acc: 0.8294\n",
            "Epoch 141/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8825 - val_loss: 0.3355 - val_acc: 0.8331\n",
            "Epoch 142/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8833 - val_loss: 0.3466 - val_acc: 0.8346\n",
            "Epoch 143/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8828 - val_loss: 0.3307 - val_acc: 0.8408\n",
            "Epoch 144/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8831 - val_loss: 0.3273 - val_acc: 0.8448\n",
            "Epoch 145/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8833 - val_loss: 0.3370 - val_acc: 0.8375\n",
            "Epoch 146/150\n",
            "59882/59882 [==============================] - 1s 15us/step - loss: 0.2868 - acc: 0.8817 - val_loss: 0.3374 - val_acc: 0.8378\n",
            "Epoch 147/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8827 - val_loss: 0.3342 - val_acc: 0.8359\n",
            "Epoch 148/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8830 - val_loss: 0.3199 - val_acc: 0.8444\n",
            "Epoch 149/150\n",
            "59882/59882 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8824 - val_loss: 0.3807 - val_acc: 0.8154\n",
            "Epoch 150/150\n",
            "59882/59882 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8828 - val_loss: 0.3230 - val_acc: 0.8430\n",
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1146    176        yes\n",
            "   1599    8382        no\n",
            "Accuracy: 0.8429620454746527\n",
            "Sensitivity: 0.8668683812405447\n",
            "Specificity: 0.8397956116621581\n",
            "Precision: 0.4174863387978142\n",
            "f_score: 0.563560363904598\n",
            "AUC: 0.8533319964513514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WZNHH5VaIiv",
        "colab_type": "code",
        "outputId": "34fb7aa4-e4cf-4d47-b777-2ca783ae4f2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fpr_nn_c,tpr_nn_c ,thresholds_nn_c,auc_nn_c=crossvalidate_NN_SMOTE(10,X,Y)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 71858 samples, validate on 4522 samples\n",
            "Epoch 1/150\n",
            "71858/71858 [==============================] - 1s 19us/step - loss: 0.3603 - acc: 0.8436 - val_loss: 0.3494 - val_acc: 0.8280\n",
            "Epoch 2/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3175 - acc: 0.8668 - val_loss: 0.3475 - val_acc: 0.8361\n",
            "Epoch 3/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3092 - acc: 0.8729 - val_loss: 0.3575 - val_acc: 0.8244\n",
            "Epoch 4/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3051 - acc: 0.8754 - val_loss: 0.3573 - val_acc: 0.8284\n",
            "Epoch 5/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3026 - acc: 0.8765 - val_loss: 0.3277 - val_acc: 0.8381\n",
            "Epoch 6/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3017 - acc: 0.8773 - val_loss: 0.3386 - val_acc: 0.8399\n",
            "Epoch 7/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3002 - acc: 0.8790 - val_loss: 0.3443 - val_acc: 0.8383\n",
            "Epoch 8/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3003 - acc: 0.8781 - val_loss: 0.3445 - val_acc: 0.8421\n",
            "Epoch 9/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8785 - val_loss: 0.3191 - val_acc: 0.8543\n",
            "Epoch 10/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2988 - acc: 0.8781 - val_loss: 0.3474 - val_acc: 0.8375\n",
            "Epoch 11/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2983 - acc: 0.8791 - val_loss: 0.3458 - val_acc: 0.8315\n",
            "Epoch 12/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2983 - acc: 0.8789 - val_loss: 0.3262 - val_acc: 0.8428\n",
            "Epoch 13/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2968 - acc: 0.8794 - val_loss: 0.3342 - val_acc: 0.8410\n",
            "Epoch 14/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2974 - acc: 0.8786 - val_loss: 0.3362 - val_acc: 0.8361\n",
            "Epoch 15/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8798 - val_loss: 0.3244 - val_acc: 0.8366\n",
            "Epoch 16/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8804 - val_loss: 0.3351 - val_acc: 0.8381\n",
            "Epoch 17/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2959 - acc: 0.8790 - val_loss: 0.3291 - val_acc: 0.8437\n",
            "Epoch 18/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2955 - acc: 0.8804 - val_loss: 0.3416 - val_acc: 0.8344\n",
            "Epoch 19/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2951 - acc: 0.8800 - val_loss: 0.3465 - val_acc: 0.8264\n",
            "Epoch 20/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2951 - acc: 0.8805 - val_loss: 0.3196 - val_acc: 0.8432\n",
            "Epoch 21/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2940 - acc: 0.8807 - val_loss: 0.3464 - val_acc: 0.8264\n",
            "Epoch 22/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2950 - acc: 0.8805 - val_loss: 0.3484 - val_acc: 0.8350\n",
            "Epoch 23/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2946 - acc: 0.8812 - val_loss: 0.3477 - val_acc: 0.8249\n",
            "Epoch 24/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.2946 - acc: 0.8812 - val_loss: 0.3284 - val_acc: 0.8425\n",
            "Epoch 25/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2944 - acc: 0.8814 - val_loss: 0.3508 - val_acc: 0.8306\n",
            "Epoch 26/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2942 - acc: 0.8818 - val_loss: 0.3394 - val_acc: 0.8313\n",
            "Epoch 27/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2941 - acc: 0.8825 - val_loss: 0.3427 - val_acc: 0.8317\n",
            "Epoch 28/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8819 - val_loss: 0.3321 - val_acc: 0.8432\n",
            "Epoch 29/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2940 - acc: 0.8826 - val_loss: 0.3349 - val_acc: 0.8359\n",
            "Epoch 30/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.2931 - acc: 0.8825 - val_loss: 0.3252 - val_acc: 0.8410\n",
            "Epoch 31/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2934 - acc: 0.8822 - val_loss: 0.3358 - val_acc: 0.8364\n",
            "Epoch 32/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2932 - acc: 0.8826 - val_loss: 0.3367 - val_acc: 0.8386\n",
            "Epoch 33/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2928 - acc: 0.8826 - val_loss: 0.3657 - val_acc: 0.8235\n",
            "Epoch 34/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2930 - acc: 0.8831 - val_loss: 0.3144 - val_acc: 0.8474\n",
            "Epoch 35/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2925 - acc: 0.8824 - val_loss: 0.3478 - val_acc: 0.8288\n",
            "Epoch 36/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2928 - acc: 0.8822 - val_loss: 0.3398 - val_acc: 0.8352\n",
            "Epoch 37/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2925 - acc: 0.8832 - val_loss: 0.3465 - val_acc: 0.8350\n",
            "Epoch 38/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2922 - acc: 0.8834 - val_loss: 0.3263 - val_acc: 0.8425\n",
            "Epoch 39/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2919 - acc: 0.8826 - val_loss: 0.3206 - val_acc: 0.8467\n",
            "Epoch 40/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2924 - acc: 0.8823 - val_loss: 0.3334 - val_acc: 0.8377\n",
            "Epoch 41/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8832 - val_loss: 0.3572 - val_acc: 0.8253\n",
            "Epoch 42/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2917 - acc: 0.8837 - val_loss: 0.3355 - val_acc: 0.8395\n",
            "Epoch 43/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8834 - val_loss: 0.3369 - val_acc: 0.8401\n",
            "Epoch 44/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3426 - val_acc: 0.8335\n",
            "Epoch 45/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2919 - acc: 0.8829 - val_loss: 0.3431 - val_acc: 0.8388\n",
            "Epoch 46/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8831 - val_loss: 0.3439 - val_acc: 0.8390\n",
            "Epoch 47/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2916 - acc: 0.8829 - val_loss: 0.3321 - val_acc: 0.8379\n",
            "Epoch 48/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2916 - acc: 0.8838 - val_loss: 0.3243 - val_acc: 0.8397\n",
            "Epoch 49/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3562 - val_acc: 0.8306\n",
            "Epoch 50/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8837 - val_loss: 0.3283 - val_acc: 0.8476\n",
            "Epoch 51/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8836 - val_loss: 0.3223 - val_acc: 0.8417\n",
            "Epoch 52/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2913 - acc: 0.8835 - val_loss: 0.3444 - val_acc: 0.8341\n",
            "Epoch 53/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8832 - val_loss: 0.3354 - val_acc: 0.8406\n",
            "Epoch 54/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8831 - val_loss: 0.3222 - val_acc: 0.8443\n",
            "Epoch 55/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2912 - acc: 0.8838 - val_loss: 0.3149 - val_acc: 0.8527\n",
            "Epoch 56/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8835 - val_loss: 0.3227 - val_acc: 0.8481\n",
            "Epoch 57/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8834 - val_loss: 0.3166 - val_acc: 0.8503\n",
            "Epoch 58/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8834 - val_loss: 0.3206 - val_acc: 0.8459\n",
            "Epoch 59/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8847 - val_loss: 0.3197 - val_acc: 0.8463\n",
            "Epoch 60/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8832 - val_loss: 0.3254 - val_acc: 0.8443\n",
            "Epoch 61/150\n",
            "71858/71858 [==============================] - 1s 18us/step - loss: 0.2907 - acc: 0.8847 - val_loss: 0.3434 - val_acc: 0.8381\n",
            "Epoch 62/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2909 - acc: 0.8834 - val_loss: 0.3287 - val_acc: 0.8375\n",
            "Epoch 63/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8835 - val_loss: 0.3339 - val_acc: 0.8401\n",
            "Epoch 64/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8836 - val_loss: 0.3543 - val_acc: 0.8322\n",
            "Epoch 65/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8840 - val_loss: 0.3277 - val_acc: 0.8379\n",
            "Epoch 66/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8838 - val_loss: 0.3394 - val_acc: 0.8379\n",
            "Epoch 67/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8837 - val_loss: 0.3473 - val_acc: 0.8386\n",
            "Epoch 68/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8840 - val_loss: 0.3284 - val_acc: 0.8463\n",
            "Epoch 69/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2903 - acc: 0.8836 - val_loss: 0.3471 - val_acc: 0.8381\n",
            "Epoch 70/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2907 - acc: 0.8839 - val_loss: 0.3285 - val_acc: 0.8375\n",
            "Epoch 71/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8847 - val_loss: 0.3357 - val_acc: 0.8395\n",
            "Epoch 72/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8842 - val_loss: 0.3289 - val_acc: 0.8410\n",
            "Epoch 73/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8839 - val_loss: 0.3315 - val_acc: 0.8441\n",
            "Epoch 74/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2909 - acc: 0.8842 - val_loss: 0.3227 - val_acc: 0.8430\n",
            "Epoch 75/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8832 - val_loss: 0.3376 - val_acc: 0.8339\n",
            "Epoch 76/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2901 - acc: 0.8836 - val_loss: 0.3442 - val_acc: 0.8333\n",
            "Epoch 77/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2908 - acc: 0.8838 - val_loss: 0.3434 - val_acc: 0.8357\n",
            "Epoch 78/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2905 - acc: 0.8830 - val_loss: 0.3381 - val_acc: 0.8392\n",
            "Epoch 79/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2906 - acc: 0.8845 - val_loss: 0.3572 - val_acc: 0.8364\n",
            "Epoch 80/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2905 - acc: 0.8835 - val_loss: 0.3182 - val_acc: 0.8445\n",
            "Epoch 81/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8834 - val_loss: 0.3307 - val_acc: 0.8412\n",
            "Epoch 82/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2906 - acc: 0.8834 - val_loss: 0.3509 - val_acc: 0.8328\n",
            "Epoch 83/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8836 - val_loss: 0.3300 - val_acc: 0.8428\n",
            "Epoch 84/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8842 - val_loss: 0.3520 - val_acc: 0.8330\n",
            "Epoch 85/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2907 - acc: 0.8839 - val_loss: 0.3524 - val_acc: 0.8319\n",
            "Epoch 86/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8837 - val_loss: 0.3350 - val_acc: 0.8428\n",
            "Epoch 87/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2906 - acc: 0.8838 - val_loss: 0.3352 - val_acc: 0.8408\n",
            "Epoch 88/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8842 - val_loss: 0.3270 - val_acc: 0.8494\n",
            "Epoch 89/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8845 - val_loss: 0.3308 - val_acc: 0.8425\n",
            "Epoch 90/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8843 - val_loss: 0.3408 - val_acc: 0.8383\n",
            "Epoch 91/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8838 - val_loss: 0.3213 - val_acc: 0.8487\n",
            "Epoch 92/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8850 - val_loss: 0.3353 - val_acc: 0.8421\n",
            "Epoch 93/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8845 - val_loss: 0.3272 - val_acc: 0.8443\n",
            "Epoch 94/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8845 - val_loss: 0.3298 - val_acc: 0.8437\n",
            "Epoch 95/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8850 - val_loss: 0.3322 - val_acc: 0.8386\n",
            "Epoch 96/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8835 - val_loss: 0.3306 - val_acc: 0.8445\n",
            "Epoch 97/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2901 - acc: 0.8841 - val_loss: 0.3251 - val_acc: 0.8437\n",
            "Epoch 98/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2907 - acc: 0.8843 - val_loss: 0.3306 - val_acc: 0.8419\n",
            "Epoch 99/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2903 - acc: 0.8843 - val_loss: 0.3573 - val_acc: 0.8315\n",
            "Epoch 100/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8846 - val_loss: 0.3396 - val_acc: 0.8417\n",
            "Epoch 101/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8837 - val_loss: 0.3366 - val_acc: 0.8406\n",
            "Epoch 102/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2902 - acc: 0.8843 - val_loss: 0.3489 - val_acc: 0.8399\n",
            "Epoch 103/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8845 - val_loss: 0.3126 - val_acc: 0.8461\n",
            "Epoch 104/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8845 - val_loss: 0.3274 - val_acc: 0.8501\n",
            "Epoch 105/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8846 - val_loss: 0.3489 - val_acc: 0.8352\n",
            "Epoch 106/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2902 - acc: 0.8839 - val_loss: 0.3542 - val_acc: 0.8337\n",
            "Epoch 107/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8841 - val_loss: 0.3343 - val_acc: 0.8412\n",
            "Epoch 108/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8843 - val_loss: 0.3351 - val_acc: 0.8406\n",
            "Epoch 109/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8851 - val_loss: 0.3449 - val_acc: 0.8425\n",
            "Epoch 110/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8843 - val_loss: 0.3279 - val_acc: 0.8472\n",
            "Epoch 111/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8844 - val_loss: 0.3498 - val_acc: 0.8366\n",
            "Epoch 112/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8838 - val_loss: 0.3363 - val_acc: 0.8439\n",
            "Epoch 113/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3452 - val_acc: 0.8399\n",
            "Epoch 114/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8844 - val_loss: 0.3235 - val_acc: 0.8483\n",
            "Epoch 115/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8847 - val_loss: 0.3457 - val_acc: 0.8386\n",
            "Epoch 116/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2896 - acc: 0.8844 - val_loss: 0.3562 - val_acc: 0.8348\n",
            "Epoch 117/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8847 - val_loss: 0.3536 - val_acc: 0.8388\n",
            "Epoch 118/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8845 - val_loss: 0.3448 - val_acc: 0.8414\n",
            "Epoch 119/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8846 - val_loss: 0.3433 - val_acc: 0.8390\n",
            "Epoch 120/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8851 - val_loss: 0.3315 - val_acc: 0.8417\n",
            "Epoch 121/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8850 - val_loss: 0.3249 - val_acc: 0.8454\n",
            "Epoch 122/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8852 - val_loss: 0.3473 - val_acc: 0.8364\n",
            "Epoch 123/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8851 - val_loss: 0.3290 - val_acc: 0.8419\n",
            "Epoch 124/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8856 - val_loss: 0.3520 - val_acc: 0.8388\n",
            "Epoch 125/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2896 - acc: 0.8851 - val_loss: 0.3330 - val_acc: 0.8430\n",
            "Epoch 126/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8856 - val_loss: 0.3328 - val_acc: 0.8467\n",
            "Epoch 127/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8847 - val_loss: 0.3425 - val_acc: 0.8395\n",
            "Epoch 128/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8859 - val_loss: 0.3326 - val_acc: 0.8412\n",
            "Epoch 129/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8849 - val_loss: 0.3362 - val_acc: 0.8445\n",
            "Epoch 130/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8857 - val_loss: 0.3326 - val_acc: 0.8417\n",
            "Epoch 131/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8856 - val_loss: 0.3429 - val_acc: 0.8408\n",
            "Epoch 132/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8857 - val_loss: 0.3324 - val_acc: 0.8432\n",
            "Epoch 133/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8859 - val_loss: 0.3366 - val_acc: 0.8501\n",
            "Epoch 134/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8854 - val_loss: 0.3439 - val_acc: 0.8408\n",
            "Epoch 135/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8852 - val_loss: 0.3153 - val_acc: 0.8534\n",
            "Epoch 136/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8861 - val_loss: 0.3215 - val_acc: 0.8428\n",
            "Epoch 137/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8856 - val_loss: 0.3252 - val_acc: 0.8492\n",
            "Epoch 138/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8861 - val_loss: 0.3523 - val_acc: 0.8370\n",
            "Epoch 139/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8860 - val_loss: 0.3455 - val_acc: 0.8423\n",
            "Epoch 140/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8855 - val_loss: 0.3401 - val_acc: 0.8430\n",
            "Epoch 141/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8854 - val_loss: 0.3656 - val_acc: 0.8313\n",
            "Epoch 142/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8856 - val_loss: 0.3507 - val_acc: 0.8403\n",
            "Epoch 143/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8853 - val_loss: 0.3415 - val_acc: 0.8439\n",
            "Epoch 144/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8856 - val_loss: 0.3403 - val_acc: 0.8417\n",
            "Epoch 145/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8851 - val_loss: 0.3293 - val_acc: 0.8456\n",
            "Epoch 146/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8861 - val_loss: 0.3401 - val_acc: 0.8450\n",
            "Epoch 147/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8859 - val_loss: 0.3286 - val_acc: 0.8474\n",
            "Epoch 148/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8858 - val_loss: 0.3473 - val_acc: 0.8430\n",
            "Epoch 149/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8856 - val_loss: 0.3524 - val_acc: 0.8355\n",
            "Epoch 150/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8860 - val_loss: 0.3431 - val_acc: 0.8388\n",
            "acc: 83.88%\n",
            "Train on 71858 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71858/71858 [==============================] - 1s 20us/step - loss: 0.3627 - acc: 0.8432 - val_loss: 0.3964 - val_acc: 0.8042\n",
            "Epoch 2/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3245 - acc: 0.8657 - val_loss: 0.3473 - val_acc: 0.8363\n",
            "Epoch 3/150\n",
            "71858/71858 [==============================] - 1s 14us/step - loss: 0.3096 - acc: 0.8757 - val_loss: 0.3655 - val_acc: 0.8246\n",
            "Epoch 4/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3045 - acc: 0.8769 - val_loss: 0.3494 - val_acc: 0.8330\n",
            "Epoch 5/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3029 - acc: 0.8770 - val_loss: 0.3823 - val_acc: 0.8151\n",
            "Epoch 6/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8786 - val_loss: 0.3481 - val_acc: 0.8295\n",
            "Epoch 7/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.3010 - acc: 0.8784 - val_loss: 0.3652 - val_acc: 0.8261\n",
            "Epoch 8/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.3003 - acc: 0.8798 - val_loss: 0.3646 - val_acc: 0.8264\n",
            "Epoch 9/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2996 - acc: 0.8790 - val_loss: 0.3729 - val_acc: 0.8230\n",
            "Epoch 10/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2981 - acc: 0.8799 - val_loss: 0.3716 - val_acc: 0.8242\n",
            "Epoch 11/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2978 - acc: 0.8797 - val_loss: 0.3508 - val_acc: 0.8268\n",
            "Epoch 12/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2972 - acc: 0.8795 - val_loss: 0.3782 - val_acc: 0.8166\n",
            "Epoch 13/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2974 - acc: 0.8792 - val_loss: 0.3667 - val_acc: 0.8226\n",
            "Epoch 14/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8781 - val_loss: 0.3698 - val_acc: 0.8186\n",
            "Epoch 15/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2965 - acc: 0.8787 - val_loss: 0.3582 - val_acc: 0.8317\n",
            "Epoch 16/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2966 - acc: 0.8789 - val_loss: 0.3457 - val_acc: 0.8345\n",
            "Epoch 17/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2970 - acc: 0.8784 - val_loss: 0.3647 - val_acc: 0.8242\n",
            "Epoch 18/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2961 - acc: 0.8793 - val_loss: 0.3676 - val_acc: 0.8211\n",
            "Epoch 19/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2958 - acc: 0.8787 - val_loss: 0.3607 - val_acc: 0.8224\n",
            "Epoch 20/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2962 - acc: 0.8789 - val_loss: 0.3573 - val_acc: 0.8359\n",
            "Epoch 21/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2958 - acc: 0.8792 - val_loss: 0.3590 - val_acc: 0.8222\n",
            "Epoch 22/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2955 - acc: 0.8789 - val_loss: 0.3581 - val_acc: 0.8277\n",
            "Epoch 23/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2952 - acc: 0.8788 - val_loss: 0.3512 - val_acc: 0.8381\n",
            "Epoch 24/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8792 - val_loss: 0.3618 - val_acc: 0.8319\n",
            "Epoch 25/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2953 - acc: 0.8788 - val_loss: 0.3624 - val_acc: 0.8301\n",
            "Epoch 26/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2951 - acc: 0.8800 - val_loss: 0.3567 - val_acc: 0.8332\n",
            "Epoch 27/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8799 - val_loss: 0.3631 - val_acc: 0.8235\n",
            "Epoch 28/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2951 - acc: 0.8798 - val_loss: 0.3625 - val_acc: 0.8275\n",
            "Epoch 29/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8790 - val_loss: 0.3401 - val_acc: 0.8452\n",
            "Epoch 30/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2949 - acc: 0.8794 - val_loss: 0.3664 - val_acc: 0.8257\n",
            "Epoch 31/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8798 - val_loss: 0.3444 - val_acc: 0.8365\n",
            "Epoch 32/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2945 - acc: 0.8801 - val_loss: 0.3658 - val_acc: 0.8359\n",
            "Epoch 33/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2943 - acc: 0.8807 - val_loss: 0.3561 - val_acc: 0.8370\n",
            "Epoch 34/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8811 - val_loss: 0.3343 - val_acc: 0.8487\n",
            "Epoch 35/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8810 - val_loss: 0.3621 - val_acc: 0.8368\n",
            "Epoch 36/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8811 - val_loss: 0.3611 - val_acc: 0.8323\n",
            "Epoch 37/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2938 - acc: 0.8807 - val_loss: 0.3486 - val_acc: 0.8414\n",
            "Epoch 38/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2935 - acc: 0.8811 - val_loss: 0.3401 - val_acc: 0.8407\n",
            "Epoch 39/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8808 - val_loss: 0.3539 - val_acc: 0.8315\n",
            "Epoch 40/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8801 - val_loss: 0.3446 - val_acc: 0.8430\n",
            "Epoch 41/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2928 - acc: 0.8806 - val_loss: 0.3628 - val_acc: 0.8266\n",
            "Epoch 42/150\n",
            "71858/71858 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8798 - val_loss: 0.3546 - val_acc: 0.8374\n",
            "Epoch 43/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2929 - acc: 0.8814 - val_loss: 0.3455 - val_acc: 0.8390\n",
            "Epoch 44/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2928 - acc: 0.8807 - val_loss: 0.3711 - val_acc: 0.8277\n",
            "Epoch 45/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2928 - acc: 0.8814 - val_loss: 0.3757 - val_acc: 0.8250\n",
            "Epoch 46/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2925 - acc: 0.8811 - val_loss: 0.3373 - val_acc: 0.8430\n",
            "Epoch 47/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2928 - acc: 0.8814 - val_loss: 0.3609 - val_acc: 0.8388\n",
            "Epoch 48/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2930 - acc: 0.8810 - val_loss: 0.3702 - val_acc: 0.8334\n",
            "Epoch 49/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2926 - acc: 0.8809 - val_loss: 0.3671 - val_acc: 0.8219\n",
            "Epoch 50/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2928 - acc: 0.8814 - val_loss: 0.3463 - val_acc: 0.8396\n",
            "Epoch 51/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2924 - acc: 0.8814 - val_loss: 0.3745 - val_acc: 0.8284\n",
            "Epoch 52/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2926 - acc: 0.8820 - val_loss: 0.3119 - val_acc: 0.8536\n",
            "Epoch 53/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2927 - acc: 0.8807 - val_loss: 0.3637 - val_acc: 0.8286\n",
            "Epoch 54/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2926 - acc: 0.8818 - val_loss: 0.3571 - val_acc: 0.8345\n",
            "Epoch 55/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2923 - acc: 0.8808 - val_loss: 0.3592 - val_acc: 0.8310\n",
            "Epoch 56/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2920 - acc: 0.8824 - val_loss: 0.3781 - val_acc: 0.8226\n",
            "Epoch 57/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2928 - acc: 0.8811 - val_loss: 0.3523 - val_acc: 0.8295\n",
            "Epoch 58/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8816 - val_loss: 0.3252 - val_acc: 0.8496\n",
            "Epoch 59/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2916 - acc: 0.8819 - val_loss: 0.3506 - val_acc: 0.8343\n",
            "Epoch 60/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2919 - acc: 0.8823 - val_loss: 0.3396 - val_acc: 0.8441\n",
            "Epoch 61/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2917 - acc: 0.8838 - val_loss: 0.3537 - val_acc: 0.8376\n",
            "Epoch 62/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2912 - acc: 0.8837 - val_loss: 0.3702 - val_acc: 0.8352\n",
            "Epoch 63/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8840 - val_loss: 0.3572 - val_acc: 0.8407\n",
            "Epoch 64/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8843 - val_loss: 0.3690 - val_acc: 0.8368\n",
            "Epoch 65/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8841 - val_loss: 0.3709 - val_acc: 0.8354\n",
            "Epoch 66/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8838 - val_loss: 0.3666 - val_acc: 0.8345\n",
            "Epoch 67/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8848 - val_loss: 0.3722 - val_acc: 0.8348\n",
            "Epoch 68/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2903 - acc: 0.8838 - val_loss: 0.3270 - val_acc: 0.8452\n",
            "Epoch 69/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2911 - acc: 0.8841 - val_loss: 0.3542 - val_acc: 0.8396\n",
            "Epoch 70/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8845 - val_loss: 0.3605 - val_acc: 0.8365\n",
            "Epoch 71/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8844 - val_loss: 0.3481 - val_acc: 0.8454\n",
            "Epoch 72/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8856 - val_loss: 0.3401 - val_acc: 0.8443\n",
            "Epoch 73/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2909 - acc: 0.8848 - val_loss: 0.3539 - val_acc: 0.8401\n",
            "Epoch 74/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8852 - val_loss: 0.3403 - val_acc: 0.8436\n",
            "Epoch 75/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8854 - val_loss: 0.3470 - val_acc: 0.8423\n",
            "Epoch 76/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8847 - val_loss: 0.3471 - val_acc: 0.8416\n",
            "Epoch 77/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8839 - val_loss: 0.3465 - val_acc: 0.8407\n",
            "Epoch 78/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8851 - val_loss: 0.3669 - val_acc: 0.8323\n",
            "Epoch 79/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8853 - val_loss: 0.3528 - val_acc: 0.8401\n",
            "Epoch 80/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8845 - val_loss: 0.3484 - val_acc: 0.8412\n",
            "Epoch 81/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8840 - val_loss: 0.3409 - val_acc: 0.8496\n",
            "Epoch 82/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8849 - val_loss: 0.3272 - val_acc: 0.8483\n",
            "Epoch 83/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3585 - val_acc: 0.8381\n",
            "Epoch 84/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3634 - val_acc: 0.8365\n",
            "Epoch 85/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8850 - val_loss: 0.3688 - val_acc: 0.8323\n",
            "Epoch 86/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8850 - val_loss: 0.3690 - val_acc: 0.8365\n",
            "Epoch 87/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8850 - val_loss: 0.3665 - val_acc: 0.8330\n",
            "Epoch 88/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8852 - val_loss: 0.3316 - val_acc: 0.8449\n",
            "Epoch 89/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8852 - val_loss: 0.3405 - val_acc: 0.8412\n",
            "Epoch 90/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8845 - val_loss: 0.3441 - val_acc: 0.8399\n",
            "Epoch 91/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8851 - val_loss: 0.3537 - val_acc: 0.8407\n",
            "Epoch 92/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3487 - val_acc: 0.8500\n",
            "Epoch 93/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8852 - val_loss: 0.3659 - val_acc: 0.8354\n",
            "Epoch 94/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8852 - val_loss: 0.3508 - val_acc: 0.8396\n",
            "Epoch 95/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8858 - val_loss: 0.3373 - val_acc: 0.8452\n",
            "Epoch 96/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8859 - val_loss: 0.3851 - val_acc: 0.8275\n",
            "Epoch 97/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3610 - val_acc: 0.8394\n",
            "Epoch 98/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8848 - val_loss: 0.3562 - val_acc: 0.8423\n",
            "Epoch 99/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8857 - val_loss: 0.3531 - val_acc: 0.8321\n",
            "Epoch 100/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8857 - val_loss: 0.3403 - val_acc: 0.8436\n",
            "Epoch 101/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8847 - val_loss: 0.3760 - val_acc: 0.8297\n",
            "Epoch 102/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8860 - val_loss: 0.3472 - val_acc: 0.8361\n",
            "Epoch 103/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8855 - val_loss: 0.3394 - val_acc: 0.8478\n",
            "Epoch 104/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8855 - val_loss: 0.3282 - val_acc: 0.8456\n",
            "Epoch 105/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8852 - val_loss: 0.3367 - val_acc: 0.8467\n",
            "Epoch 106/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8847 - val_loss: 0.3395 - val_acc: 0.8447\n",
            "Epoch 107/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8850 - val_loss: 0.3601 - val_acc: 0.8341\n",
            "Epoch 108/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8856 - val_loss: 0.3505 - val_acc: 0.8370\n",
            "Epoch 109/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8855 - val_loss: 0.3425 - val_acc: 0.8476\n",
            "Epoch 110/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8845 - val_loss: 0.3610 - val_acc: 0.8363\n",
            "Epoch 111/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8857 - val_loss: 0.3703 - val_acc: 0.8339\n",
            "Epoch 112/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8861 - val_loss: 0.3771 - val_acc: 0.8359\n",
            "Epoch 113/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8852 - val_loss: 0.3453 - val_acc: 0.8436\n",
            "Epoch 114/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8856 - val_loss: 0.3511 - val_acc: 0.8447\n",
            "Epoch 115/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8862 - val_loss: 0.3575 - val_acc: 0.8407\n",
            "Epoch 116/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8858 - val_loss: 0.3327 - val_acc: 0.8476\n",
            "Epoch 117/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8846 - val_loss: 0.3470 - val_acc: 0.8434\n",
            "Epoch 118/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8852 - val_loss: 0.3505 - val_acc: 0.8385\n",
            "Epoch 119/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8856 - val_loss: 0.3557 - val_acc: 0.8401\n",
            "Epoch 120/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8856 - val_loss: 0.3551 - val_acc: 0.8381\n",
            "Epoch 121/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8852 - val_loss: 0.3634 - val_acc: 0.8330\n",
            "Epoch 122/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8853 - val_loss: 0.3724 - val_acc: 0.8326\n",
            "Epoch 123/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3478 - val_acc: 0.8423\n",
            "Epoch 124/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8854 - val_loss: 0.3576 - val_acc: 0.8374\n",
            "Epoch 125/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8868 - val_loss: 0.3746 - val_acc: 0.8297\n",
            "Epoch 126/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3711 - val_acc: 0.8310\n",
            "Epoch 127/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8856 - val_loss: 0.3536 - val_acc: 0.8383\n",
            "Epoch 128/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8860 - val_loss: 0.3664 - val_acc: 0.8350\n",
            "Epoch 129/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8859 - val_loss: 0.3518 - val_acc: 0.8445\n",
            "Epoch 130/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3535 - val_acc: 0.8463\n",
            "Epoch 131/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8851 - val_loss: 0.3464 - val_acc: 0.8438\n",
            "Epoch 132/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8862 - val_loss: 0.3612 - val_acc: 0.8363\n",
            "Epoch 133/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2884 - acc: 0.8861 - val_loss: 0.3461 - val_acc: 0.8430\n",
            "Epoch 134/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2880 - acc: 0.8857 - val_loss: 0.3524 - val_acc: 0.8414\n",
            "Epoch 135/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8856 - val_loss: 0.3659 - val_acc: 0.8359\n",
            "Epoch 136/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8849 - val_loss: 0.3473 - val_acc: 0.8394\n",
            "Epoch 137/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3647 - val_acc: 0.8301\n",
            "Epoch 138/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8856 - val_loss: 0.3528 - val_acc: 0.8421\n",
            "Epoch 139/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3557 - val_acc: 0.8381\n",
            "Epoch 140/150\n",
            "71858/71858 [==============================] - 1s 18us/step - loss: 0.2881 - acc: 0.8859 - val_loss: 0.3385 - val_acc: 0.8474\n",
            "Epoch 141/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8851 - val_loss: 0.3373 - val_acc: 0.8432\n",
            "Epoch 142/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8858 - val_loss: 0.3771 - val_acc: 0.8323\n",
            "Epoch 143/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8858 - val_loss: 0.3566 - val_acc: 0.8394\n",
            "Epoch 144/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8850 - val_loss: 0.3577 - val_acc: 0.8405\n",
            "Epoch 145/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8853 - val_loss: 0.3425 - val_acc: 0.8412\n",
            "Epoch 146/150\n",
            "71858/71858 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8859 - val_loss: 0.3663 - val_acc: 0.8381\n",
            "Epoch 147/150\n",
            "71858/71858 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8857 - val_loss: 0.3371 - val_acc: 0.8461\n",
            "Epoch 148/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8853 - val_loss: 0.3583 - val_acc: 0.8394\n",
            "Epoch 149/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2879 - acc: 0.8860 - val_loss: 0.3727 - val_acc: 0.8350\n",
            "Epoch 150/150\n",
            "71858/71858 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8859 - val_loss: 0.3651 - val_acc: 0.8343\n",
            "acc: 83.43%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 1s 20us/step - loss: 0.3556 - acc: 0.8503 - val_loss: 0.3425 - val_acc: 0.8394\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 14us/step - loss: 0.3118 - acc: 0.8717 - val_loss: 0.3344 - val_acc: 0.8396\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3014 - acc: 0.8770 - val_loss: 0.3482 - val_acc: 0.8354\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2978 - acc: 0.8795 - val_loss: 0.3237 - val_acc: 0.8461\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8789 - val_loss: 0.3160 - val_acc: 0.8518\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 14us/step - loss: 0.2946 - acc: 0.8800 - val_loss: 0.3343 - val_acc: 0.8465\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2940 - acc: 0.8805 - val_loss: 0.3414 - val_acc: 0.8385\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2935 - acc: 0.8807 - val_loss: 0.3070 - val_acc: 0.8560\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2926 - acc: 0.8811 - val_loss: 0.3223 - val_acc: 0.8498\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2918 - acc: 0.8812 - val_loss: 0.3104 - val_acc: 0.8558\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2918 - acc: 0.8821 - val_loss: 0.3224 - val_acc: 0.8449\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2910 - acc: 0.8823 - val_loss: 0.3287 - val_acc: 0.8483\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2900 - acc: 0.8827 - val_loss: 0.3500 - val_acc: 0.8390\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2904 - acc: 0.8826 - val_loss: 0.3084 - val_acc: 0.8593\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8837 - val_loss: 0.2993 - val_acc: 0.8514\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8832 - val_loss: 0.3084 - val_acc: 0.8571\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2901 - acc: 0.8833 - val_loss: 0.3203 - val_acc: 0.8514\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2895 - acc: 0.8839 - val_loss: 0.3399 - val_acc: 0.8425\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2899 - acc: 0.8834 - val_loss: 0.3264 - val_acc: 0.8489\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2896 - acc: 0.8828 - val_loss: 0.3288 - val_acc: 0.8487\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8846 - val_loss: 0.3311 - val_acc: 0.8418\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2898 - acc: 0.8837 - val_loss: 0.3353 - val_acc: 0.8416\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2893 - acc: 0.8842 - val_loss: 0.3033 - val_acc: 0.8604\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8840 - val_loss: 0.3168 - val_acc: 0.8503\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8835 - val_loss: 0.3247 - val_acc: 0.8516\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8833 - val_loss: 0.2989 - val_acc: 0.8613\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8829 - val_loss: 0.3080 - val_acc: 0.8536\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8839 - val_loss: 0.3262 - val_acc: 0.8491\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8840 - val_loss: 0.3296 - val_acc: 0.8452\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8845 - val_loss: 0.3118 - val_acc: 0.8545\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2897 - acc: 0.8836 - val_loss: 0.3311 - val_acc: 0.8467\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8838 - val_loss: 0.3337 - val_acc: 0.8430\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3281 - val_acc: 0.8476\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3507 - val_acc: 0.8396\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8846 - val_loss: 0.3002 - val_acc: 0.8653\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8839 - val_loss: 0.3160 - val_acc: 0.8551\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8841 - val_loss: 0.3321 - val_acc: 0.8469\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8854 - val_loss: 0.3204 - val_acc: 0.8542\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8842 - val_loss: 0.3165 - val_acc: 0.8516\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8847 - val_loss: 0.3341 - val_acc: 0.8465\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8842 - val_loss: 0.3080 - val_acc: 0.8602\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8831 - val_loss: 0.3295 - val_acc: 0.8483\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8842 - val_loss: 0.3278 - val_acc: 0.8489\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8838 - val_loss: 0.3263 - val_acc: 0.8461\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8844 - val_loss: 0.3448 - val_acc: 0.8361\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3145 - val_acc: 0.8514\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8842 - val_loss: 0.3395 - val_acc: 0.8452\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8840 - val_loss: 0.3329 - val_acc: 0.8472\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8838 - val_loss: 0.3298 - val_acc: 0.8540\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8854 - val_loss: 0.3211 - val_acc: 0.8531\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3125 - val_acc: 0.8571\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8850 - val_loss: 0.3496 - val_acc: 0.8350\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8850 - val_loss: 0.3398 - val_acc: 0.8421\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3034 - val_acc: 0.8571\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8853 - val_loss: 0.3231 - val_acc: 0.8443\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2893 - acc: 0.8842 - val_loss: 0.3491 - val_acc: 0.8388\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8845 - val_loss: 0.3362 - val_acc: 0.8430\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8844 - val_loss: 0.3349 - val_acc: 0.8474\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3177 - val_acc: 0.8485\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8846 - val_loss: 0.3225 - val_acc: 0.8531\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8847 - val_loss: 0.3114 - val_acc: 0.8571\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8845 - val_loss: 0.3312 - val_acc: 0.8478\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8857 - val_loss: 0.3160 - val_acc: 0.8531\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8842 - val_loss: 0.3219 - val_acc: 0.8496\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3146 - val_acc: 0.8500\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8845 - val_loss: 0.3226 - val_acc: 0.8491\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8847 - val_loss: 0.3172 - val_acc: 0.8551\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2892 - acc: 0.8843 - val_loss: 0.3154 - val_acc: 0.8520\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3215 - val_acc: 0.8518\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8843 - val_loss: 0.3309 - val_acc: 0.8483\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8847 - val_loss: 0.3249 - val_acc: 0.8478\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8843 - val_loss: 0.3206 - val_acc: 0.8516\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8847 - val_loss: 0.3535 - val_acc: 0.8381\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8845 - val_loss: 0.3260 - val_acc: 0.8496\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8849 - val_loss: 0.3233 - val_acc: 0.8507\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8848 - val_loss: 0.3146 - val_acc: 0.8483\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3170 - val_acc: 0.8551\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3319 - val_acc: 0.8503\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8840 - val_loss: 0.3419 - val_acc: 0.8465\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8842 - val_loss: 0.3479 - val_acc: 0.8376\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8842 - val_loss: 0.3396 - val_acc: 0.8352\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2883 - acc: 0.8844 - val_loss: 0.3090 - val_acc: 0.8545\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8855 - val_loss: 0.3440 - val_acc: 0.8412\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8850 - val_loss: 0.3138 - val_acc: 0.8527\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2888 - acc: 0.8844 - val_loss: 0.3301 - val_acc: 0.8503\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8844 - val_loss: 0.3099 - val_acc: 0.8564\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8847 - val_loss: 0.3295 - val_acc: 0.8425\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8852 - val_loss: 0.3103 - val_acc: 0.8534\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8854 - val_loss: 0.3397 - val_acc: 0.8399\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8848 - val_loss: 0.3228 - val_acc: 0.8489\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8844 - val_loss: 0.3255 - val_acc: 0.8529\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8845 - val_loss: 0.3017 - val_acc: 0.8602\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8848 - val_loss: 0.3344 - val_acc: 0.8476\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8838 - val_loss: 0.3451 - val_acc: 0.8352\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8849 - val_loss: 0.3519 - val_acc: 0.8436\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8848 - val_loss: 0.3187 - val_acc: 0.8540\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8845 - val_loss: 0.3435 - val_acc: 0.8447\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8847 - val_loss: 0.3126 - val_acc: 0.8529\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8846 - val_loss: 0.3234 - val_acc: 0.8520\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8847 - val_loss: 0.3274 - val_acc: 0.8509\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8852 - val_loss: 0.3238 - val_acc: 0.8540\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3016 - val_acc: 0.8600\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8839 - val_loss: 0.3266 - val_acc: 0.8449\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8844 - val_loss: 0.3282 - val_acc: 0.8480\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8834 - val_loss: 0.3354 - val_acc: 0.8434\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2880 - acc: 0.8849 - val_loss: 0.3111 - val_acc: 0.8545\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8843 - val_loss: 0.3434 - val_acc: 0.8423\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8844 - val_loss: 0.3163 - val_acc: 0.8582\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8841 - val_loss: 0.3084 - val_acc: 0.8540\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3126 - val_acc: 0.8516\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8838 - val_loss: 0.3465 - val_acc: 0.8352\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8848 - val_loss: 0.3381 - val_acc: 0.8441\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8839 - val_loss: 0.3348 - val_acc: 0.8483\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8846 - val_loss: 0.3068 - val_acc: 0.8564\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8854 - val_loss: 0.3462 - val_acc: 0.8392\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8849 - val_loss: 0.3293 - val_acc: 0.8489\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8841 - val_loss: 0.3175 - val_acc: 0.8534\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8838 - val_loss: 0.3334 - val_acc: 0.8456\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8852 - val_loss: 0.3123 - val_acc: 0.8551\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8835 - val_loss: 0.3085 - val_acc: 0.8540\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8846 - val_loss: 0.3359 - val_acc: 0.8489\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8842 - val_loss: 0.3201 - val_acc: 0.8511\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3336 - val_acc: 0.8474\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8842 - val_loss: 0.3291 - val_acc: 0.8427\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8858 - val_loss: 0.3221 - val_acc: 0.8500\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8849 - val_loss: 0.3145 - val_acc: 0.8494\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8839 - val_loss: 0.3260 - val_acc: 0.8531\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8843 - val_loss: 0.3331 - val_acc: 0.8467\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8849 - val_loss: 0.3048 - val_acc: 0.8580\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8845 - val_loss: 0.3297 - val_acc: 0.8478\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3429 - val_acc: 0.8345\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3375 - val_acc: 0.8463\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8841 - val_loss: 0.3322 - val_acc: 0.8509\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8842 - val_loss: 0.3057 - val_acc: 0.8584\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2888 - acc: 0.8838 - val_loss: 0.3419 - val_acc: 0.8410\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3149 - val_acc: 0.8562\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2891 - acc: 0.8852 - val_loss: 0.3028 - val_acc: 0.8584\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2880 - acc: 0.8848 - val_loss: 0.3311 - val_acc: 0.8441\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8850 - val_loss: 0.3267 - val_acc: 0.8483\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8849 - val_loss: 0.3221 - val_acc: 0.8487\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2882 - acc: 0.8844 - val_loss: 0.3262 - val_acc: 0.8476\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2885 - acc: 0.8851 - val_loss: 0.3282 - val_acc: 0.8465\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2886 - acc: 0.8847 - val_loss: 0.3337 - val_acc: 0.8427\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8848 - val_loss: 0.3448 - val_acc: 0.8430\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8843 - val_loss: 0.3290 - val_acc: 0.8483\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8847 - val_loss: 0.3105 - val_acc: 0.8573\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8842 - val_loss: 0.3272 - val_acc: 0.8494\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8850 - val_loss: 0.3102 - val_acc: 0.8556\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8847 - val_loss: 0.3158 - val_acc: 0.8516\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8839 - val_loss: 0.3384 - val_acc: 0.8423\n",
            "acc: 84.23%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 21us/step - loss: 0.3491 - acc: 0.8537 - val_loss: 0.3614 - val_acc: 0.8284\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3129 - acc: 0.8722 - val_loss: 0.3719 - val_acc: 0.8321\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3076 - acc: 0.8752 - val_loss: 0.3530 - val_acc: 0.8374\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3045 - acc: 0.8760 - val_loss: 0.3375 - val_acc: 0.8385\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3032 - acc: 0.8764 - val_loss: 0.3370 - val_acc: 0.8421\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3018 - acc: 0.8766 - val_loss: 0.3145 - val_acc: 0.8463\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3022 - acc: 0.8772 - val_loss: 0.3435 - val_acc: 0.8328\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3011 - acc: 0.8774 - val_loss: 0.3359 - val_acc: 0.8418\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.3010 - acc: 0.8780 - val_loss: 0.3218 - val_acc: 0.8452\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3004 - acc: 0.8786 - val_loss: 0.3508 - val_acc: 0.8350\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3007 - acc: 0.8769 - val_loss: 0.3537 - val_acc: 0.8365\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3004 - acc: 0.8778 - val_loss: 0.3315 - val_acc: 0.8361\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2998 - acc: 0.8786 - val_loss: 0.3503 - val_acc: 0.8368\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2999 - acc: 0.8785 - val_loss: 0.3398 - val_acc: 0.8427\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2998 - acc: 0.8782 - val_loss: 0.3471 - val_acc: 0.8359\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2996 - acc: 0.8787 - val_loss: 0.3160 - val_acc: 0.8452\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2994 - acc: 0.8785 - val_loss: 0.3396 - val_acc: 0.8392\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2993 - acc: 0.8791 - val_loss: 0.3684 - val_acc: 0.8321\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2993 - acc: 0.8785 - val_loss: 0.3187 - val_acc: 0.8469\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8789 - val_loss: 0.3202 - val_acc: 0.8465\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8787 - val_loss: 0.3426 - val_acc: 0.8394\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2983 - acc: 0.8779 - val_loss: 0.3181 - val_acc: 0.8456\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8792 - val_loss: 0.3410 - val_acc: 0.8441\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8787 - val_loss: 0.3625 - val_acc: 0.8259\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2982 - acc: 0.8791 - val_loss: 0.3267 - val_acc: 0.8452\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8787 - val_loss: 0.3402 - val_acc: 0.8423\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8791 - val_loss: 0.3178 - val_acc: 0.8463\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8791 - val_loss: 0.3399 - val_acc: 0.8372\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8788 - val_loss: 0.3330 - val_acc: 0.8418\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8781 - val_loss: 0.3319 - val_acc: 0.8423\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8788 - val_loss: 0.3237 - val_acc: 0.8469\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8789 - val_loss: 0.3414 - val_acc: 0.8412\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2984 - acc: 0.8792 - val_loss: 0.3264 - val_acc: 0.8410\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8792 - val_loss: 0.3307 - val_acc: 0.8461\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2985 - acc: 0.8788 - val_loss: 0.3214 - val_acc: 0.8480\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8790 - val_loss: 0.3333 - val_acc: 0.8469\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8788 - val_loss: 0.3235 - val_acc: 0.8449\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8788 - val_loss: 0.3285 - val_acc: 0.8456\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8785 - val_loss: 0.3177 - val_acc: 0.8511\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2980 - acc: 0.8787 - val_loss: 0.3485 - val_acc: 0.8354\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8787 - val_loss: 0.3638 - val_acc: 0.8317\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8782 - val_loss: 0.3321 - val_acc: 0.8423\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8786 - val_loss: 0.3351 - val_acc: 0.8425\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8783 - val_loss: 0.3299 - val_acc: 0.8454\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2976 - acc: 0.8789 - val_loss: 0.3297 - val_acc: 0.8427\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8790 - val_loss: 0.3358 - val_acc: 0.8474\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2983 - acc: 0.8788 - val_loss: 0.3538 - val_acc: 0.8328\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2982 - acc: 0.8790 - val_loss: 0.3132 - val_acc: 0.8527\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8790 - val_loss: 0.3354 - val_acc: 0.8438\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2982 - acc: 0.8783 - val_loss: 0.3459 - val_acc: 0.8405\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2979 - acc: 0.8796 - val_loss: 0.3386 - val_acc: 0.8418\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8799 - val_loss: 0.3282 - val_acc: 0.8480\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8797 - val_loss: 0.3290 - val_acc: 0.8465\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8785 - val_loss: 0.3382 - val_acc: 0.8454\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8794 - val_loss: 0.3347 - val_acc: 0.8461\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2979 - acc: 0.8797 - val_loss: 0.3373 - val_acc: 0.8432\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8788 - val_loss: 0.3442 - val_acc: 0.8401\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8789 - val_loss: 0.3139 - val_acc: 0.8494\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2981 - acc: 0.8790 - val_loss: 0.3299 - val_acc: 0.8461\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8792 - val_loss: 0.3186 - val_acc: 0.8507\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2981 - acc: 0.8786 - val_loss: 0.3531 - val_acc: 0.8372\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2973 - acc: 0.8786 - val_loss: 0.3393 - val_acc: 0.8410\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2974 - acc: 0.8790 - val_loss: 0.3423 - val_acc: 0.8376\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2973 - acc: 0.8791 - val_loss: 0.3331 - val_acc: 0.8421\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2972 - acc: 0.8786 - val_loss: 0.3396 - val_acc: 0.8414\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2972 - acc: 0.8796 - val_loss: 0.3250 - val_acc: 0.8463\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2978 - acc: 0.8790 - val_loss: 0.3403 - val_acc: 0.8421\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2976 - acc: 0.8784 - val_loss: 0.3241 - val_acc: 0.8430\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8785 - val_loss: 0.3459 - val_acc: 0.8372\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8778 - val_loss: 0.3554 - val_acc: 0.8370\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8797 - val_loss: 0.3359 - val_acc: 0.8370\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8794 - val_loss: 0.3284 - val_acc: 0.8507\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8786 - val_loss: 0.3397 - val_acc: 0.8388\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2978 - acc: 0.8790 - val_loss: 0.3333 - val_acc: 0.8423\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2979 - acc: 0.8785 - val_loss: 0.3342 - val_acc: 0.8447\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2975 - acc: 0.8793 - val_loss: 0.3426 - val_acc: 0.8365\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2971 - acc: 0.8788 - val_loss: 0.3405 - val_acc: 0.8425\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2976 - acc: 0.8792 - val_loss: 0.3360 - val_acc: 0.8425\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2977 - acc: 0.8794 - val_loss: 0.3376 - val_acc: 0.8438\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8786 - val_loss: 0.3223 - val_acc: 0.8503\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2974 - acc: 0.8792 - val_loss: 0.3409 - val_acc: 0.8403\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8789 - val_loss: 0.3333 - val_acc: 0.8390\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8797 - val_loss: 0.3352 - val_acc: 0.8416\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2975 - acc: 0.8793 - val_loss: 0.3318 - val_acc: 0.8441\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8789 - val_loss: 0.3221 - val_acc: 0.8496\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2970 - acc: 0.8788 - val_loss: 0.3272 - val_acc: 0.8500\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8800 - val_loss: 0.3232 - val_acc: 0.8476\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8788 - val_loss: 0.3377 - val_acc: 0.8445\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2973 - acc: 0.8793 - val_loss: 0.3265 - val_acc: 0.8489\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2973 - acc: 0.8791 - val_loss: 0.3354 - val_acc: 0.8412\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8793 - val_loss: 0.3390 - val_acc: 0.8434\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2970 - acc: 0.8795 - val_loss: 0.3258 - val_acc: 0.8496\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8792 - val_loss: 0.3222 - val_acc: 0.8480\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8788 - val_loss: 0.3359 - val_acc: 0.8478\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8797 - val_loss: 0.3299 - val_acc: 0.8478\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2971 - acc: 0.8791 - val_loss: 0.3145 - val_acc: 0.8549\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8797 - val_loss: 0.3416 - val_acc: 0.8423\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8792 - val_loss: 0.3239 - val_acc: 0.8474\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8796 - val_loss: 0.3358 - val_acc: 0.8456\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8791 - val_loss: 0.3365 - val_acc: 0.8465\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8797 - val_loss: 0.3366 - val_acc: 0.8445\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2968 - acc: 0.8798 - val_loss: 0.3293 - val_acc: 0.8480\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8788 - val_loss: 0.3249 - val_acc: 0.8472\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2969 - acc: 0.8793 - val_loss: 0.3252 - val_acc: 0.8496\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8798 - val_loss: 0.3356 - val_acc: 0.8443\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8800 - val_loss: 0.3443 - val_acc: 0.8376\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2970 - acc: 0.8796 - val_loss: 0.3274 - val_acc: 0.8476\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8785 - val_loss: 0.3317 - val_acc: 0.8480\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8802 - val_loss: 0.3562 - val_acc: 0.8381\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8798 - val_loss: 0.3129 - val_acc: 0.8511\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2969 - acc: 0.8795 - val_loss: 0.3206 - val_acc: 0.8507\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8798 - val_loss: 0.3218 - val_acc: 0.8491\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2962 - acc: 0.8800 - val_loss: 0.3056 - val_acc: 0.8560\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2968 - acc: 0.8796 - val_loss: 0.3442 - val_acc: 0.8418\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8799 - val_loss: 0.3268 - val_acc: 0.8443\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8795 - val_loss: 0.3433 - val_acc: 0.8427\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2970 - acc: 0.8791 - val_loss: 0.3433 - val_acc: 0.8452\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8791 - val_loss: 0.3152 - val_acc: 0.8507\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2962 - acc: 0.8808 - val_loss: 0.3248 - val_acc: 0.8489\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8801 - val_loss: 0.3357 - val_acc: 0.8465\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2972 - acc: 0.8792 - val_loss: 0.3334 - val_acc: 0.8489\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8801 - val_loss: 0.3353 - val_acc: 0.8452\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8798 - val_loss: 0.3315 - val_acc: 0.8427\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8798 - val_loss: 0.3406 - val_acc: 0.8445\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2962 - acc: 0.8799 - val_loss: 0.3276 - val_acc: 0.8449\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8794 - val_loss: 0.3341 - val_acc: 0.8434\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8796 - val_loss: 0.3233 - val_acc: 0.8476\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2964 - acc: 0.8801 - val_loss: 0.3142 - val_acc: 0.8549\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8798 - val_loss: 0.3323 - val_acc: 0.8476\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2967 - acc: 0.8802 - val_loss: 0.3358 - val_acc: 0.8474\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2965 - acc: 0.8797 - val_loss: 0.3185 - val_acc: 0.8507\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8799 - val_loss: 0.3359 - val_acc: 0.8432\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8785 - val_loss: 0.3362 - val_acc: 0.8447\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2965 - acc: 0.8800 - val_loss: 0.3195 - val_acc: 0.8503\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2967 - acc: 0.8801 - val_loss: 0.3353 - val_acc: 0.8430\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8802 - val_loss: 0.3338 - val_acc: 0.8449\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2967 - acc: 0.8795 - val_loss: 0.3228 - val_acc: 0.8476\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2970 - acc: 0.8798 - val_loss: 0.3398 - val_acc: 0.8412\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8787 - val_loss: 0.3351 - val_acc: 0.8472\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2968 - acc: 0.8795 - val_loss: 0.3410 - val_acc: 0.8423\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2966 - acc: 0.8798 - val_loss: 0.3343 - val_acc: 0.8500\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2959 - acc: 0.8802 - val_loss: 0.3308 - val_acc: 0.8469\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8799 - val_loss: 0.3257 - val_acc: 0.8469\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2965 - acc: 0.8802 - val_loss: 0.3438 - val_acc: 0.8396\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8806 - val_loss: 0.3321 - val_acc: 0.8376\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2961 - acc: 0.8796 - val_loss: 0.3377 - val_acc: 0.8432\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2969 - acc: 0.8797 - val_loss: 0.3290 - val_acc: 0.8452\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2964 - acc: 0.8799 - val_loss: 0.3314 - val_acc: 0.8456\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8800 - val_loss: 0.3338 - val_acc: 0.8441\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8799 - val_loss: 0.3465 - val_acc: 0.8476\n",
            "acc: 84.76%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 22us/step - loss: 0.3547 - acc: 0.8497 - val_loss: 0.4038 - val_acc: 0.8122\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3152 - acc: 0.8712 - val_loss: 0.3738 - val_acc: 0.8186\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3073 - acc: 0.8750 - val_loss: 0.3495 - val_acc: 0.8343\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3036 - acc: 0.8765 - val_loss: 0.3534 - val_acc: 0.8310\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3028 - acc: 0.8769 - val_loss: 0.3506 - val_acc: 0.8385\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3010 - acc: 0.8770 - val_loss: 0.3314 - val_acc: 0.8423\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3008 - acc: 0.8778 - val_loss: 0.3532 - val_acc: 0.8326\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3001 - acc: 0.8783 - val_loss: 0.3578 - val_acc: 0.8312\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2993 - acc: 0.8783 - val_loss: 0.3381 - val_acc: 0.8381\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2995 - acc: 0.8776 - val_loss: 0.3760 - val_acc: 0.8264\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3000 - acc: 0.8778 - val_loss: 0.3466 - val_acc: 0.8383\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8788 - val_loss: 0.3710 - val_acc: 0.8259\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2992 - acc: 0.8782 - val_loss: 0.3404 - val_acc: 0.8368\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8786 - val_loss: 0.3563 - val_acc: 0.8279\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8790 - val_loss: 0.3611 - val_acc: 0.8290\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2985 - acc: 0.8790 - val_loss: 0.3566 - val_acc: 0.8392\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2983 - acc: 0.8780 - val_loss: 0.3970 - val_acc: 0.8197\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2980 - acc: 0.8787 - val_loss: 0.3543 - val_acc: 0.8319\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2986 - acc: 0.8782 - val_loss: 0.3300 - val_acc: 0.8467\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8786 - val_loss: 0.3621 - val_acc: 0.8321\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2978 - acc: 0.8788 - val_loss: 0.3286 - val_acc: 0.8443\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2972 - acc: 0.8799 - val_loss: 0.3357 - val_acc: 0.8376\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8785 - val_loss: 0.3494 - val_acc: 0.8350\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2979 - acc: 0.8789 - val_loss: 0.3389 - val_acc: 0.8407\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2977 - acc: 0.8785 - val_loss: 0.3352 - val_acc: 0.8447\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2974 - acc: 0.8793 - val_loss: 0.3502 - val_acc: 0.8392\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2975 - acc: 0.8794 - val_loss: 0.3502 - val_acc: 0.8312\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2972 - acc: 0.8799 - val_loss: 0.3388 - val_acc: 0.8388\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2976 - acc: 0.8796 - val_loss: 0.3605 - val_acc: 0.8321\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2968 - acc: 0.8798 - val_loss: 0.3308 - val_acc: 0.8388\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8797 - val_loss: 0.3471 - val_acc: 0.8390\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2963 - acc: 0.8804 - val_loss: 0.3505 - val_acc: 0.8319\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2961 - acc: 0.8808 - val_loss: 0.3563 - val_acc: 0.8372\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8807 - val_loss: 0.3748 - val_acc: 0.8328\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2957 - acc: 0.8807 - val_loss: 0.3415 - val_acc: 0.8352\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2957 - acc: 0.8805 - val_loss: 0.3644 - val_acc: 0.8299\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8804 - val_loss: 0.3503 - val_acc: 0.8372\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2957 - acc: 0.8805 - val_loss: 0.3569 - val_acc: 0.8303\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2953 - acc: 0.8807 - val_loss: 0.3489 - val_acc: 0.8357\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8809 - val_loss: 0.3230 - val_acc: 0.8516\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2952 - acc: 0.8813 - val_loss: 0.3379 - val_acc: 0.8407\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2950 - acc: 0.8811 - val_loss: 0.3630 - val_acc: 0.8326\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2955 - acc: 0.8810 - val_loss: 0.3408 - val_acc: 0.8432\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2955 - acc: 0.8814 - val_loss: 0.3533 - val_acc: 0.8334\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2944 - acc: 0.8822 - val_loss: 0.3581 - val_acc: 0.8341\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8815 - val_loss: 0.3641 - val_acc: 0.8284\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2951 - acc: 0.8822 - val_loss: 0.3337 - val_acc: 0.8412\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2949 - acc: 0.8819 - val_loss: 0.3783 - val_acc: 0.8370\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8812 - val_loss: 0.3311 - val_acc: 0.8507\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2948 - acc: 0.8824 - val_loss: 0.3295 - val_acc: 0.8485\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2946 - acc: 0.8826 - val_loss: 0.3525 - val_acc: 0.8368\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2949 - acc: 0.8818 - val_loss: 0.3435 - val_acc: 0.8399\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8826 - val_loss: 0.3689 - val_acc: 0.8345\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8821 - val_loss: 0.3565 - val_acc: 0.8370\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8831 - val_loss: 0.3330 - val_acc: 0.8461\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8818 - val_loss: 0.3707 - val_acc: 0.8330\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8828 - val_loss: 0.3561 - val_acc: 0.8399\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2945 - acc: 0.8816 - val_loss: 0.3606 - val_acc: 0.8317\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8823 - val_loss: 0.3847 - val_acc: 0.8284\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8825 - val_loss: 0.3512 - val_acc: 0.8361\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2947 - acc: 0.8815 - val_loss: 0.3640 - val_acc: 0.8328\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2935 - acc: 0.8828 - val_loss: 0.3361 - val_acc: 0.8469\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2945 - acc: 0.8818 - val_loss: 0.3710 - val_acc: 0.8310\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8809 - val_loss: 0.3341 - val_acc: 0.8463\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8823 - val_loss: 0.3408 - val_acc: 0.8434\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8818 - val_loss: 0.3748 - val_acc: 0.8317\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8825 - val_loss: 0.3403 - val_acc: 0.8385\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8812 - val_loss: 0.3591 - val_acc: 0.8345\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8820 - val_loss: 0.3456 - val_acc: 0.8432\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2933 - acc: 0.8828 - val_loss: 0.3345 - val_acc: 0.8441\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8815 - val_loss: 0.3791 - val_acc: 0.8246\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8812 - val_loss: 0.3341 - val_acc: 0.8405\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8810 - val_loss: 0.3382 - val_acc: 0.8410\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8821 - val_loss: 0.3323 - val_acc: 0.8449\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8814 - val_loss: 0.3482 - val_acc: 0.8374\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2933 - acc: 0.8828 - val_loss: 0.3528 - val_acc: 0.8343\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8820 - val_loss: 0.3471 - val_acc: 0.8396\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8820 - val_loss: 0.3739 - val_acc: 0.8317\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8814 - val_loss: 0.3375 - val_acc: 0.8396\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8823 - val_loss: 0.3418 - val_acc: 0.8421\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.8815 - val_loss: 0.3671 - val_acc: 0.8352\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8819 - val_loss: 0.3360 - val_acc: 0.8474\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8821 - val_loss: 0.3570 - val_acc: 0.8359\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8818 - val_loss: 0.3561 - val_acc: 0.8363\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8810 - val_loss: 0.3415 - val_acc: 0.8452\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8824 - val_loss: 0.3441 - val_acc: 0.8436\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2941 - acc: 0.8821 - val_loss: 0.3579 - val_acc: 0.8354\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8820 - val_loss: 0.3479 - val_acc: 0.8423\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8819 - val_loss: 0.3388 - val_acc: 0.8421\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8820 - val_loss: 0.3653 - val_acc: 0.8339\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8825 - val_loss: 0.3626 - val_acc: 0.8385\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2942 - acc: 0.8818 - val_loss: 0.3283 - val_acc: 0.8447\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2942 - acc: 0.8811 - val_loss: 0.3341 - val_acc: 0.8491\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8817 - val_loss: 0.3484 - val_acc: 0.8432\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8814 - val_loss: 0.3451 - val_acc: 0.8412\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2937 - acc: 0.8824 - val_loss: 0.3323 - val_acc: 0.8469\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8813 - val_loss: 0.3291 - val_acc: 0.8494\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8816 - val_loss: 0.3661 - val_acc: 0.8317\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8814 - val_loss: 0.3415 - val_acc: 0.8436\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8817 - val_loss: 0.3510 - val_acc: 0.8370\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8812 - val_loss: 0.3470 - val_acc: 0.8430\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2931 - acc: 0.8816 - val_loss: 0.3558 - val_acc: 0.8385\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8820 - val_loss: 0.3520 - val_acc: 0.8361\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8814 - val_loss: 0.3389 - val_acc: 0.8454\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8820 - val_loss: 0.3647 - val_acc: 0.8348\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8828 - val_loss: 0.3524 - val_acc: 0.8418\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2945 - acc: 0.8811 - val_loss: 0.3509 - val_acc: 0.8427\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8820 - val_loss: 0.3533 - val_acc: 0.8410\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2943 - acc: 0.8816 - val_loss: 0.3339 - val_acc: 0.8458\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8824 - val_loss: 0.3441 - val_acc: 0.8425\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2937 - acc: 0.8809 - val_loss: 0.3470 - val_acc: 0.8416\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8818 - val_loss: 0.3392 - val_acc: 0.8405\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8815 - val_loss: 0.3542 - val_acc: 0.8385\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8817 - val_loss: 0.3502 - val_acc: 0.8407\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2938 - acc: 0.8813 - val_loss: 0.3445 - val_acc: 0.8434\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8809 - val_loss: 0.3480 - val_acc: 0.8425\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8815 - val_loss: 0.3481 - val_acc: 0.8407\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2948 - acc: 0.8813 - val_loss: 0.3499 - val_acc: 0.8443\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2935 - acc: 0.8817 - val_loss: 0.3582 - val_acc: 0.8334\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8818 - val_loss: 0.3629 - val_acc: 0.8376\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2942 - acc: 0.8808 - val_loss: 0.3572 - val_acc: 0.8368\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2936 - acc: 0.8819 - val_loss: 0.3531 - val_acc: 0.8401\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8813 - val_loss: 0.3542 - val_acc: 0.8376\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2942 - acc: 0.8820 - val_loss: 0.3521 - val_acc: 0.8445\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8817 - val_loss: 0.3363 - val_acc: 0.8480\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8821 - val_loss: 0.3545 - val_acc: 0.8390\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2945 - acc: 0.8808 - val_loss: 0.3634 - val_acc: 0.8334\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2941 - acc: 0.8823 - val_loss: 0.3602 - val_acc: 0.8365\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2939 - acc: 0.8819 - val_loss: 0.3749 - val_acc: 0.8319\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2930 - acc: 0.8816 - val_loss: 0.3678 - val_acc: 0.8337\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8817 - val_loss: 0.3813 - val_acc: 0.8299\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2938 - acc: 0.8814 - val_loss: 0.3761 - val_acc: 0.8317\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8820 - val_loss: 0.3479 - val_acc: 0.8405\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2932 - acc: 0.8827 - val_loss: 0.3445 - val_acc: 0.8430\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2935 - acc: 0.8819 - val_loss: 0.3330 - val_acc: 0.8491\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2939 - acc: 0.8815 - val_loss: 0.3377 - val_acc: 0.8445\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8808 - val_loss: 0.3533 - val_acc: 0.8359\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2934 - acc: 0.8822 - val_loss: 0.3324 - val_acc: 0.8500\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2939 - acc: 0.8820 - val_loss: 0.3639 - val_acc: 0.8365\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8816 - val_loss: 0.3620 - val_acc: 0.8372\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2933 - acc: 0.8816 - val_loss: 0.3514 - val_acc: 0.8392\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2937 - acc: 0.8817 - val_loss: 0.3680 - val_acc: 0.8361\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2936 - acc: 0.8816 - val_loss: 0.3550 - val_acc: 0.8412\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8820 - val_loss: 0.3501 - val_acc: 0.8414\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8818 - val_loss: 0.3268 - val_acc: 0.8472\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2933 - acc: 0.8822 - val_loss: 0.3705 - val_acc: 0.8301\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8819 - val_loss: 0.3621 - val_acc: 0.8354\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8814 - val_loss: 0.3700 - val_acc: 0.8354\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8825 - val_loss: 0.3541 - val_acc: 0.8392\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2932 - acc: 0.8830 - val_loss: 0.3505 - val_acc: 0.8434\n",
            "acc: 84.34%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 23us/step - loss: 0.3483 - acc: 0.8532 - val_loss: 0.3456 - val_acc: 0.8410\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3101 - acc: 0.8749 - val_loss: 0.3368 - val_acc: 0.8454\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3048 - acc: 0.8767 - val_loss: 0.3140 - val_acc: 0.8538\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3006 - acc: 0.8783 - val_loss: 0.3324 - val_acc: 0.8458\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2997 - acc: 0.8789 - val_loss: 0.3275 - val_acc: 0.8441\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2978 - acc: 0.8798 - val_loss: 0.3504 - val_acc: 0.8410\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2970 - acc: 0.8801 - val_loss: 0.3484 - val_acc: 0.8368\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8802 - val_loss: 0.3228 - val_acc: 0.8503\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8801 - val_loss: 0.3250 - val_acc: 0.8525\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2955 - acc: 0.8808 - val_loss: 0.3178 - val_acc: 0.8536\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2950 - acc: 0.8797 - val_loss: 0.3338 - val_acc: 0.8452\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2944 - acc: 0.8804 - val_loss: 0.3283 - val_acc: 0.8494\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2944 - acc: 0.8816 - val_loss: 0.3436 - val_acc: 0.8472\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8816 - val_loss: 0.3201 - val_acc: 0.8509\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2944 - acc: 0.8809 - val_loss: 0.3361 - val_acc: 0.8432\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8820 - val_loss: 0.3522 - val_acc: 0.8394\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2935 - acc: 0.8814 - val_loss: 0.3250 - val_acc: 0.8474\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2941 - acc: 0.8814 - val_loss: 0.3241 - val_acc: 0.8480\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8818 - val_loss: 0.3280 - val_acc: 0.8480\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2936 - acc: 0.8829 - val_loss: 0.3349 - val_acc: 0.8456\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2934 - acc: 0.8815 - val_loss: 0.3698 - val_acc: 0.8308\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2940 - acc: 0.8817 - val_loss: 0.3436 - val_acc: 0.8458\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2933 - acc: 0.8817 - val_loss: 0.3310 - val_acc: 0.8445\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8819 - val_loss: 0.3168 - val_acc: 0.8553\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2926 - acc: 0.8818 - val_loss: 0.3362 - val_acc: 0.8407\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2929 - acc: 0.8816 - val_loss: 0.3203 - val_acc: 0.8516\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2926 - acc: 0.8821 - val_loss: 0.3127 - val_acc: 0.8518\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3332 - val_acc: 0.8461\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2926 - acc: 0.8828 - val_loss: 0.3395 - val_acc: 0.8449\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2931 - acc: 0.8826 - val_loss: 0.3395 - val_acc: 0.8454\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2925 - acc: 0.8825 - val_loss: 0.3401 - val_acc: 0.8425\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2924 - acc: 0.8827 - val_loss: 0.3352 - val_acc: 0.8456\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2921 - acc: 0.8825 - val_loss: 0.3376 - val_acc: 0.8465\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8822 - val_loss: 0.3320 - val_acc: 0.8483\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2927 - acc: 0.8825 - val_loss: 0.3226 - val_acc: 0.8467\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2924 - acc: 0.8831 - val_loss: 0.3271 - val_acc: 0.8584\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2922 - acc: 0.8829 - val_loss: 0.3373 - val_acc: 0.8452\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2929 - acc: 0.8821 - val_loss: 0.3437 - val_acc: 0.8425\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2919 - acc: 0.8833 - val_loss: 0.3295 - val_acc: 0.8500\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8831 - val_loss: 0.3357 - val_acc: 0.8483\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8831 - val_loss: 0.3172 - val_acc: 0.8525\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2923 - acc: 0.8826 - val_loss: 0.3257 - val_acc: 0.8494\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2920 - acc: 0.8835 - val_loss: 0.3140 - val_acc: 0.8520\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3339 - val_acc: 0.8434\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2916 - acc: 0.8830 - val_loss: 0.3362 - val_acc: 0.8527\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2918 - acc: 0.8833 - val_loss: 0.3207 - val_acc: 0.8551\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2920 - acc: 0.8831 - val_loss: 0.3367 - val_acc: 0.8456\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2917 - acc: 0.8832 - val_loss: 0.3288 - val_acc: 0.8445\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2916 - acc: 0.8833 - val_loss: 0.3235 - val_acc: 0.8538\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2917 - acc: 0.8830 - val_loss: 0.3351 - val_acc: 0.8496\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2916 - acc: 0.8832 - val_loss: 0.3216 - val_acc: 0.8498\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2919 - acc: 0.8828 - val_loss: 0.3177 - val_acc: 0.8536\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8832 - val_loss: 0.3186 - val_acc: 0.8576\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8830 - val_loss: 0.3223 - val_acc: 0.8514\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8835 - val_loss: 0.3287 - val_acc: 0.8500\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8839 - val_loss: 0.3268 - val_acc: 0.8556\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2915 - acc: 0.8830 - val_loss: 0.3300 - val_acc: 0.8496\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8830 - val_loss: 0.3339 - val_acc: 0.8491\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2916 - acc: 0.8841 - val_loss: 0.3089 - val_acc: 0.8604\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8843 - val_loss: 0.3164 - val_acc: 0.8487\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8838 - val_loss: 0.3359 - val_acc: 0.8483\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2913 - acc: 0.8834 - val_loss: 0.3315 - val_acc: 0.8487\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8837 - val_loss: 0.3250 - val_acc: 0.8505\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8833 - val_loss: 0.3278 - val_acc: 0.8514\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2917 - acc: 0.8831 - val_loss: 0.3158 - val_acc: 0.8558\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8842 - val_loss: 0.3470 - val_acc: 0.8432\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8835 - val_loss: 0.3227 - val_acc: 0.8556\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8836 - val_loss: 0.3164 - val_acc: 0.8602\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2917 - acc: 0.8835 - val_loss: 0.3329 - val_acc: 0.8500\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8833 - val_loss: 0.3069 - val_acc: 0.8540\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3219 - val_acc: 0.8527\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8837 - val_loss: 0.3297 - val_acc: 0.8562\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2907 - acc: 0.8841 - val_loss: 0.3411 - val_acc: 0.8478\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8835 - val_loss: 0.3348 - val_acc: 0.8483\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8836 - val_loss: 0.3270 - val_acc: 0.8487\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8838 - val_loss: 0.3185 - val_acc: 0.8567\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8833 - val_loss: 0.3246 - val_acc: 0.8518\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8842 - val_loss: 0.3326 - val_acc: 0.8511\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2918 - acc: 0.8834 - val_loss: 0.3337 - val_acc: 0.8522\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8840 - val_loss: 0.3298 - val_acc: 0.8531\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8833 - val_loss: 0.3212 - val_acc: 0.8556\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8842 - val_loss: 0.3490 - val_acc: 0.8416\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8842 - val_loss: 0.3289 - val_acc: 0.8511\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8848 - val_loss: 0.3443 - val_acc: 0.8463\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8846 - val_loss: 0.3266 - val_acc: 0.8516\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8844 - val_loss: 0.3383 - val_acc: 0.8474\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8835 - val_loss: 0.3307 - val_acc: 0.8485\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8847 - val_loss: 0.3261 - val_acc: 0.8514\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8848 - val_loss: 0.3343 - val_acc: 0.8511\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2913 - acc: 0.8842 - val_loss: 0.3359 - val_acc: 0.8500\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8840 - val_loss: 0.3179 - val_acc: 0.8511\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8850 - val_loss: 0.3255 - val_acc: 0.8542\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2910 - acc: 0.8838 - val_loss: 0.3205 - val_acc: 0.8576\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8839 - val_loss: 0.3487 - val_acc: 0.8454\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8845 - val_loss: 0.3245 - val_acc: 0.8498\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8839 - val_loss: 0.3394 - val_acc: 0.8507\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2907 - acc: 0.8840 - val_loss: 0.3417 - val_acc: 0.8461\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2915 - acc: 0.8837 - val_loss: 0.3127 - val_acc: 0.8553\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8845 - val_loss: 0.3315 - val_acc: 0.8498\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8847 - val_loss: 0.3418 - val_acc: 0.8483\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2912 - acc: 0.8838 - val_loss: 0.3342 - val_acc: 0.8527\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8845 - val_loss: 0.3349 - val_acc: 0.8467\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8845 - val_loss: 0.3116 - val_acc: 0.8567\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8839 - val_loss: 0.3372 - val_acc: 0.8469\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8841 - val_loss: 0.3442 - val_acc: 0.8489\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8843 - val_loss: 0.3267 - val_acc: 0.8496\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8843 - val_loss: 0.3274 - val_acc: 0.8527\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8843 - val_loss: 0.3274 - val_acc: 0.8514\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8846 - val_loss: 0.3249 - val_acc: 0.8516\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8846 - val_loss: 0.3211 - val_acc: 0.8547\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8847 - val_loss: 0.3257 - val_acc: 0.8529\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3188 - val_acc: 0.8540\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8841 - val_loss: 0.3358 - val_acc: 0.8494\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8837 - val_loss: 0.3409 - val_acc: 0.8456\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8842 - val_loss: 0.3374 - val_acc: 0.8505\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8839 - val_loss: 0.3264 - val_acc: 0.8483\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8846 - val_loss: 0.3497 - val_acc: 0.8425\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8842 - val_loss: 0.3310 - val_acc: 0.8478\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8838 - val_loss: 0.3476 - val_acc: 0.8480\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8841 - val_loss: 0.3255 - val_acc: 0.8529\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8836 - val_loss: 0.3343 - val_acc: 0.8505\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8840 - val_loss: 0.3393 - val_acc: 0.8540\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8837 - val_loss: 0.3289 - val_acc: 0.8553\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8836 - val_loss: 0.3060 - val_acc: 0.8569\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8848 - val_loss: 0.3217 - val_acc: 0.8547\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8844 - val_loss: 0.3367 - val_acc: 0.8500\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8847 - val_loss: 0.3364 - val_acc: 0.8474\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8842 - val_loss: 0.3130 - val_acc: 0.8593\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8841 - val_loss: 0.3301 - val_acc: 0.8522\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2914 - acc: 0.8844 - val_loss: 0.3390 - val_acc: 0.8511\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8846 - val_loss: 0.3186 - val_acc: 0.8573\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2913 - acc: 0.8841 - val_loss: 0.3210 - val_acc: 0.8567\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8841 - val_loss: 0.3500 - val_acc: 0.8449\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8837 - val_loss: 0.3184 - val_acc: 0.8540\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2913 - acc: 0.8842 - val_loss: 0.3277 - val_acc: 0.8529\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8847 - val_loss: 0.3387 - val_acc: 0.8458\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8839 - val_loss: 0.3322 - val_acc: 0.8534\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8841 - val_loss: 0.3190 - val_acc: 0.8564\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8847 - val_loss: 0.3138 - val_acc: 0.8595\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8845 - val_loss: 0.3356 - val_acc: 0.8494\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2905 - acc: 0.8847 - val_loss: 0.3137 - val_acc: 0.8604\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2910 - acc: 0.8840 - val_loss: 0.3288 - val_acc: 0.8551\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2907 - acc: 0.8849 - val_loss: 0.3177 - val_acc: 0.8569\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8841 - val_loss: 0.3382 - val_acc: 0.8500\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8848 - val_loss: 0.3182 - val_acc: 0.8540\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2911 - acc: 0.8839 - val_loss: 0.3298 - val_acc: 0.8509\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8840 - val_loss: 0.3446 - val_acc: 0.8498\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8850 - val_loss: 0.3348 - val_acc: 0.8509\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8844 - val_loss: 0.3368 - val_acc: 0.8518\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8854 - val_loss: 0.3283 - val_acc: 0.8514\n",
            "acc: 85.14%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 25us/step - loss: 0.3595 - acc: 0.8439 - val_loss: 0.3807 - val_acc: 0.8204\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3189 - acc: 0.8680 - val_loss: 0.3789 - val_acc: 0.8237\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3086 - acc: 0.8740 - val_loss: 0.3560 - val_acc: 0.8330\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.3043 - acc: 0.8750 - val_loss: 0.3656 - val_acc: 0.8275\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3001 - acc: 0.8784 - val_loss: 0.3451 - val_acc: 0.8461\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8795 - val_loss: 0.3757 - val_acc: 0.8288\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2971 - acc: 0.8797 - val_loss: 0.3649 - val_acc: 0.8350\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2960 - acc: 0.8804 - val_loss: 0.3478 - val_acc: 0.8434\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2950 - acc: 0.8819 - val_loss: 0.3505 - val_acc: 0.8432\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2947 - acc: 0.8810 - val_loss: 0.3642 - val_acc: 0.8414\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2946 - acc: 0.8810 - val_loss: 0.3348 - val_acc: 0.8494\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2937 - acc: 0.8818 - val_loss: 0.3563 - val_acc: 0.8432\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2929 - acc: 0.8826 - val_loss: 0.3806 - val_acc: 0.8310\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2921 - acc: 0.8821 - val_loss: 0.3814 - val_acc: 0.8341\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2917 - acc: 0.8825 - val_loss: 0.3445 - val_acc: 0.8421\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2920 - acc: 0.8821 - val_loss: 0.3487 - val_acc: 0.8452\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8828 - val_loss: 0.3438 - val_acc: 0.8496\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2909 - acc: 0.8832 - val_loss: 0.3349 - val_acc: 0.8527\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2913 - acc: 0.8821 - val_loss: 0.3602 - val_acc: 0.8458\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8832 - val_loss: 0.3511 - val_acc: 0.8478\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8830 - val_loss: 0.3601 - val_acc: 0.8394\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8841 - val_loss: 0.3532 - val_acc: 0.8436\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2905 - acc: 0.8831 - val_loss: 0.3636 - val_acc: 0.8432\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8832 - val_loss: 0.3803 - val_acc: 0.8354\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8834 - val_loss: 0.3613 - val_acc: 0.8434\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8824 - val_loss: 0.3623 - val_acc: 0.8430\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2900 - acc: 0.8832 - val_loss: 0.3481 - val_acc: 0.8489\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8837 - val_loss: 0.3534 - val_acc: 0.8489\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8826 - val_loss: 0.3421 - val_acc: 0.8489\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8836 - val_loss: 0.3370 - val_acc: 0.8507\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2906 - acc: 0.8827 - val_loss: 0.3662 - val_acc: 0.8381\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3536 - val_acc: 0.8467\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8834 - val_loss: 0.3511 - val_acc: 0.8388\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8842 - val_loss: 0.3725 - val_acc: 0.8388\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8826 - val_loss: 0.3424 - val_acc: 0.8465\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8827 - val_loss: 0.3737 - val_acc: 0.8385\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8835 - val_loss: 0.3294 - val_acc: 0.8545\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2894 - acc: 0.8829 - val_loss: 0.3792 - val_acc: 0.8383\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8839 - val_loss: 0.3470 - val_acc: 0.8500\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8834 - val_loss: 0.3525 - val_acc: 0.8423\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8836 - val_loss: 0.3588 - val_acc: 0.8425\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8839 - val_loss: 0.3620 - val_acc: 0.8458\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8832 - val_loss: 0.3589 - val_acc: 0.8412\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8839 - val_loss: 0.3529 - val_acc: 0.8443\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8834 - val_loss: 0.3527 - val_acc: 0.8445\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8838 - val_loss: 0.3494 - val_acc: 0.8507\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8829 - val_loss: 0.3498 - val_acc: 0.8483\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8834 - val_loss: 0.3375 - val_acc: 0.8514\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8838 - val_loss: 0.3745 - val_acc: 0.8352\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8837 - val_loss: 0.3567 - val_acc: 0.8443\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8831 - val_loss: 0.3633 - val_acc: 0.8385\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8836 - val_loss: 0.3509 - val_acc: 0.8396\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8833 - val_loss: 0.3402 - val_acc: 0.8465\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2898 - acc: 0.8833 - val_loss: 0.3324 - val_acc: 0.8525\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8844 - val_loss: 0.3488 - val_acc: 0.8401\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8832 - val_loss: 0.3735 - val_acc: 0.8381\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8837 - val_loss: 0.3552 - val_acc: 0.8423\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2894 - acc: 0.8836 - val_loss: 0.3539 - val_acc: 0.8441\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8830 - val_loss: 0.3442 - val_acc: 0.8522\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2894 - acc: 0.8836 - val_loss: 0.3691 - val_acc: 0.8388\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2894 - acc: 0.8842 - val_loss: 0.3539 - val_acc: 0.8407\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2901 - acc: 0.8833 - val_loss: 0.3546 - val_acc: 0.8463\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8831 - val_loss: 0.3643 - val_acc: 0.8401\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3605 - val_acc: 0.8438\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8825 - val_loss: 0.3424 - val_acc: 0.8476\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8830 - val_loss: 0.3584 - val_acc: 0.8472\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2892 - acc: 0.8847 - val_loss: 0.3771 - val_acc: 0.8372\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8842 - val_loss: 0.3451 - val_acc: 0.8467\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8838 - val_loss: 0.3451 - val_acc: 0.8503\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8833 - val_loss: 0.3373 - val_acc: 0.8536\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2898 - acc: 0.8827 - val_loss: 0.3663 - val_acc: 0.8427\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3828 - val_acc: 0.8401\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2895 - acc: 0.8835 - val_loss: 0.3599 - val_acc: 0.8396\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8833 - val_loss: 0.3750 - val_acc: 0.8345\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8840 - val_loss: 0.3286 - val_acc: 0.8534\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8830 - val_loss: 0.3854 - val_acc: 0.8337\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8834 - val_loss: 0.3435 - val_acc: 0.8478\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8832 - val_loss: 0.3429 - val_acc: 0.8564\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8832 - val_loss: 0.3641 - val_acc: 0.8407\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8827 - val_loss: 0.3365 - val_acc: 0.8498\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8838 - val_loss: 0.3521 - val_acc: 0.8487\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8842 - val_loss: 0.3430 - val_acc: 0.8480\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8829 - val_loss: 0.3538 - val_acc: 0.8456\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8829 - val_loss: 0.3315 - val_acc: 0.8485\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3859 - val_acc: 0.8330\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2897 - acc: 0.8832 - val_loss: 0.3441 - val_acc: 0.8498\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2888 - acc: 0.8839 - val_loss: 0.3278 - val_acc: 0.8514\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2903 - acc: 0.8836 - val_loss: 0.3357 - val_acc: 0.8498\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2899 - acc: 0.8832 - val_loss: 0.3246 - val_acc: 0.8483\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2896 - acc: 0.8831 - val_loss: 0.3453 - val_acc: 0.8449\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2896 - acc: 0.8831 - val_loss: 0.3523 - val_acc: 0.8485\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8833 - val_loss: 0.3654 - val_acc: 0.8412\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8838 - val_loss: 0.3614 - val_acc: 0.8421\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8843 - val_loss: 0.3353 - val_acc: 0.8478\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8837 - val_loss: 0.3456 - val_acc: 0.8500\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8836 - val_loss: 0.3510 - val_acc: 0.8410\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8834 - val_loss: 0.3729 - val_acc: 0.8323\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8839 - val_loss: 0.3537 - val_acc: 0.8418\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2893 - acc: 0.8847 - val_loss: 0.3489 - val_acc: 0.8454\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8837 - val_loss: 0.3469 - val_acc: 0.8494\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8844 - val_loss: 0.3376 - val_acc: 0.8438\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8831 - val_loss: 0.3633 - val_acc: 0.8458\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8837 - val_loss: 0.3427 - val_acc: 0.8496\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8844 - val_loss: 0.3664 - val_acc: 0.8405\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8839 - val_loss: 0.3465 - val_acc: 0.8432\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8846 - val_loss: 0.3671 - val_acc: 0.8432\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8837 - val_loss: 0.3372 - val_acc: 0.8536\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2895 - acc: 0.8835 - val_loss: 0.3705 - val_acc: 0.8385\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8836 - val_loss: 0.3489 - val_acc: 0.8476\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8837 - val_loss: 0.3534 - val_acc: 0.8438\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8831 - val_loss: 0.3374 - val_acc: 0.8489\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2900 - acc: 0.8840 - val_loss: 0.3259 - val_acc: 0.8569\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2888 - acc: 0.8841 - val_loss: 0.3325 - val_acc: 0.8498\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8840 - val_loss: 0.3709 - val_acc: 0.8381\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8845 - val_loss: 0.3624 - val_acc: 0.8405\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8840 - val_loss: 0.3402 - val_acc: 0.8518\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8837 - val_loss: 0.3453 - val_acc: 0.8536\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8836 - val_loss: 0.3478 - val_acc: 0.8467\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8841 - val_loss: 0.3452 - val_acc: 0.8463\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8840 - val_loss: 0.3313 - val_acc: 0.8529\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8851 - val_loss: 0.3469 - val_acc: 0.8416\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2894 - acc: 0.8834 - val_loss: 0.3452 - val_acc: 0.8496\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8834 - val_loss: 0.3464 - val_acc: 0.8441\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2897 - acc: 0.8840 - val_loss: 0.3594 - val_acc: 0.8407\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8836 - val_loss: 0.3312 - val_acc: 0.8489\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2891 - acc: 0.8842 - val_loss: 0.3380 - val_acc: 0.8498\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8841 - val_loss: 0.3598 - val_acc: 0.8436\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8836 - val_loss: 0.3572 - val_acc: 0.8443\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8845 - val_loss: 0.3389 - val_acc: 0.8551\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8841 - val_loss: 0.3621 - val_acc: 0.8418\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8843 - val_loss: 0.3481 - val_acc: 0.8423\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8839 - val_loss: 0.3457 - val_acc: 0.8483\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2888 - acc: 0.8836 - val_loss: 0.3509 - val_acc: 0.8452\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2893 - acc: 0.8837 - val_loss: 0.3703 - val_acc: 0.8421\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8845 - val_loss: 0.3498 - val_acc: 0.8474\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8834 - val_loss: 0.3584 - val_acc: 0.8418\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8835 - val_loss: 0.3549 - val_acc: 0.8414\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8843 - val_loss: 0.3454 - val_acc: 0.8447\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8838 - val_loss: 0.3489 - val_acc: 0.8416\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8831 - val_loss: 0.3329 - val_acc: 0.8518\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2892 - acc: 0.8837 - val_loss: 0.3533 - val_acc: 0.8456\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8836 - val_loss: 0.3586 - val_acc: 0.8441\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 15us/step - loss: 0.2887 - acc: 0.8839 - val_loss: 0.3518 - val_acc: 0.8478\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8837 - val_loss: 0.3567 - val_acc: 0.8434\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8830 - val_loss: 0.3491 - val_acc: 0.8472\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8834 - val_loss: 0.3401 - val_acc: 0.8498\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2888 - acc: 0.8838 - val_loss: 0.3690 - val_acc: 0.8379\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8839 - val_loss: 0.3471 - val_acc: 0.8483\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8834 - val_loss: 0.3468 - val_acc: 0.8498\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8831 - val_loss: 0.3842 - val_acc: 0.8368\n",
            "acc: 83.68%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 24us/step - loss: 0.3512 - acc: 0.8532 - val_loss: 0.3655 - val_acc: 0.8321\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3117 - acc: 0.8734 - val_loss: 0.2916 - val_acc: 0.8646\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3055 - acc: 0.8752 - val_loss: 0.3357 - val_acc: 0.8403\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3030 - acc: 0.8771 - val_loss: 0.3276 - val_acc: 0.8505\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3009 - acc: 0.8786 - val_loss: 0.3062 - val_acc: 0.8677\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2980 - acc: 0.8808 - val_loss: 0.3362 - val_acc: 0.8498\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2973 - acc: 0.8820 - val_loss: 0.3152 - val_acc: 0.8591\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2966 - acc: 0.8823 - val_loss: 0.3196 - val_acc: 0.8582\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2954 - acc: 0.8836 - val_loss: 0.3203 - val_acc: 0.8633\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2943 - acc: 0.8845 - val_loss: 0.3235 - val_acc: 0.8567\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2946 - acc: 0.8843 - val_loss: 0.3406 - val_acc: 0.8573\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2935 - acc: 0.8848 - val_loss: 0.3350 - val_acc: 0.8551\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2927 - acc: 0.8853 - val_loss: 0.3147 - val_acc: 0.8607\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2929 - acc: 0.8845 - val_loss: 0.3234 - val_acc: 0.8576\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2915 - acc: 0.8861 - val_loss: 0.3051 - val_acc: 0.8657\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2919 - acc: 0.8859 - val_loss: 0.3104 - val_acc: 0.8613\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2910 - acc: 0.8858 - val_loss: 0.3313 - val_acc: 0.8569\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2912 - acc: 0.8863 - val_loss: 0.3038 - val_acc: 0.8673\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2911 - acc: 0.8862 - val_loss: 0.3445 - val_acc: 0.8454\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2908 - acc: 0.8872 - val_loss: 0.3237 - val_acc: 0.8591\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2903 - acc: 0.8869 - val_loss: 0.3002 - val_acc: 0.8682\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2900 - acc: 0.8863 - val_loss: 0.3240 - val_acc: 0.8631\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8870 - val_loss: 0.3022 - val_acc: 0.8682\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.8865 - val_loss: 0.3417 - val_acc: 0.8545\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8865 - val_loss: 0.3018 - val_acc: 0.8653\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2899 - acc: 0.8866 - val_loss: 0.3411 - val_acc: 0.8500\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8870 - val_loss: 0.3090 - val_acc: 0.8564\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2904 - acc: 0.8874 - val_loss: 0.3138 - val_acc: 0.8587\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8874 - val_loss: 0.3110 - val_acc: 0.8618\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2899 - acc: 0.8871 - val_loss: 0.3207 - val_acc: 0.8553\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8873 - val_loss: 0.3054 - val_acc: 0.8604\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2901 - acc: 0.8868 - val_loss: 0.3197 - val_acc: 0.8551\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8873 - val_loss: 0.3107 - val_acc: 0.8649\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8871 - val_loss: 0.2859 - val_acc: 0.8708\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2896 - acc: 0.8881 - val_loss: 0.3191 - val_acc: 0.8629\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2896 - acc: 0.8873 - val_loss: 0.3166 - val_acc: 0.8613\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8880 - val_loss: 0.3109 - val_acc: 0.8556\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2891 - acc: 0.8881 - val_loss: 0.3329 - val_acc: 0.8547\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2892 - acc: 0.8881 - val_loss: 0.3145 - val_acc: 0.8615\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8884 - val_loss: 0.3089 - val_acc: 0.8598\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2893 - acc: 0.8876 - val_loss: 0.3242 - val_acc: 0.8560\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8870 - val_loss: 0.2884 - val_acc: 0.8655\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8875 - val_loss: 0.3104 - val_acc: 0.8642\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8880 - val_loss: 0.3094 - val_acc: 0.8560\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8879 - val_loss: 0.3224 - val_acc: 0.8622\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8879 - val_loss: 0.3298 - val_acc: 0.8589\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8877 - val_loss: 0.3194 - val_acc: 0.8631\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2888 - acc: 0.8880 - val_loss: 0.3191 - val_acc: 0.8571\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8880 - val_loss: 0.3174 - val_acc: 0.8631\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2890 - acc: 0.8872 - val_loss: 0.3312 - val_acc: 0.8569\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8875 - val_loss: 0.3238 - val_acc: 0.8580\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2883 - acc: 0.8887 - val_loss: 0.3304 - val_acc: 0.8542\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8884 - val_loss: 0.3194 - val_acc: 0.8564\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8870 - val_loss: 0.2914 - val_acc: 0.8668\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8879 - val_loss: 0.3279 - val_acc: 0.8576\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8878 - val_loss: 0.3143 - val_acc: 0.8609\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8880 - val_loss: 0.3124 - val_acc: 0.8651\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8883 - val_loss: 0.3098 - val_acc: 0.8688\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8887 - val_loss: 0.3262 - val_acc: 0.8607\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8877 - val_loss: 0.3098 - val_acc: 0.8604\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8885 - val_loss: 0.3088 - val_acc: 0.8666\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2886 - acc: 0.8876 - val_loss: 0.3174 - val_acc: 0.8629\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2889 - acc: 0.8886 - val_loss: 0.3149 - val_acc: 0.8644\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8879 - val_loss: 0.3288 - val_acc: 0.8558\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8883 - val_loss: 0.3123 - val_acc: 0.8640\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8880 - val_loss: 0.3199 - val_acc: 0.8624\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8886 - val_loss: 0.3125 - val_acc: 0.8587\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8880 - val_loss: 0.3278 - val_acc: 0.8549\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2887 - acc: 0.8879 - val_loss: 0.3223 - val_acc: 0.8576\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8889 - val_loss: 0.3168 - val_acc: 0.8584\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8894 - val_loss: 0.3059 - val_acc: 0.8651\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8884 - val_loss: 0.3104 - val_acc: 0.8611\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8889 - val_loss: 0.3265 - val_acc: 0.8587\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2880 - acc: 0.8889 - val_loss: 0.3162 - val_acc: 0.8611\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2883 - acc: 0.8890 - val_loss: 0.3302 - val_acc: 0.8542\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8887 - val_loss: 0.3320 - val_acc: 0.8549\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8881 - val_loss: 0.3130 - val_acc: 0.8620\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8885 - val_loss: 0.2996 - val_acc: 0.8655\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8885 - val_loss: 0.3415 - val_acc: 0.8489\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8887 - val_loss: 0.3153 - val_acc: 0.8609\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8878 - val_loss: 0.3006 - val_acc: 0.8640\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8884 - val_loss: 0.3206 - val_acc: 0.8589\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8886 - val_loss: 0.3058 - val_acc: 0.8675\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8886 - val_loss: 0.3159 - val_acc: 0.8644\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8889 - val_loss: 0.3134 - val_acc: 0.8609\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8887 - val_loss: 0.3150 - val_acc: 0.8600\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8883 - val_loss: 0.3031 - val_acc: 0.8609\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8875 - val_loss: 0.3149 - val_acc: 0.8578\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2885 - acc: 0.8880 - val_loss: 0.3262 - val_acc: 0.8567\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8891 - val_loss: 0.3088 - val_acc: 0.8651\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8887 - val_loss: 0.3094 - val_acc: 0.8655\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8883 - val_loss: 0.3284 - val_acc: 0.8511\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8893 - val_loss: 0.3117 - val_acc: 0.8587\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8881 - val_loss: 0.3167 - val_acc: 0.8607\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8883 - val_loss: 0.3109 - val_acc: 0.8637\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8887 - val_loss: 0.3263 - val_acc: 0.8538\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8883 - val_loss: 0.3270 - val_acc: 0.8558\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8884 - val_loss: 0.3051 - val_acc: 0.8635\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8893 - val_loss: 0.3404 - val_acc: 0.8556\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8885 - val_loss: 0.3137 - val_acc: 0.8593\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8885 - val_loss: 0.3127 - val_acc: 0.8604\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8883 - val_loss: 0.3146 - val_acc: 0.8591\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8879 - val_loss: 0.3082 - val_acc: 0.8629\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8891 - val_loss: 0.3349 - val_acc: 0.8564\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8884 - val_loss: 0.3123 - val_acc: 0.8622\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2883 - acc: 0.8884 - val_loss: 0.3345 - val_acc: 0.8534\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8882 - val_loss: 0.3257 - val_acc: 0.8522\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8886 - val_loss: 0.3083 - val_acc: 0.8595\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8886 - val_loss: 0.3215 - val_acc: 0.8576\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8895 - val_loss: 0.3168 - val_acc: 0.8540\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8888 - val_loss: 0.3127 - val_acc: 0.8598\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8891 - val_loss: 0.3226 - val_acc: 0.8602\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2884 - acc: 0.8886 - val_loss: 0.3028 - val_acc: 0.8677\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8881 - val_loss: 0.3102 - val_acc: 0.8651\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8891 - val_loss: 0.3239 - val_acc: 0.8598\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8888 - val_loss: 0.3162 - val_acc: 0.8609\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2878 - acc: 0.8891 - val_loss: 0.3335 - val_acc: 0.8534\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8892 - val_loss: 0.3008 - val_acc: 0.8642\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8875 - val_loss: 0.3228 - val_acc: 0.8560\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8884 - val_loss: 0.2984 - val_acc: 0.8633\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8880 - val_loss: 0.3211 - val_acc: 0.8635\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8888 - val_loss: 0.3006 - val_acc: 0.8651\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8890 - val_loss: 0.3226 - val_acc: 0.8529\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8885 - val_loss: 0.3083 - val_acc: 0.8587\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8892 - val_loss: 0.3244 - val_acc: 0.8540\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8882 - val_loss: 0.3059 - val_acc: 0.8611\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8887 - val_loss: 0.3000 - val_acc: 0.8600\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2881 - acc: 0.8894 - val_loss: 0.3334 - val_acc: 0.8564\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8890 - val_loss: 0.3167 - val_acc: 0.8536\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8884 - val_loss: 0.3224 - val_acc: 0.8582\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8888 - val_loss: 0.3179 - val_acc: 0.8584\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8892 - val_loss: 0.3125 - val_acc: 0.8584\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8888 - val_loss: 0.3114 - val_acc: 0.8571\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8888 - val_loss: 0.3128 - val_acc: 0.8556\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8879 - val_loss: 0.3430 - val_acc: 0.8449\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8889 - val_loss: 0.3392 - val_acc: 0.8527\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8888 - val_loss: 0.3161 - val_acc: 0.8560\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8888 - val_loss: 0.3164 - val_acc: 0.8564\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8888 - val_loss: 0.3254 - val_acc: 0.8540\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8886 - val_loss: 0.3171 - val_acc: 0.8591\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8892 - val_loss: 0.3355 - val_acc: 0.8514\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8886 - val_loss: 0.3026 - val_acc: 0.8600\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8880 - val_loss: 0.3009 - val_acc: 0.8607\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8887 - val_loss: 0.3151 - val_acc: 0.8602\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8893 - val_loss: 0.3326 - val_acc: 0.8547\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8883 - val_loss: 0.3019 - val_acc: 0.8644\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8882 - val_loss: 0.3227 - val_acc: 0.8580\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8887 - val_loss: 0.3086 - val_acc: 0.8609\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8892 - val_loss: 0.3298 - val_acc: 0.8553\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8886 - val_loss: 0.3355 - val_acc: 0.8503\n",
            "acc: 85.03%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 26us/step - loss: 0.3581 - acc: 0.8483 - val_loss: 0.3692 - val_acc: 0.8257\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3207 - acc: 0.8679 - val_loss: 0.3760 - val_acc: 0.8230\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3136 - acc: 0.8702 - val_loss: 0.3624 - val_acc: 0.8368\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3092 - acc: 0.8731 - val_loss: 0.3309 - val_acc: 0.8480\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3058 - acc: 0.8751 - val_loss: 0.3481 - val_acc: 0.8368\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3040 - acc: 0.8766 - val_loss: 0.3847 - val_acc: 0.8312\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.3018 - acc: 0.8782 - val_loss: 0.3731 - val_acc: 0.8277\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2989 - acc: 0.8793 - val_loss: 0.3588 - val_acc: 0.8370\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2981 - acc: 0.8800 - val_loss: 0.3500 - val_acc: 0.8401\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2965 - acc: 0.8800 - val_loss: 0.3371 - val_acc: 0.8472\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2949 - acc: 0.8807 - val_loss: 0.3491 - val_acc: 0.8379\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2952 - acc: 0.8808 - val_loss: 0.3569 - val_acc: 0.8376\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2948 - acc: 0.8815 - val_loss: 0.3270 - val_acc: 0.8516\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2944 - acc: 0.8814 - val_loss: 0.3575 - val_acc: 0.8361\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2933 - acc: 0.8811 - val_loss: 0.3677 - val_acc: 0.8328\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.8813 - val_loss: 0.3147 - val_acc: 0.8569\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2935 - acc: 0.8814 - val_loss: 0.3462 - val_acc: 0.8410\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2937 - acc: 0.8815 - val_loss: 0.3403 - val_acc: 0.8418\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2935 - acc: 0.8816 - val_loss: 0.3551 - val_acc: 0.8383\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2931 - acc: 0.8816 - val_loss: 0.3717 - val_acc: 0.8306\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2934 - acc: 0.8815 - val_loss: 0.3342 - val_acc: 0.8454\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2931 - acc: 0.8829 - val_loss: 0.3543 - val_acc: 0.8345\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2919 - acc: 0.8822 - val_loss: 0.3568 - val_acc: 0.8350\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2916 - acc: 0.8826 - val_loss: 0.3319 - val_acc: 0.8441\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2912 - acc: 0.8830 - val_loss: 0.3381 - val_acc: 0.8441\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8829 - val_loss: 0.3656 - val_acc: 0.8350\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2912 - acc: 0.8824 - val_loss: 0.3371 - val_acc: 0.8485\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2909 - acc: 0.8827 - val_loss: 0.3607 - val_acc: 0.8257\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8830 - val_loss: 0.3458 - val_acc: 0.8394\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2903 - acc: 0.8829 - val_loss: 0.3627 - val_acc: 0.8317\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2902 - acc: 0.8828 - val_loss: 0.3292 - val_acc: 0.8436\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2897 - acc: 0.8826 - val_loss: 0.3454 - val_acc: 0.8376\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8833 - val_loss: 0.3652 - val_acc: 0.8345\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2901 - acc: 0.8830 - val_loss: 0.3294 - val_acc: 0.8445\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2898 - acc: 0.8835 - val_loss: 0.3555 - val_acc: 0.8383\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2895 - acc: 0.8823 - val_loss: 0.3425 - val_acc: 0.8374\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2889 - acc: 0.8833 - val_loss: 0.3310 - val_acc: 0.8461\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2891 - acc: 0.8827 - val_loss: 0.3445 - val_acc: 0.8441\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2899 - acc: 0.8824 - val_loss: 0.3237 - val_acc: 0.8514\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2894 - acc: 0.8831 - val_loss: 0.3327 - val_acc: 0.8454\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8838 - val_loss: 0.3602 - val_acc: 0.8363\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2894 - acc: 0.8829 - val_loss: 0.3502 - val_acc: 0.8381\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2888 - acc: 0.8833 - val_loss: 0.3314 - val_acc: 0.8494\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8833 - val_loss: 0.3295 - val_acc: 0.8445\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8825 - val_loss: 0.3468 - val_acc: 0.8443\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8828 - val_loss: 0.3455 - val_acc: 0.8436\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8836 - val_loss: 0.3481 - val_acc: 0.8421\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8835 - val_loss: 0.3399 - val_acc: 0.8443\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2889 - acc: 0.8835 - val_loss: 0.3498 - val_acc: 0.8394\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8836 - val_loss: 0.3579 - val_acc: 0.8399\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2882 - acc: 0.8833 - val_loss: 0.3230 - val_acc: 0.8496\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2887 - acc: 0.8829 - val_loss: 0.3405 - val_acc: 0.8441\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8839 - val_loss: 0.3516 - val_acc: 0.8392\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8834 - val_loss: 0.3259 - val_acc: 0.8498\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2886 - acc: 0.8835 - val_loss: 0.3557 - val_acc: 0.8365\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8841 - val_loss: 0.3506 - val_acc: 0.8383\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8843 - val_loss: 0.3330 - val_acc: 0.8483\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2885 - acc: 0.8830 - val_loss: 0.3272 - val_acc: 0.8463\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8828 - val_loss: 0.3330 - val_acc: 0.8445\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8835 - val_loss: 0.3533 - val_acc: 0.8403\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2884 - acc: 0.8836 - val_loss: 0.3433 - val_acc: 0.8445\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2881 - acc: 0.8834 - val_loss: 0.3467 - val_acc: 0.8436\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2887 - acc: 0.8835 - val_loss: 0.3394 - val_acc: 0.8443\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8842 - val_loss: 0.3519 - val_acc: 0.8370\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8842 - val_loss: 0.3428 - val_acc: 0.8434\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8844 - val_loss: 0.3421 - val_acc: 0.8412\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8839 - val_loss: 0.3420 - val_acc: 0.8456\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8831 - val_loss: 0.3455 - val_acc: 0.8410\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8840 - val_loss: 0.3437 - val_acc: 0.8403\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2879 - acc: 0.8838 - val_loss: 0.3332 - val_acc: 0.8480\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2880 - acc: 0.8834 - val_loss: 0.3616 - val_acc: 0.8352\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8832 - val_loss: 0.3334 - val_acc: 0.8414\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8836 - val_loss: 0.3381 - val_acc: 0.8461\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2878 - acc: 0.8838 - val_loss: 0.3673 - val_acc: 0.8334\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8842 - val_loss: 0.3476 - val_acc: 0.8416\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8844 - val_loss: 0.3279 - val_acc: 0.8474\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2876 - acc: 0.8839 - val_loss: 0.3368 - val_acc: 0.8394\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2877 - acc: 0.8844 - val_loss: 0.3550 - val_acc: 0.8361\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8841 - val_loss: 0.3610 - val_acc: 0.8376\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8835 - val_loss: 0.3141 - val_acc: 0.8505\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8841 - val_loss: 0.3305 - val_acc: 0.8496\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8843 - val_loss: 0.3539 - val_acc: 0.8394\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2876 - acc: 0.8839 - val_loss: 0.3247 - val_acc: 0.8507\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8843 - val_loss: 0.3330 - val_acc: 0.8447\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8837 - val_loss: 0.3259 - val_acc: 0.8476\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8841 - val_loss: 0.3665 - val_acc: 0.8317\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2874 - acc: 0.8843 - val_loss: 0.3369 - val_acc: 0.8454\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8841 - val_loss: 0.3355 - val_acc: 0.8485\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2878 - acc: 0.8840 - val_loss: 0.3832 - val_acc: 0.8286\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2878 - acc: 0.8843 - val_loss: 0.3618 - val_acc: 0.8345\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8845 - val_loss: 0.3288 - val_acc: 0.8434\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8842 - val_loss: 0.3442 - val_acc: 0.8427\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2879 - acc: 0.8841 - val_loss: 0.3287 - val_acc: 0.8478\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8845 - val_loss: 0.3630 - val_acc: 0.8368\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2879 - acc: 0.8839 - val_loss: 0.3625 - val_acc: 0.8326\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8845 - val_loss: 0.3384 - val_acc: 0.8432\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8849 - val_loss: 0.3399 - val_acc: 0.8441\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8849 - val_loss: 0.3386 - val_acc: 0.8487\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2877 - acc: 0.8846 - val_loss: 0.3655 - val_acc: 0.8257\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2877 - acc: 0.8843 - val_loss: 0.3513 - val_acc: 0.8383\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8839 - val_loss: 0.3585 - val_acc: 0.8359\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8844 - val_loss: 0.3250 - val_acc: 0.8505\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8842 - val_loss: 0.3413 - val_acc: 0.8441\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8846 - val_loss: 0.3562 - val_acc: 0.8385\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8841 - val_loss: 0.3550 - val_acc: 0.8352\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8845 - val_loss: 0.3498 - val_acc: 0.8368\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2875 - acc: 0.8850 - val_loss: 0.3405 - val_acc: 0.8427\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2862 - acc: 0.8846 - val_loss: 0.3457 - val_acc: 0.8441\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2874 - acc: 0.8844 - val_loss: 0.3506 - val_acc: 0.8427\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8845 - val_loss: 0.3347 - val_acc: 0.8427\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8845 - val_loss: 0.3631 - val_acc: 0.8388\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8846 - val_loss: 0.3532 - val_acc: 0.8385\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2873 - acc: 0.8839 - val_loss: 0.3373 - val_acc: 0.8445\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2869 - acc: 0.8854 - val_loss: 0.3614 - val_acc: 0.8376\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2872 - acc: 0.8835 - val_loss: 0.3340 - val_acc: 0.8505\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8839 - val_loss: 0.3344 - val_acc: 0.8516\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8853 - val_loss: 0.3478 - val_acc: 0.8421\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8846 - val_loss: 0.3396 - val_acc: 0.8494\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8843 - val_loss: 0.3525 - val_acc: 0.8423\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8840 - val_loss: 0.3506 - val_acc: 0.8416\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8846 - val_loss: 0.3334 - val_acc: 0.8511\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2868 - acc: 0.8850 - val_loss: 0.3411 - val_acc: 0.8414\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8846 - val_loss: 0.3317 - val_acc: 0.8478\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8842 - val_loss: 0.3545 - val_acc: 0.8392\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8844 - val_loss: 0.3596 - val_acc: 0.8390\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8848 - val_loss: 0.3332 - val_acc: 0.8511\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2868 - acc: 0.8847 - val_loss: 0.3514 - val_acc: 0.8361\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8849 - val_loss: 0.3302 - val_acc: 0.8511\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8848 - val_loss: 0.3354 - val_acc: 0.8443\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8845 - val_loss: 0.3582 - val_acc: 0.8403\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2862 - acc: 0.8848 - val_loss: 0.3447 - val_acc: 0.8399\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8849 - val_loss: 0.3416 - val_acc: 0.8441\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8849 - val_loss: 0.3474 - val_acc: 0.8416\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8854 - val_loss: 0.3460 - val_acc: 0.8350\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8847 - val_loss: 0.3331 - val_acc: 0.8461\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2864 - acc: 0.8858 - val_loss: 0.3268 - val_acc: 0.8522\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8850 - val_loss: 0.3576 - val_acc: 0.8359\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8850 - val_loss: 0.3343 - val_acc: 0.8443\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2863 - acc: 0.8853 - val_loss: 0.3454 - val_acc: 0.8416\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2870 - acc: 0.8846 - val_loss: 0.3279 - val_acc: 0.8443\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8845 - val_loss: 0.3433 - val_acc: 0.8427\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2868 - acc: 0.8850 - val_loss: 0.3612 - val_acc: 0.8361\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2870 - acc: 0.8847 - val_loss: 0.3572 - val_acc: 0.8306\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8846 - val_loss: 0.3596 - val_acc: 0.8407\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2862 - acc: 0.8858 - val_loss: 0.3370 - val_acc: 0.8454\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3586 - val_acc: 0.8376\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8844 - val_loss: 0.3488 - val_acc: 0.8401\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2865 - acc: 0.8853 - val_loss: 0.3528 - val_acc: 0.8368\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8851 - val_loss: 0.3463 - val_acc: 0.8412\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8845 - val_loss: 0.3567 - val_acc: 0.8396\n",
            "acc: 83.96%\n",
            "Train on 71860 samples, validate on 4521 samples\n",
            "Epoch 1/150\n",
            "71860/71860 [==============================] - 2s 28us/step - loss: 0.3519 - acc: 0.8503 - val_loss: 0.3400 - val_acc: 0.8330\n",
            "Epoch 2/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.3158 - acc: 0.8706 - val_loss: 0.3611 - val_acc: 0.8292\n",
            "Epoch 3/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.3036 - acc: 0.8750 - val_loss: 0.3334 - val_acc: 0.8414\n",
            "Epoch 4/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2992 - acc: 0.8772 - val_loss: 0.3417 - val_acc: 0.8352\n",
            "Epoch 5/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2974 - acc: 0.8780 - val_loss: 0.3474 - val_acc: 0.8328\n",
            "Epoch 6/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2959 - acc: 0.8789 - val_loss: 0.3754 - val_acc: 0.8180\n",
            "Epoch 7/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2950 - acc: 0.8796 - val_loss: 0.3525 - val_acc: 0.8288\n",
            "Epoch 8/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2947 - acc: 0.8795 - val_loss: 0.3220 - val_acc: 0.8472\n",
            "Epoch 9/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2933 - acc: 0.8819 - val_loss: 0.3337 - val_acc: 0.8425\n",
            "Epoch 10/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2930 - acc: 0.8813 - val_loss: 0.3477 - val_acc: 0.8394\n",
            "Epoch 11/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8822 - val_loss: 0.3390 - val_acc: 0.8416\n",
            "Epoch 12/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2923 - acc: 0.8824 - val_loss: 0.3247 - val_acc: 0.8469\n",
            "Epoch 13/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2921 - acc: 0.8824 - val_loss: 0.3221 - val_acc: 0.8447\n",
            "Epoch 14/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8824 - val_loss: 0.3483 - val_acc: 0.8396\n",
            "Epoch 15/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2917 - acc: 0.8815 - val_loss: 0.3485 - val_acc: 0.8379\n",
            "Epoch 16/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2915 - acc: 0.8819 - val_loss: 0.3295 - val_acc: 0.8487\n",
            "Epoch 17/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2915 - acc: 0.8825 - val_loss: 0.3536 - val_acc: 0.8337\n",
            "Epoch 18/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.8818 - val_loss: 0.3486 - val_acc: 0.8383\n",
            "Epoch 19/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2914 - acc: 0.8825 - val_loss: 0.3484 - val_acc: 0.8396\n",
            "Epoch 20/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2905 - acc: 0.8837 - val_loss: 0.3606 - val_acc: 0.8427\n",
            "Epoch 21/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2906 - acc: 0.8839 - val_loss: 0.3371 - val_acc: 0.8449\n",
            "Epoch 22/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2904 - acc: 0.8826 - val_loss: 0.3256 - val_acc: 0.8516\n",
            "Epoch 23/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2900 - acc: 0.8831 - val_loss: 0.3408 - val_acc: 0.8374\n",
            "Epoch 24/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2901 - acc: 0.8824 - val_loss: 0.3231 - val_acc: 0.8558\n",
            "Epoch 25/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2899 - acc: 0.8826 - val_loss: 0.3260 - val_acc: 0.8520\n",
            "Epoch 26/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2896 - acc: 0.8838 - val_loss: 0.3649 - val_acc: 0.8312\n",
            "Epoch 27/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2893 - acc: 0.8835 - val_loss: 0.3416 - val_acc: 0.8483\n",
            "Epoch 28/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2891 - acc: 0.8836 - val_loss: 0.3411 - val_acc: 0.8370\n",
            "Epoch 29/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2890 - acc: 0.8834 - val_loss: 0.3286 - val_acc: 0.8480\n",
            "Epoch 30/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2892 - acc: 0.8831 - val_loss: 0.3353 - val_acc: 0.8381\n",
            "Epoch 31/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2886 - acc: 0.8843 - val_loss: 0.3551 - val_acc: 0.8368\n",
            "Epoch 32/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2890 - acc: 0.8841 - val_loss: 0.3213 - val_acc: 0.8487\n",
            "Epoch 33/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2887 - acc: 0.8837 - val_loss: 0.3512 - val_acc: 0.8326\n",
            "Epoch 34/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8844 - val_loss: 0.3551 - val_acc: 0.8306\n",
            "Epoch 35/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2884 - acc: 0.8837 - val_loss: 0.3359 - val_acc: 0.8376\n",
            "Epoch 36/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2884 - acc: 0.8842 - val_loss: 0.3443 - val_acc: 0.8354\n",
            "Epoch 37/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8847 - val_loss: 0.3284 - val_acc: 0.8465\n",
            "Epoch 38/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2883 - acc: 0.8834 - val_loss: 0.3384 - val_acc: 0.8476\n",
            "Epoch 39/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8840 - val_loss: 0.3586 - val_acc: 0.8321\n",
            "Epoch 40/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2880 - acc: 0.8840 - val_loss: 0.3327 - val_acc: 0.8425\n",
            "Epoch 41/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8837 - val_loss: 0.3070 - val_acc: 0.8613\n",
            "Epoch 42/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2882 - acc: 0.8844 - val_loss: 0.3305 - val_acc: 0.8516\n",
            "Epoch 43/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2882 - acc: 0.8844 - val_loss: 0.3143 - val_acc: 0.8529\n",
            "Epoch 44/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2875 - acc: 0.8849 - val_loss: 0.3259 - val_acc: 0.8491\n",
            "Epoch 45/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8846 - val_loss: 0.3560 - val_acc: 0.8363\n",
            "Epoch 46/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2881 - acc: 0.8850 - val_loss: 0.3410 - val_acc: 0.8374\n",
            "Epoch 47/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2877 - acc: 0.8849 - val_loss: 0.3410 - val_acc: 0.8363\n",
            "Epoch 48/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2875 - acc: 0.8846 - val_loss: 0.3285 - val_acc: 0.8465\n",
            "Epoch 49/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8850 - val_loss: 0.3328 - val_acc: 0.8423\n",
            "Epoch 50/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8853 - val_loss: 0.3364 - val_acc: 0.8425\n",
            "Epoch 51/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8853 - val_loss: 0.3360 - val_acc: 0.8447\n",
            "Epoch 52/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8858 - val_loss: 0.3201 - val_acc: 0.8536\n",
            "Epoch 53/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8856 - val_loss: 0.3324 - val_acc: 0.8480\n",
            "Epoch 54/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8851 - val_loss: 0.3311 - val_acc: 0.8445\n",
            "Epoch 55/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2872 - acc: 0.8855 - val_loss: 0.3286 - val_acc: 0.8463\n",
            "Epoch 56/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2872 - acc: 0.8854 - val_loss: 0.3210 - val_acc: 0.8503\n",
            "Epoch 57/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2874 - acc: 0.8844 - val_loss: 0.3198 - val_acc: 0.8494\n",
            "Epoch 58/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2871 - acc: 0.8851 - val_loss: 0.3242 - val_acc: 0.8463\n",
            "Epoch 59/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8864 - val_loss: 0.3221 - val_acc: 0.8438\n",
            "Epoch 60/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2869 - acc: 0.8852 - val_loss: 0.3370 - val_acc: 0.8427\n",
            "Epoch 61/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2873 - acc: 0.8856 - val_loss: 0.3427 - val_acc: 0.8381\n",
            "Epoch 62/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2866 - acc: 0.8857 - val_loss: 0.3199 - val_acc: 0.8538\n",
            "Epoch 63/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8853 - val_loss: 0.3181 - val_acc: 0.8514\n",
            "Epoch 64/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8852 - val_loss: 0.3367 - val_acc: 0.8357\n",
            "Epoch 65/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2871 - acc: 0.8853 - val_loss: 0.3077 - val_acc: 0.8507\n",
            "Epoch 66/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8847 - val_loss: 0.3092 - val_acc: 0.8573\n",
            "Epoch 67/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2862 - acc: 0.8860 - val_loss: 0.3399 - val_acc: 0.8430\n",
            "Epoch 68/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8862 - val_loss: 0.3321 - val_acc: 0.8456\n",
            "Epoch 69/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8849 - val_loss: 0.3576 - val_acc: 0.8339\n",
            "Epoch 70/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8857 - val_loss: 0.3460 - val_acc: 0.8357\n",
            "Epoch 71/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8859 - val_loss: 0.3314 - val_acc: 0.8438\n",
            "Epoch 72/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8857 - val_loss: 0.3243 - val_acc: 0.8500\n",
            "Epoch 73/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8859 - val_loss: 0.3127 - val_acc: 0.8461\n",
            "Epoch 74/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8853 - val_loss: 0.3348 - val_acc: 0.8438\n",
            "Epoch 75/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2866 - acc: 0.8856 - val_loss: 0.3349 - val_acc: 0.8436\n",
            "Epoch 76/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2870 - acc: 0.8855 - val_loss: 0.3458 - val_acc: 0.8394\n",
            "Epoch 77/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8857 - val_loss: 0.3289 - val_acc: 0.8436\n",
            "Epoch 78/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2862 - acc: 0.8862 - val_loss: 0.3289 - val_acc: 0.8436\n",
            "Epoch 79/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8857 - val_loss: 0.3346 - val_acc: 0.8412\n",
            "Epoch 80/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2858 - acc: 0.8865 - val_loss: 0.3469 - val_acc: 0.8352\n",
            "Epoch 81/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8858 - val_loss: 0.3389 - val_acc: 0.8363\n",
            "Epoch 82/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3369 - val_acc: 0.8441\n",
            "Epoch 83/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2866 - acc: 0.8862 - val_loss: 0.3225 - val_acc: 0.8461\n",
            "Epoch 84/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2863 - acc: 0.8852 - val_loss: 0.3354 - val_acc: 0.8478\n",
            "Epoch 85/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2858 - acc: 0.8865 - val_loss: 0.3269 - val_acc: 0.8474\n",
            "Epoch 86/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8860 - val_loss: 0.3197 - val_acc: 0.8551\n",
            "Epoch 87/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8869 - val_loss: 0.3336 - val_acc: 0.8480\n",
            "Epoch 88/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2867 - acc: 0.8853 - val_loss: 0.3257 - val_acc: 0.8405\n",
            "Epoch 89/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3393 - val_acc: 0.8416\n",
            "Epoch 90/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8869 - val_loss: 0.3422 - val_acc: 0.8399\n",
            "Epoch 91/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2868 - acc: 0.8858 - val_loss: 0.3338 - val_acc: 0.8465\n",
            "Epoch 92/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8871 - val_loss: 0.3522 - val_acc: 0.8323\n",
            "Epoch 93/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8858 - val_loss: 0.3109 - val_acc: 0.8536\n",
            "Epoch 94/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8865 - val_loss: 0.3237 - val_acc: 0.8478\n",
            "Epoch 95/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2859 - acc: 0.8859 - val_loss: 0.3455 - val_acc: 0.8412\n",
            "Epoch 96/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2862 - acc: 0.8862 - val_loss: 0.3439 - val_acc: 0.8427\n",
            "Epoch 97/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8855 - val_loss: 0.3293 - val_acc: 0.8441\n",
            "Epoch 98/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2865 - acc: 0.8860 - val_loss: 0.3509 - val_acc: 0.8399\n",
            "Epoch 99/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8860 - val_loss: 0.3325 - val_acc: 0.8396\n",
            "Epoch 100/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2863 - acc: 0.8872 - val_loss: 0.3476 - val_acc: 0.8372\n",
            "Epoch 101/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8864 - val_loss: 0.3506 - val_acc: 0.8363\n",
            "Epoch 102/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2857 - acc: 0.8861 - val_loss: 0.3011 - val_acc: 0.8589\n",
            "Epoch 103/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2864 - acc: 0.8854 - val_loss: 0.3433 - val_acc: 0.8357\n",
            "Epoch 104/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2859 - acc: 0.8863 - val_loss: 0.3550 - val_acc: 0.8319\n",
            "Epoch 105/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2855 - acc: 0.8868 - val_loss: 0.3443 - val_acc: 0.8372\n",
            "Epoch 106/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2858 - acc: 0.8867 - val_loss: 0.3409 - val_acc: 0.8394\n",
            "Epoch 107/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8861 - val_loss: 0.3376 - val_acc: 0.8427\n",
            "Epoch 108/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8869 - val_loss: 0.3319 - val_acc: 0.8443\n",
            "Epoch 109/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2865 - acc: 0.8857 - val_loss: 0.3330 - val_acc: 0.8452\n",
            "Epoch 110/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8870 - val_loss: 0.3277 - val_acc: 0.8474\n",
            "Epoch 111/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8866 - val_loss: 0.3272 - val_acc: 0.8410\n",
            "Epoch 112/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8856 - val_loss: 0.3105 - val_acc: 0.8553\n",
            "Epoch 113/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2865 - acc: 0.8859 - val_loss: 0.3400 - val_acc: 0.8412\n",
            "Epoch 114/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2857 - acc: 0.8867 - val_loss: 0.3217 - val_acc: 0.8474\n",
            "Epoch 115/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2859 - acc: 0.8862 - val_loss: 0.3458 - val_acc: 0.8312\n",
            "Epoch 116/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2860 - acc: 0.8859 - val_loss: 0.3429 - val_acc: 0.8423\n",
            "Epoch 117/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8860 - val_loss: 0.3338 - val_acc: 0.8427\n",
            "Epoch 118/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2852 - acc: 0.8869 - val_loss: 0.3286 - val_acc: 0.8438\n",
            "Epoch 119/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2860 - acc: 0.8861 - val_loss: 0.3392 - val_acc: 0.8343\n",
            "Epoch 120/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8860 - val_loss: 0.3306 - val_acc: 0.8425\n",
            "Epoch 121/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8859 - val_loss: 0.3331 - val_acc: 0.8359\n",
            "Epoch 122/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2863 - acc: 0.8855 - val_loss: 0.3266 - val_acc: 0.8463\n",
            "Epoch 123/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2860 - acc: 0.8868 - val_loss: 0.3183 - val_acc: 0.8498\n",
            "Epoch 124/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2859 - acc: 0.8868 - val_loss: 0.3360 - val_acc: 0.8403\n",
            "Epoch 125/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2862 - acc: 0.8866 - val_loss: 0.3412 - val_acc: 0.8401\n",
            "Epoch 126/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8867 - val_loss: 0.3256 - val_acc: 0.8500\n",
            "Epoch 127/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8867 - val_loss: 0.3320 - val_acc: 0.8407\n",
            "Epoch 128/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8869 - val_loss: 0.3250 - val_acc: 0.8472\n",
            "Epoch 129/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2861 - acc: 0.8862 - val_loss: 0.3550 - val_acc: 0.8343\n",
            "Epoch 130/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2854 - acc: 0.8865 - val_loss: 0.3136 - val_acc: 0.8483\n",
            "Epoch 131/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2861 - acc: 0.8867 - val_loss: 0.3344 - val_acc: 0.8423\n",
            "Epoch 132/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8864 - val_loss: 0.3383 - val_acc: 0.8454\n",
            "Epoch 133/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2854 - acc: 0.8868 - val_loss: 0.3342 - val_acc: 0.8434\n",
            "Epoch 134/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2856 - acc: 0.8868 - val_loss: 0.3259 - val_acc: 0.8483\n",
            "Epoch 135/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2859 - acc: 0.8860 - val_loss: 0.3416 - val_acc: 0.8407\n",
            "Epoch 136/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2855 - acc: 0.8862 - val_loss: 0.3326 - val_acc: 0.8445\n",
            "Epoch 137/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2858 - acc: 0.8868 - val_loss: 0.3555 - val_acc: 0.8306\n",
            "Epoch 138/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8864 - val_loss: 0.3414 - val_acc: 0.8385\n",
            "Epoch 139/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2855 - acc: 0.8870 - val_loss: 0.3241 - val_acc: 0.8509\n",
            "Epoch 140/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8863 - val_loss: 0.3553 - val_acc: 0.8383\n",
            "Epoch 141/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2854 - acc: 0.8859 - val_loss: 0.3224 - val_acc: 0.8456\n",
            "Epoch 142/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2856 - acc: 0.8865 - val_loss: 0.3411 - val_acc: 0.8425\n",
            "Epoch 143/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2859 - acc: 0.8859 - val_loss: 0.3347 - val_acc: 0.8452\n",
            "Epoch 144/150\n",
            "71860/71860 [==============================] - 1s 19us/step - loss: 0.2860 - acc: 0.8868 - val_loss: 0.3244 - val_acc: 0.8474\n",
            "Epoch 145/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2857 - acc: 0.8857 - val_loss: 0.3368 - val_acc: 0.8399\n",
            "Epoch 146/150\n",
            "71860/71860 [==============================] - 1s 18us/step - loss: 0.2864 - acc: 0.8859 - val_loss: 0.3361 - val_acc: 0.8438\n",
            "Epoch 147/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8861 - val_loss: 0.3371 - val_acc: 0.8396\n",
            "Epoch 148/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2862 - acc: 0.8867 - val_loss: 0.3238 - val_acc: 0.8445\n",
            "Epoch 149/150\n",
            "71860/71860 [==============================] - 1s 17us/step - loss: 0.2857 - acc: 0.8863 - val_loss: 0.3136 - val_acc: 0.8531\n",
            "Epoch 150/150\n",
            "71860/71860 [==============================] - 1s 16us/step - loss: 0.2854 - acc: 0.8866 - val_loss: 0.3361 - val_acc: 0.8394\n",
            "acc: 83.94%\n",
            "84.24% (+/- 0.54%)\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   460    68        yes\n",
            "   644    3348        no\n",
            "Accuracy: 0.8423834907434031\n",
            "Sensitivity: 0.8704859141614671\n",
            "Specificity: 0.8386603877561245\n",
            "Precision: 0.41684019918515164\n",
            "f_score: 0.5637320925676502\n",
            "AUC: 0.8545699298699752\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWtdWxPr-nQh",
        "colab_type": "code",
        "outputId": "fac178b6-b6b5-479c-f225-d14e394bc5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "fpr_svm_p,tpr_svm_p ,thresholds_svm_p,auc_svm_p = percentage_split_SVM_SMOTE(0.25,X,Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1049    273        yes\n",
            "   1275    8706        no\n",
            "Accuracy: 0.8630452092364859\n",
            "Sensitivity: 0.7934947049924357\n",
            "Specificity: 0.8722572888488127\n",
            "Precision: 0.45137693631669534\n",
            "f_score: 0.5754251234229292\n",
            "AUC: 0.8328759969206242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj6yBcko-5Bd",
        "colab_type": "code",
        "outputId": "5dc94937-405e-4131-e563-04b2454107ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "fpr_svm_c,tpr_svm_c ,thresholds_svm_c,auc_svm_c=crossvalidate_SVM_Smote(10,X,Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8646616541353384\n",
            "Acc: 0.8599867285998672\n",
            "Acc: 0.8637469586374696\n",
            "Acc: 0.864852908648529\n",
            "Acc: 0.8626410086264101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rUh7Wppn4Jm",
        "colab_type": "code",
        "outputId": "f504cc07-1033-4ef0-f7c8-afb854acf63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "fpr_lr_p,tpr_lr_p ,thresholds_lr_p,auc_lr_p = percentage_split_LogisticsRegression_SMOTE(0.25,X,Y)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1086    236        yes\n",
            "   1526    8455        no\n",
            "Accuracy: 0.8441121826063877\n",
            "Sensitivity: 0.8214826021180031\n",
            "Specificity: 0.8471095080653241\n",
            "Precision: 0.41577335375191427\n",
            "f_score: 0.5521098118962888\n",
            "AUC: 0.8342960550916636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHb61PKYqJ4x",
        "colab_type": "code",
        "outputId": "8527ba23-6b0c-45cf-fcfd-f19fa1b676f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "fpr_lr_c,tpr_lr_c ,thresholds_lr_c,auc_lr_c = crossvalidate_LogisticsRegression_SMOTE(10,X,Y)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8456435205661211\n",
            "Acc: 0.8391948683919487\n",
            "Acc: 0.8464941384649414\n",
            "Acc: 0.8489272284892723\n",
            "Acc: 0.8411855784118558\n",
            "Acc: 0.8577748285777483\n",
            "Acc: 0.8422915284229153\n",
            "Acc: 0.8553417385534173\n",
            "Acc: 0.8374253483742535\n",
            "Acc: 0.8425127184251272\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   429    99        yes\n",
            "   598    3394        no\n",
            "Accuracy: 0.8456791488796974\n",
            "Sensitivity: 0.8116846284741918\n",
            "Specificity: 0.8501828565703121\n",
            "Precision: 0.41785088573097134\n",
            "f_score: 0.5516931182933881\n",
            "AUC: 0.8309320876956869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--bMKOQUyGkc",
        "colab_type": "code",
        "outputId": "abc38883-c353-485b-acaf-4c7c3a380b84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "tpr_dt_p, fpr_dt_p, threshold_dt_p, auc_dt_p = DecisionTree_percentage_SMOTE(0.25,X, Y)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1032    290         yes\n",
            "   1665    8316         no\n",
            "\n",
            "Accuracy Score: 0.8270370698044767\n",
            "Sensitivity: 0.7806354009077155\n",
            "Specificity: 0.8331830477908025\n",
            "\n",
            "F1 Score: 0.5135605872107489\n",
            "Precision: 0.38264738598442716\n",
            "AUC: 0.8069092243492592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ3fZs48yHDJ",
        "colab_type": "code",
        "outputId": "2aed2571-40d1-4e61-9b5c-0edb0950dc3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "tpr_dt_c, fpr_dt_c, threshold_dt_c, auc_dt_c = DecisionTree_crossvalidation_SMOTE(10,X, Y)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8341441839893853\n",
            "Acc: 0.8259234682592347\n",
            "Acc: 0.8332227383322274\n",
            "Acc: 0.8440610484406105\n",
            "Acc: 0.8305684583056846\n",
            "Acc: 0.820836098208361\n",
            "Acc: 0.82304799823048\n",
            "Acc: 0.8259234682592347\n",
            "Acc: 0.8312320283123202\n",
            "Acc: 0.8367617783676178\n",
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   413    115        yes\n",
            "   650    3341        no\n",
            "Accuracy: 0.8305722058791003\n",
            "Sensitivity: 0.781811306485158\n",
            "Specificity: 0.8370322128149893\n",
            "Precision: 0.38859129781035623\n",
            "f_score: 0.5191462649089768\n",
            "AUC: 0.8094210535177652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dxBFS0-eK2e",
        "colab_type": "code",
        "outputId": "19feac3a-a3b5-47db-f7eb-94cde0865fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "tpr_knn_p, fpr_knn_p, threshold_knn_p, auc_knn_p = KNN_percentage_SMOTE(0.25,X, Y)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   926    396         yes\n",
            "   1586    8395         no\n",
            "\n",
            "Accuracy Score: 0.8246483234539502\n",
            "Sensitivity: 0.7004538577912254\n",
            "Specificity: 0.8410980863640918\n",
            "\n",
            "F1 Score: 0.48304642670839854\n",
            "Precision: 0.36863057324840764\n",
            "AUC: 0.7707759720776586\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBkTiNWTeQ4b",
        "colab_type": "code",
        "outputId": "7f88d37e-bcca-42b8-9fa6-a87426816008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "tpr_knn_c, fpr_knn_c, threshold_knn_c, auc_knn_c = KNN_crossvalidation_SMOTE(10,X, Y)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8226448474126493\n",
            "Acc: 0.8168546781685467\n",
            "Acc: 0.8270294182702942\n",
            "Acc: 0.818624198186242\n",
            "Acc: 0.8263658482636584\n",
            "Acc: 0.8345498783454988\n",
            "Acc: 0.8241539482415395\n",
            "Acc: 0.8310108383101084\n",
            "Acc: 0.8245963282459633\n",
            "Acc: 0.8283565582835656\n",
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   376    152        yes\n",
            "   636    3355        no\n",
            "Accuracy: 0.8254185928203315\n",
            "Sensitivity: 0.7118547929665343\n",
            "Specificity: 0.8404639046139973\n",
            "Precision: 0.3715216104203671\n",
            "f_score: 0.4882318615055437\n",
            "AUC: 0.7761602623648213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcqlLVmmi560",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ec53d8dc-443b-4385-b2c3-4d4e18ee7979"
      },
      "source": [
        "tpr_rf_p, fpr_rf_p, threshold_rf_p, auc_rf_p = RandomForest_percentage_SMOTE(0.25,X, Y)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   1035    287         yes\n",
            "   1332    8649         no\n",
            "\n",
            "Accuracy Score: 0.856763691055472\n",
            "Sensitivity: 0.7829046898638427\n",
            "Specificity: 0.866546438232642\n",
            "\n",
            "F1 Score: 0.5611276768772026\n",
            "Precision: 0.4372623574144487\n",
            "AUC: 0.8247255640482424\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx8NFQ2Xi6Ym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "345aec06-23fa-4378-f126-6a146bc6170e"
      },
      "source": [
        "tpr_rf_c, fpr_rf_c, threshold_rf_c, auc_rf_c = RandomForest_crossvalidation_SMOTE(10,X, Y)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.848297213622291\n",
            "\n",
            "Accuracy: 0.851802698518027\n",
            "\n",
            "Accuracy: 0.851802698518027\n",
            "\n",
            "Accuracy: 0.8548993585489936\n",
            "\n",
            "Accuracy: 0.8537934085379341\n",
            "\n",
            "Accuracy: 0.8683919486839194\n",
            "\n",
            "Accuracy: 0.847378898473789\n",
            "\n",
            "Accuracy: 0.8599867285998672\n",
            "\n",
            "Accuracy: 0.8513603185136032\n",
            "\n",
            "Accuracy: 0.8469365184693651\n",
            "\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   410    118        yes\n",
            "   544    3447        no\n",
            "Accuracy: 0.8534648647453054\n",
            "Sensitivity: 0.77670637171488\n",
            "Specificity: 0.8636340864686138\n",
            "Precision: 0.43006700167504186\n",
            "f_score: 0.5536015093322553\n",
            "AUC: 0.8201713759944733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpkeIgvckaAv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "7f2d2fd1-dadc-4d55-f6ed-cfff19b91613"
      },
      "source": [
        "fpr_bayes_p,tpr_bayes_p ,thresholds_bayes_p,auc_bayes_p=percentage_split_Bayes(0.25,X,Y)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage Split\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   680    642        yes\n",
            "   1051    8930        no\n",
            "Accuracy: 0.8502167566132885\n",
            "Sensitivity: 0.5143721633888049\n",
            "Specificity: 0.8946999298667468\n",
            "Precision: 0.3928365106874639\n",
            "f_score: 0.44546347854569274\n",
            "AUC: 0.7045360466277759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-2GtHXnkaUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "db698aec-498d-4c72-ae72-2200f56b8dea"
      },
      "source": [
        "fpr_bayes_c,tpr_bayes_c ,thresholds_bayes_c,auc_bayes_c=crossvalidate_Bayes(10,X,Y)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.851172047766475\n",
            "Acc: 0.8445034284450342\n",
            "Acc: 0.8577748285777483\n",
            "Acc: 0.8445034284450342\n",
            "Acc: 0.8533510285335103\n",
            "Acc: 0.8597655385976554\n",
            "Acc: 0.8522450785224508\n",
            "Acc: 0.8546781685467817\n",
            "Acc: 0.8537934085379341\n",
            "Acc: 0.8456093784560937\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   yes     no   <<-- Classified as   \n",
            "   268    260        yes\n",
            "   410    3581        no\n",
            "Accuracy: 0.8517396208887218\n",
            "Sensitivity: 0.5084136887880506\n",
            "Specificity: 0.8972245879464956\n",
            "Precision: 0.39590694935217896\n",
            "f_score: 0.4451618243522886\n",
            "AUC: 0.702816801768454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg5pbP0HmKRO",
        "colab_type": "code",
        "outputId": "1b8df2f0-873b-4618-8c73-7eeecc59cf85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "plt.plot(fpr_nn_p, tpr_nn_p, label='NN (area = {:.3f})'.format(auc_nn_p))\n",
        "#plt.plot(fpr_svm_p, tpr_svm_p, label='SVM (area = {:.3f})'.format(auc_svm_p))\n",
        "plt.plot(fpr_lr_p, tpr_lr_p, label='LR (area = {:.3f})'.format(auc_lr_p))\n",
        "plt.plot(fpr_dt_p, tpr_dt_p, label='DT (area = {:.3f})'.format(auc_dt_p))\n",
        "plt.plot(fpr_knn_p, tpr_knn_p, label='KNN (area = {:.3f})'.format(auc_knn_p))\n",
        "plt.plot(fpr_rf_p, tpr_rf_p, label='RF (area = {:.3f})'.format(auc_rf_p))\n",
        "plt.plot(fpr_bayes_p, tpr_bayes_p, label='NB (area = {:.3f})'.format(auc_bayes_p))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECSxi7ytl1RT",
        "colab_type": "code",
        "outputId": "3c9916c0-db94-41e9-8df4-77cba27a9d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "plt.plot(fpr_nn_c, tpr_nn_c, label='NN (area = {:.3f})'.format(auc_nn_c))\n",
        "#plt.plot(fpr_svm_c, tpr_svm_c, label='SVM (area = {:.3f})'.format(auc_svm_c))\n",
        "plt.plot(fpr_lr_c, tpr_lr_c, label='LR (area = {:.3f})'.format(auc_lr_c))\n",
        "plt.plot(fpr_dt_c, tpr_dt_c, label='DT (area = {:.3f})'.format(auc_dt_c))\n",
        "plt.plot(fpr_knn_c, tpr_knn_c, label='KNN (area = {:.3f})'.format(auc_knn_c))\n",
        "plt.plot(fpr_rf_c, tpr_rf_c, label='RF (area = {:.3f})'.format(auc_rf_c))\n",
        "plt.plot(fpr_bayes_c, tpr_bayes_c, label='NB (area = {:.3f})'.format(auc_bayes_c))\n",
        "\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHOUIFAqFoozz/3TwSeZAKGB4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hyleu8Sh01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clf = svm.SVC(kernel=\"linear\")\n",
        "# clf.fit(x_train, y_train)\n",
        "# predict_svm = clf.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFxB6kc4M4Tf",
        "colab_type": "code",
        "outputId": "1e829502-2d3b-4007-f3a5-6e2432bac28d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Confusion_matrix(y_test,predict_svm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "   0     1   << Classified as   \n",
            "   29    101\n",
            "   35    966\n",
            "Accuracy: 0.8797524314765695\n",
            "Sensitivity: 0.2230769230769231\n",
            "Specificity: 0.965034965034965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5bGwv2lS7Me",
        "colab_type": "code",
        "outputId": "23fcc982-b7fe-4819-bd36-ca1c30873b2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# crossvalidate_SVM(10,X,Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8741721854304636\n",
            "Acc: 0.8761061946902655\n",
            "Acc: 0.8849557522123894\n",
            "Acc: 0.8938053097345132\n",
            "Acc: 0.8915929203539823\n",
            "Acc: 0.8738938053097345\n",
            "Acc: 0.8761061946902655\n",
            "Acc: 0.8938053097345132\n",
            "Acc: 0.8938053097345132\n",
            "Acc: 0.8938053097345132\n",
            "Cross-validation\n",
            "Confusion Matrix\n",
            "   0     1   << Classified as   \n",
            "   15    37\n",
            "   14    385\n",
            "Accuracy: 0.8815207780725022\n",
            "Sensitivity: 0.28982725527831094\n",
            "Specificity: 0.9627500000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrryyGXfy-Uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}